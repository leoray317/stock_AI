{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8B4513 size=100 face=\"標楷體\"> 抓資料 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "import statsmodels.api as sm \n",
    "import scipy.stats as scs \n",
    "from scipy.fftpack import fft,ifft\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import datetime, time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.base.datetools import dates_from_str\n",
    "import math\n",
    "from statsmodels.tsa.vector_ar.hypothesis_test_results import \\\n",
    "    CausalityTestResults, NormalityTestResults, WhitenessTestResults\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓取股票清單\n",
    "tai50 = \"https://www.taifex.com.tw/cht/9/futuresQADetail\"\n",
    "response_tai50 = requests.get(tai50)\n",
    "soup = bs(response_tai50.text)\n",
    "stock_list = re.findall(r\"[\\d][\\d][\\d][\\d][\\ ]\",soup.text)#抓出股票代碼的形式\n",
    "stock_list = re.findall(r\"[\\d][\\d][\\d][\\d]\",str(stock_list[::]))\n",
    "# stock_list = re.findall(r\"[\\d][\\d][\\d][\\d]\",str(stock_list[::2]))#取權重前450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = datetime.datetime.today()#抓日期\n",
    "#int(time.mktime(t.timetuple()))yahoo財經的日期為秒,因此轉秒\n",
    "names = locals()\n",
    "for etf in stock_list:\n",
    "    try:\n",
    "        site = \"https://query1.finance.yahoo.com/v7/finance/download/%s.TW?period1=1483228800&period2=%s&interval=1d&events=history\"%(etf,int(time.mktime(t.timetuple())))\n",
    "        response = requests.get(site)\n",
    "        names[\"df_%s\"%etf] = pd.read_csv(StringIO(response.text))\n",
    "        names[\"df_%s\"%etf] = names[\"df_%s\"%etf].fillna(method='ffill')\n",
    "        names[\"df_%s\"%etf][\"價格漲幅\"]=names[\"df_%s\"%etf][\"Adj Close\"].diff()\n",
    "        names[\"df_%s\"%etf][\"成交量變動\"]=names[\"df_%s\"%etf][\"Volume\"].diff()\n",
    "        names[\"df_%s\"%etf] = names[\"df_%s\"%etf].dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)    \n",
    "        names[\"df_%s\"%etf] = names[\"df_%s\"%etf].rename(columns={\n",
    " 'Open':'Open_%s'% etf,\n",
    " 'High':'High_%s'% etf,\n",
    " 'Low':'Low_%s'% etf,\n",
    " 'Close':'Close_%s'% etf,\n",
    " 'Adj Close':'Adj Close_%s'% etf,\n",
    " 'Volume':'Volume_%s'% etf,\n",
    " '價格漲幅':'價格漲幅_%s'% etf,\n",
    " '漲幅...':'漲幅..._%s'% etf,\n",
    " '振幅...':'振幅..._%s'% etf,\n",
    " '成交量變動':'成交量變動_%s'% etf,})\n",
    "    except:\n",
    "        print(names[\"df_%s\"%etf],\"抓不到\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#建立df清單\n",
    "stock_df_list=[]\n",
    "for stock in stock_list:\n",
    "    stock_df =\"df_\"+stock\n",
    "    stock_df_list.append(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合併成一個df\n",
    "all_stock_df=df_2330.iloc[:,[0]]\n",
    "for stock_df in stock_df_list:\n",
    "    all_stock_df = pd.merge(all_stock_df, names[stock_df], how='left', on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FITX_close = pd.read_csv(r\"C:\\Users\\user\\Desktop\\class\\金融\\台指近.csv\")\n",
    "# FITX_close = FITX_close.rename(columns={\"時間\":\"Date\"}).iloc[0:,[0,4]]\n",
    "# FITX_close[\"Date\"] = pd.to_datetime(FITX_close[\"Date\"])  #日期格式化\n",
    "# FITX_close[\"Date\"] = FITX_close[\"Date\"].astype(str)  #merge的那個形態要一樣\n",
    "# FITX_close[\"Target\"] = FITX_close[\"收盤價\"].shift(-1)\n",
    "# FITX_close = FITX.iloc[:,[0,4,FITX.shape[1]-1]]\n",
    "# Significant_factor = pd.merge(all_stock_df, FITX_close, how='left', on='Date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8B4513 size=100 face=\"標楷體\"> 資料預處理 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#向量自回歸抓領先變數\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.base.datetools import dates_from_str\n",
    "import math\n",
    "from statsmodels.tsa.vector_ar.hypothesis_test_results import \\\n",
    "    CausalityTestResults, NormalityTestResults, WhitenessTestResults\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前資料共 854 根日K\n"
     ]
    }
   ],
   "source": [
    "# #先把shift的收盤價帶入當成train\n",
    "# FITX = pd.read_csv(r\"C:\\Users\\user\\Desktop\\class\\金融\\台指近.csv\")\n",
    "# FITX = FITX.rename(columns={\"時間\":\"Date\"}).iloc[0:,[0,4]]\n",
    "# FITX[\"Date\"] = pd.to_datetime(FITX[\"Date\"])  #日期格式化\n",
    "# FITX[\"Date\"] = FITX[\"Date\"].astype(str)  #merge的那個形態要一樣\n",
    "# all_stock_df[\"Date\"] = all_stock_df[\"Date\"].astype(str)\n",
    "# all_stock_df_FITX = all_stock_df\n",
    "# all_stock_df_FITX = pd.merge(all_stock_df_FITX, FITX.iloc[:,[0,19]], how='left', on='Date')\n",
    "# print(\"目前資料共\",df_2330.shape[0],\"根日K\")\n",
    "print(\"目前資料共\",df_2330.shape[0],\"根日K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "顯著的因子有 169 個\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#逐一併入再找出有領先因子\n",
    "#統計區間終點=日前日K-1\n",
    "間隔 = 300\n",
    "統計區間終點 =500\n",
    "統計區間起點 = 統計區間終點-間隔   #樣本內\n",
    "Significant_factor = df_2330.iloc[0:,[0]]\n",
    "FITX_close = pd.read_csv(r\"C:\\Users\\user\\Desktop\\class\\金融\\台指近.csv\")\n",
    "FITX_close = FITX_close.rename(columns={\"時間\":\"Date\"}).iloc[0:,[0,4]]\n",
    "FITX_close[\"Date\"] = pd.to_datetime(FITX_close[\"Date\"])  #日期格式化\n",
    "FITX_close[\"Date\"] = FITX_close[\"Date\"].astype(str)  #merge的那個形態要一樣\n",
    "for i in stock_df_list:\n",
    "    try:\n",
    "        names['%s' % i][\"Date\"] = names['%s' % i][\"Date\"].astype(str)\n",
    "        append_FITX = pd.merge(names['%s' % i], FITX_close, how='left', on='Date')\n",
    "        model = VAR(append_FITX.iloc[統計區間起點:統計區間終點,1:append_FITX.shape[1]])  #用樣本內跑檢定,避免選擇性偏誤\n",
    "        results = model.fit(1)\n",
    "        for a in range(1,append_FITX.shape[1]-1):\n",
    "            p_value = results.test_causality(\"收盤價\",causing=append_FITX.iloc[:,[a]], kind='f', signif=0.1).pvalue\n",
    "            if p_value<.07:\n",
    "                Significant_factor = pd.merge(Significant_factor, append_FITX.iloc[:,[0,a]], how='left', on='Date')\n",
    "    except:\n",
    "        pass\n",
    "Significant_factor = Significant_factor.dropna(axis=1, how='any', thresh=None, subset=None, inplace=False)\n",
    "\n",
    "# 共線性,去除vif>100的\n",
    "cc = sp.corrcoef(Significant_factor.iloc[:,1:Significant_factor.shape[1]], rowvar=False)\n",
    "VIF = np.linalg.inv(cc)\n",
    "VIF_list=VIF.diagonal()\n",
    "VIF_100 = np.where(VIF_list>100)    #找VIF>100的列\n",
    "VIF_100 = VIF_100[0][1:]            #留日期\n",
    "Significant_factor.drop(Significant_factor.iloc[:,1:Significant_factor.shape[1]].columns[VIF_100], axis=1,inplace=True)\n",
    "Significant_factor.shape\n",
    "\n",
    "\n",
    "print(\"顯著的因子有\",Significant_factor.shape[1],\"個\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8B4513 size=100 face=\"標楷體\"> 建模ML </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, auc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC,SVR\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error,mean_absolute_error\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import sqlite3\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from xgboost import plot_tree\n",
    "from graphviz import Digraph\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Significant_factor_test = Significant_factor\n",
    "預測=[]\n",
    "實際 =[]\n",
    "格 = 13\n",
    "for 起 in range(500,len(Significant_factor_test)-格):\n",
    "    終 = 起+格\n",
    "\n",
    "    model_xgb = xgb.XGBRegressor(\n",
    "    max_depth=15,\n",
    "    subsample = 0.1  \n",
    "    )\n",
    "    \n",
    "     \n",
    "    #合併\n",
    "    FITX = pd.read_csv(r\"C:\\Users\\user\\Desktop\\class\\金融\\台指近.csv\")\n",
    "    FITX = FITX.rename(columns={\"時間\":\"Date\"})\n",
    "    FITX[\"Date\"] = pd.to_datetime(FITX[\"Date\"])  #日期格式化\n",
    "    FITX[\"Date\"] = FITX[\"Date\"].astype(str)  #merge的那個形態要一樣\n",
    "    FITX[\"(期權)未平倉\"] = FITX[\"(期權)未平倉\"].str.replace(\"口\",\"\")\n",
    "    FITX[\"(期權)未平倉\"] = FITX[\"(期權)未平倉\"].astype(float)\n",
    "    FITX[\"(期權)未平倉變化\"] = FITX[\"(期權)未平倉變化\"].str.replace(\"口\",\"\")\n",
    "    FITX[\"(期權)未平倉變化\"] = FITX[\"(期權)未平倉變化\"].astype(float)\n",
    "    FITX[\"diff收盤價\"] = FITX[\"收盤價\"].diff()\n",
    "    FITX[\"Target\"] = FITX[\"收盤價\"].shift(-1)\n",
    "    FITX_target = FITX.iloc[:,[0,FITX.shape[1]-1]]\n",
    "    Significant_factor_test =  pd.merge(Significant_factor, FITX_target, how='left', on='Date')\n",
    "    Significant_factor_test = Significant_factor_test.set_index('Date')\n",
    "    #拆分訓練測試\n",
    "\n",
    "    安安=0\n",
    "\n",
    "    all_feature= Significant_factor_test\n",
    "#     .dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "    train_data = all_feature.iloc[起:終,1:all_feature.shape[1]-1]\n",
    "    train_targets = all_feature.iloc[起:終,[all_feature.shape[1]-1]]\n",
    "    test_data = all_feature.iloc[終:終+1,1:all_feature.shape[1]-1]\n",
    "    test_targets = all_feature.iloc[終:終+1,all_feature.shape[1]-1]\n",
    "\n",
    "\n",
    "\n",
    "    # train_data, test_data, train_targets, test_targets = train_test_split(all_feature.iloc[:,0:all_feature.shape[1]-2], all_feature.iloc[:,all_feature.shape[1]-1] , test_size=0.1,shuffle=False)\n",
    "\n",
    "\n",
    "    #訓練\n",
    "    model_xgb.fit(train_data, train_targets)\n",
    "    y_pred = model_xgb.predict(test_data)\n",
    "\n",
    "    預測.append(y_pred[0])\n",
    "    實際.append(test_targets[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#視覺化,紅色為標準答案\n",
    "plt.style.use('seaborn')\n",
    "test_targets_array=test_targets.values\n",
    "plt.figure(figsize=(15, 6)) \n",
    "xt = test_data[:]\n",
    "plt.plot(實際[-10:], 'rd', label='test_targets_array')\n",
    "plt.plot(預測[-10:], 'gd', label='model_xgb')\n",
    "# plt.plot(model_AdaBoostRegressor.predict(xt), '#00008B', label='model_AdaBoostRegressor')\n",
    "# plt.plot(Vote.predict(xt), 'bd', label='Vote')\n",
    "\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False,\n",
    "                labelbottom=False)\n",
    "plt.ylabel('predicted')\n",
    "plt.xlabel('training samples')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Comparison of individual predictions with test_targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#簡易回測,方法1  視預測值漲跌決定買賣\n",
    "all_profit = 0\n",
    "profit = 0\n",
    "獲利圖 =[]\n",
    "for i in range(1,len(預測)):\n",
    "    if (預測[i]+預測[i-1])/2+0>(預測[i]+預測[i-1]+預測[i-2]+預測[i-3])/4:   \n",
    "        profit = 實際[i]-實際[i-1]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "    else:\n",
    "        \n",
    "        profit = 實際[i-1]-實際[i]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "獲利圖array=np.array(獲利圖)\n",
    "plt.style.use('seaborn')\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.plot(獲利圖array, 'r', label='test_targets_array')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#簡易回測,方法2  \n",
    "all_profit = 0\n",
    "profit = 0\n",
    "獲利圖 =[]\n",
    "for i in range(1,len(預測)-2):\n",
    "    if 預測[i]>(預測[i-2]):   \n",
    "        profit = 實際[i+1]-實際[i]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "    else:\n",
    "        pass\n",
    "        profit = 實際[i-1]-實際[i]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "獲利圖array=np.array(獲利圖)\n",
    "plt.style.use('seaborn')\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.plot(獲利圖array, 'r', label='test_targets_array')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天的預測是\n",
      "上漲\n"
     ]
    }
   ],
   "source": [
    "#簡易回測,方法3  \n",
    "all_profit = 0\n",
    "profit = 0\n",
    "獲利圖 =[]\n",
    "for i in range(1,len(預測)-2):\n",
    "    if 預測[i]+20>(預測[i-1]):   \n",
    "        profit = 實際[i]-實際[i-1]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "    else:\n",
    "        pass\n",
    "        profit = 實際[i-1]-實際[i]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "獲利圖array=np.array(獲利圖)\n",
    "plt.style.use('seaborn')\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.plot(獲利圖array, 'r', label='test_targets_array')\n",
    "plt.show()\n",
    "   \n",
    "print(\"明天的預測是\")\n",
    "if 預測[-1]>預測[-2]:\n",
    "    print(\"上漲\")\n",
    "else:\n",
    "    print(\"下跌\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#簡易回測,方法4\n",
    "all_profit = 0\n",
    "profit = 0\n",
    "獲利圖 =[]\n",
    "for i in range(1,len(預測)):\n",
    "    if 預測[i]+10<實際[i-2]:\n",
    "        profit = 實際[i]-實際[i-1]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "    else:\n",
    "        profit = 實際[i-1]-實際[i]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "獲利圖array=np.array(獲利圖)\n",
    "plt.style.use('seaborn')\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.plot(獲利圖array, 'r', label='test_targets_array')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#雙重條件\n",
    "all_profit = 0\n",
    "profit = 0\n",
    "獲利圖 =[]\n",
    "for i in range(1,len(預測)):\n",
    "    if ((預測[i]+預測[i-1])/2>(預測[i]+預測[i-1]+預測[i-2]+預測[i-3])/4)&(預測[i]>預測[i-1]):   \n",
    "        profit = 實際[i]-實際[i-1]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "    elif ((預測[i]+預測[i-1])/2<(預測[i]+預測[i-1]+預測[i-2]+預測[i-3])/4)&(預測[i]<預測[i-1]): \n",
    "        \n",
    "        profit = 實際[i-1]-實際[i]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "    else:\n",
    "        pass\n",
    "獲利圖array=np.array(獲利圖)\n",
    "plt.style.use('seaborn')\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.plot(獲利圖array, 'r', label='test_targets_array')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天的預測是\n",
      "下跌\n"
     ]
    }
   ],
   "source": [
    "#預測結果1\n",
    "print(\"明天的預測是\")\n",
    "if (預測[-1]+預測[-2])/2>(預測[-1]+預測[-2]+預測[-3]+預測[-4])/4:\n",
    "    print(\"上漲\")\n",
    "else:\n",
    "    print(\"下跌\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天的預測是\n",
      "上漲\n"
     ]
    }
   ],
   "source": [
    "#預測結果3\n",
    "print(\"明天的預測是\")\n",
    "if 預測[-1]>預測[-2]:\n",
    "    print(\"上漲\")\n",
    "else:\n",
    "    print(\"下跌\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_profit = 0\n",
    "profit = 0\n",
    "獲利圖 =[]\n",
    "for i in range(1,len(預測)):\n",
    "    if 預測[i]>預測[i-2]:   \n",
    "        profit = 實際[i]-實際[i-1]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "    else:\n",
    "        profit = 實際[i-1]-實際[i]\n",
    "        all_profit+=profit\n",
    "        獲利圖.append(all_profit)\n",
    "獲利圖array=np.array(獲利圖)\n",
    "plt.style.use('seaborn')\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.plot(獲利圖array, 'r', label='test_targets_array')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '下單預測' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-daa11559e811>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#預測結果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mmodel_xgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m下單預測\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"明天的預測是\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"上漲\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '下單預測' is not defined"
     ]
    }
   ],
   "source": [
    "#預測結果\n",
    "predict =model_xgb.predict(下單預測)\n",
    "print(\"明天的預測是\")\n",
    "if predict[len(predict)-1]>=predict[len(predict)-3]: \n",
    "    print(\"上漲\")\n",
    "else:\n",
    "    print(\"下跌\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 天積獲利 -277.0 第 1 天累積獲利 -277.0\n",
      "第 2 天積獲利 38.0 第 2 天累積獲利 -239.0\n",
      "第 3 天積獲利 -126.0 第 3 天累積獲利 -365.0\n",
      "第 4 天積獲利 238.0 第 4 天累積獲利 -127.0\n",
      "第 5 天積獲利 -337.0 第 5 天累積獲利 -464.0\n",
      "第 6 天積獲利 -109.0 第 6 天累積獲利 -573.0\n",
      "第 7 天積獲利 30.0 第 7 天累積獲利 -543.0\n",
      "第 8 天積獲利 -84.0 第 8 天累積獲利 -627.0\n",
      "第 9 天積獲利 61.0 第 9 天累積獲利 -566.0\n",
      "第 10 天積獲利 -88.0 第 10 天累積獲利 -654.0\n",
      "第 11 天積獲利 117.0 第 11 天累積獲利 -537.0\n",
      "第 12 天積獲利 -49.0 第 12 天累積獲利 -586.0\n",
      "第 13 天積獲利 -165.0 第 13 天累積獲利 -751.0\n",
      "第 14 天積獲利 -68.0 第 14 天累積獲利 -819.0\n",
      "第 15 天積獲利 56.0 第 15 天累積獲利 -763.0\n",
      "第 16 天積獲利 107.0 第 16 天累積獲利 -656.0\n",
      "第 17 天積獲利 19.0 第 17 天累積獲利 -637.0\n",
      "第 18 天積獲利 -66.0 第 18 天累積獲利 -703.0\n",
      "第 19 天積獲利 -209.0 第 19 天累積獲利 -912.0\n",
      "第 20 天積獲利 77.0 第 20 天累積獲利 -835.0\n",
      "第 21 天積獲利 163.0 第 21 天累積獲利 -672.0\n",
      "第 22 天積獲利 11.0 第 22 天累積獲利 -661.0\n",
      "第 23 天積獲利 -62.0 第 23 天累積獲利 -723.0\n",
      "第 24 天積獲利 8.0 第 24 天累積獲利 -715.0\n",
      "第 25 天積獲利 -138.0 第 25 天累積獲利 -853.0\n",
      "第 26 天積獲利 50.0 第 26 天累積獲利 -803.0\n",
      "第 27 天積獲利 -176.0 第 27 天累積獲利 -979.0\n",
      "第 28 天積獲利 -75.0 第 28 天累積獲利 -1054.0\n",
      "第 29 天積獲利 99.0 第 29 天累積獲利 -955.0\n",
      "第 30 天積獲利 113.0 第 30 天累積獲利 -842.0\n",
      "第 31 天積獲利 -36.0 第 31 天累積獲利 -878.0\n",
      "第 32 天積獲利 98.0 第 32 天累積獲利 -780.0\n",
      "第 33 天積獲利 213.0 第 33 天累積獲利 -567.0\n",
      "第 34 天積獲利 -100.0 第 34 天累積獲利 -667.0\n",
      "第 35 天積獲利 -127.0 第 35 天累積獲利 -794.0\n",
      "第 36 天積獲利 258.0 第 36 天累積獲利 -536.0\n",
      "第 37 天積獲利 nan 第 37 天累積獲利 nan\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 38 is out of bounds for axis 0 with size 38",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-0f09f63ec441>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mall_profit\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mprofit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mprofit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_targets_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtest_targets_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mall_profit\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mprofit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"第\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"天積獲利\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprofit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"第\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"天累積獲利\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_profit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 38 is out of bounds for axis 0 with size 38"
     ]
    }
   ],
   "source": [
    "#簡易回測,方法1  視預測值漲跌決定買賣\n",
    "predict = Vote.predict(test_data)\n",
    "all_profit = 0\n",
    "profit = 0\n",
    "for i in range(1,len(predict)):\n",
    "    if predict[i]>=predict[i-1]:   \n",
    "        profit = test_targets_array[i]-test_targets_array[i-1]\n",
    "        all_profit+=profit\n",
    "    else:\n",
    "        profit = test_targets_array[i-1]-test_targets_array[i]\n",
    "        all_profit+=profit\n",
    "    print(\"第\", i,\"天積獲利\",profit,\"第\", i,\"天累積獲利\",all_profit)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8B4513 size=100 face=\"標楷體\"> 建模DL </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Low_2884</th>\n",
       "      <th>Volume_6415</th>\n",
       "      <th>價格漲幅_6415</th>\n",
       "      <th>成交量變動_6415</th>\n",
       "      <th>Volume_2890</th>\n",
       "      <th>價格漲幅_2890</th>\n",
       "      <th>價格漲幅_2105</th>\n",
       "      <th>價格漲幅_2633</th>\n",
       "      <th>價格漲幅_2823</th>\n",
       "      <th>價格漲幅_2324</th>\n",
       "      <th>...</th>\n",
       "      <th>價格漲幅_1714</th>\n",
       "      <th>Volume_2480</th>\n",
       "      <th>價格漲幅_2480</th>\n",
       "      <th>成交量變動_2480</th>\n",
       "      <th>成交量變動_1321</th>\n",
       "      <th>Volume_1519</th>\n",
       "      <th>價格漲幅_1519</th>\n",
       "      <th>成交量變動_1519</th>\n",
       "      <th>價格漲幅_3530</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.857800</td>\n",
       "      <td>626000.0</td>\n",
       "      <td>17.530304</td>\n",
       "      <td>423000.0</td>\n",
       "      <td>6583345.0</td>\n",
       "      <td>-0.022478</td>\n",
       "      <td>0.267430</td>\n",
       "      <td>-0.046169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>0.371946</td>\n",
       "      <td>268000.0</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>176000.0</td>\n",
       "      <td>0.092657</td>\n",
       "      <td>-22000.0</td>\n",
       "      <td>-1.436646</td>\n",
       "      <td>9342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.898300</td>\n",
       "      <td>171000.0</td>\n",
       "      <td>5.843475</td>\n",
       "      <td>-455000.0</td>\n",
       "      <td>12639896.0</td>\n",
       "      <td>0.067425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.369341</td>\n",
       "      <td>0.081284</td>\n",
       "      <td>0.126235</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027159</td>\n",
       "      <td>148000.0</td>\n",
       "      <td>-0.041328</td>\n",
       "      <td>-181000.0</td>\n",
       "      <td>-81000.0</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0.046328</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>-0.383110</td>\n",
       "      <td>9365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.898300</td>\n",
       "      <td>506000.0</td>\n",
       "      <td>1.947784</td>\n",
       "      <td>335000.0</td>\n",
       "      <td>10379642.0</td>\n",
       "      <td>0.029969</td>\n",
       "      <td>0.802292</td>\n",
       "      <td>0.184668</td>\n",
       "      <td>-0.446740</td>\n",
       "      <td>-0.042077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>243000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>731000.0</td>\n",
       "      <td>0.138985</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>-0.095772</td>\n",
       "      <td>9342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.776900</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>-11.199920</td>\n",
       "      <td>-306000.0</td>\n",
       "      <td>8408650.0</td>\n",
       "      <td>-0.029969</td>\n",
       "      <td>0.534858</td>\n",
       "      <td>-0.138502</td>\n",
       "      <td>0.203072</td>\n",
       "      <td>-0.084158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054326</td>\n",
       "      <td>211000.0</td>\n",
       "      <td>-0.041327</td>\n",
       "      <td>-32000.0</td>\n",
       "      <td>-43000.0</td>\n",
       "      <td>302000.0</td>\n",
       "      <td>-0.092657</td>\n",
       "      <td>-429000.0</td>\n",
       "      <td>2.873291</td>\n",
       "      <td>9341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.736400</td>\n",
       "      <td>78000.0</td>\n",
       "      <td>-1.460846</td>\n",
       "      <td>-122000.0</td>\n",
       "      <td>8668352.0</td>\n",
       "      <td>-0.029967</td>\n",
       "      <td>-0.802291</td>\n",
       "      <td>0.046165</td>\n",
       "      <td>0.040596</td>\n",
       "      <td>0.042079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027167</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-22000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>220000.0</td>\n",
       "      <td>-0.092656</td>\n",
       "      <td>-82000.0</td>\n",
       "      <td>-0.469299</td>\n",
       "      <td>9337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.776900</td>\n",
       "      <td>155000.0</td>\n",
       "      <td>6.817352</td>\n",
       "      <td>77000.0</td>\n",
       "      <td>9286992.0</td>\n",
       "      <td>0.044956</td>\n",
       "      <td>1.337154</td>\n",
       "      <td>-0.046165</td>\n",
       "      <td>-0.121881</td>\n",
       "      <td>-0.042079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036219</td>\n",
       "      <td>313000.0</td>\n",
       "      <td>0.041327</td>\n",
       "      <td>124000.0</td>\n",
       "      <td>-30000.0</td>\n",
       "      <td>795000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>575000.0</td>\n",
       "      <td>5.669960</td>\n",
       "      <td>9411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.857800</td>\n",
       "      <td>153000.0</td>\n",
       "      <td>-2.921722</td>\n",
       "      <td>-2000.0</td>\n",
       "      <td>14103096.0</td>\n",
       "      <td>0.022470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046165</td>\n",
       "      <td>-0.527930</td>\n",
       "      <td>-0.126237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190144</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-167000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>574000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-221000.0</td>\n",
       "      <td>-3.754433</td>\n",
       "      <td>9375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.857800</td>\n",
       "      <td>156000.0</td>\n",
       "      <td>-8.765168</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>8483605.0</td>\n",
       "      <td>-0.014980</td>\n",
       "      <td>-0.178291</td>\n",
       "      <td>0.046169</td>\n",
       "      <td>-0.731096</td>\n",
       "      <td>0.210395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>341000.0</td>\n",
       "      <td>0.082655</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>172000.0</td>\n",
       "      <td>-0.092657</td>\n",
       "      <td>-402000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.817400</td>\n",
       "      <td>194000.0</td>\n",
       "      <td>-5.843444</td>\n",
       "      <td>38000.0</td>\n",
       "      <td>8759142.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.534859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.284264</td>\n",
       "      <td>0.210395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280689</td>\n",
       "      <td>118000.0</td>\n",
       "      <td>-0.082655</td>\n",
       "      <td>-223000.0</td>\n",
       "      <td>-27000.0</td>\n",
       "      <td>228000.0</td>\n",
       "      <td>-0.185310</td>\n",
       "      <td>56000.0</td>\n",
       "      <td>-0.469314</td>\n",
       "      <td>9340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.857800</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>6.330413</td>\n",
       "      <td>-113000.0</td>\n",
       "      <td>6391207.0</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>0.267429</td>\n",
       "      <td>-0.046169</td>\n",
       "      <td>0.121787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027167</td>\n",
       "      <td>148000.0</td>\n",
       "      <td>-0.165307</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>271000.0</td>\n",
       "      <td>0.046328</td>\n",
       "      <td>43000.0</td>\n",
       "      <td>0.938614</td>\n",
       "      <td>9340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14.857800</td>\n",
       "      <td>234000.0</td>\n",
       "      <td>10.712952</td>\n",
       "      <td>153000.0</td>\n",
       "      <td>9353502.0</td>\n",
       "      <td>-0.037459</td>\n",
       "      <td>-0.802284</td>\n",
       "      <td>-0.138498</td>\n",
       "      <td>0.365549</td>\n",
       "      <td>-0.042077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009053</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>0.165307</td>\n",
       "      <td>-67000.0</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>272000.0</td>\n",
       "      <td>0.046329</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-0.478890</td>\n",
       "      <td>9304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.776900</td>\n",
       "      <td>139000.0</td>\n",
       "      <td>-1.460876</td>\n",
       "      <td>-95000.0</td>\n",
       "      <td>11534578.0</td>\n",
       "      <td>-0.029967</td>\n",
       "      <td>0.089138</td>\n",
       "      <td>-0.138502</td>\n",
       "      <td>-0.162478</td>\n",
       "      <td>-0.042081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036220</td>\n",
       "      <td>111000.0</td>\n",
       "      <td>0.041328</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>-47000.0</td>\n",
       "      <td>154000.0</td>\n",
       "      <td>-0.092657</td>\n",
       "      <td>-118000.0</td>\n",
       "      <td>-2.375259</td>\n",
       "      <td>9330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14.776900</td>\n",
       "      <td>269000.0</td>\n",
       "      <td>-0.486939</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>7953643.0</td>\n",
       "      <td>0.014980</td>\n",
       "      <td>-0.624000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203074</td>\n",
       "      <td>-0.042077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018105</td>\n",
       "      <td>126000.0</td>\n",
       "      <td>0.082655</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>39000.0</td>\n",
       "      <td>167000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>-0.019142</td>\n",
       "      <td>9407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.979300</td>\n",
       "      <td>137000.0</td>\n",
       "      <td>-8.278198</td>\n",
       "      <td>-132000.0</td>\n",
       "      <td>15221082.0</td>\n",
       "      <td>0.074924</td>\n",
       "      <td>0.534862</td>\n",
       "      <td>0.092332</td>\n",
       "      <td>0.121881</td>\n",
       "      <td>0.042077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027167</td>\n",
       "      <td>149000.0</td>\n",
       "      <td>-0.123983</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>430000.0</td>\n",
       "      <td>311000.0</td>\n",
       "      <td>0.138985</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>3.198928</td>\n",
       "      <td>9442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.898300</td>\n",
       "      <td>148000.0</td>\n",
       "      <td>3.895630</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>14719625.0</td>\n",
       "      <td>0.014980</td>\n",
       "      <td>-0.178291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081190</td>\n",
       "      <td>-0.042077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072432</td>\n",
       "      <td>93000.0</td>\n",
       "      <td>0.041328</td>\n",
       "      <td>-56000.0</td>\n",
       "      <td>-347000.0</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>0.231638</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>-3.103157</td>\n",
       "      <td>9375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.776900</td>\n",
       "      <td>476000.0</td>\n",
       "      <td>9.739044</td>\n",
       "      <td>328000.0</td>\n",
       "      <td>24740329.0</td>\n",
       "      <td>-0.007490</td>\n",
       "      <td>-0.624005</td>\n",
       "      <td>-0.092332</td>\n",
       "      <td>-0.487335</td>\n",
       "      <td>-0.210399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>99000.0</td>\n",
       "      <td>0.041327</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>532000.0</td>\n",
       "      <td>1676000.0</td>\n",
       "      <td>0.416952</td>\n",
       "      <td>1351000.0</td>\n",
       "      <td>0.861992</td>\n",
       "      <td>9437.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.776900</td>\n",
       "      <td>111000.0</td>\n",
       "      <td>4.869507</td>\n",
       "      <td>-365000.0</td>\n",
       "      <td>21384259.0</td>\n",
       "      <td>-0.037458</td>\n",
       "      <td>0.534859</td>\n",
       "      <td>0.092332</td>\n",
       "      <td>0.162477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018105</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91000.0</td>\n",
       "      <td>-162000.0</td>\n",
       "      <td>663000.0</td>\n",
       "      <td>-0.231642</td>\n",
       "      <td>-1013000.0</td>\n",
       "      <td>-0.957763</td>\n",
       "      <td>9505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14.817400</td>\n",
       "      <td>170000.0</td>\n",
       "      <td>4.869568</td>\n",
       "      <td>59000.0</td>\n",
       "      <td>21882549.0</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>0.178288</td>\n",
       "      <td>0.092337</td>\n",
       "      <td>0.609215</td>\n",
       "      <td>0.084160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162977</td>\n",
       "      <td>227000.0</td>\n",
       "      <td>0.123981</td>\n",
       "      <td>37000.0</td>\n",
       "      <td>795000.0</td>\n",
       "      <td>700000.0</td>\n",
       "      <td>0.277967</td>\n",
       "      <td>37000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.776900</td>\n",
       "      <td>589000.0</td>\n",
       "      <td>14.608612</td>\n",
       "      <td>419000.0</td>\n",
       "      <td>19173623.0</td>\n",
       "      <td>0.044956</td>\n",
       "      <td>1.248012</td>\n",
       "      <td>0.092331</td>\n",
       "      <td>0.081192</td>\n",
       "      <td>0.042079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>231000.0</td>\n",
       "      <td>0.082657</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>-1007000.0</td>\n",
       "      <td>339000.0</td>\n",
       "      <td>-0.046325</td>\n",
       "      <td>-361000.0</td>\n",
       "      <td>-0.871567</td>\n",
       "      <td>9515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>14.776900</td>\n",
       "      <td>333000.0</td>\n",
       "      <td>6.817322</td>\n",
       "      <td>-256000.0</td>\n",
       "      <td>20684330.0</td>\n",
       "      <td>0.029968</td>\n",
       "      <td>-0.802296</td>\n",
       "      <td>0.323172</td>\n",
       "      <td>0.081190</td>\n",
       "      <td>-0.042079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054327</td>\n",
       "      <td>236000.0</td>\n",
       "      <td>0.082653</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>-77000.0</td>\n",
       "      <td>410000.0</td>\n",
       "      <td>-0.046332</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>-0.038315</td>\n",
       "      <td>9574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.817400</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>-16.556397</td>\n",
       "      <td>-153000.0</td>\n",
       "      <td>24039344.0</td>\n",
       "      <td>0.052439</td>\n",
       "      <td>0.713147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.609216</td>\n",
       "      <td>-0.084160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045274</td>\n",
       "      <td>129000.0</td>\n",
       "      <td>-0.041327</td>\n",
       "      <td>-107000.0</td>\n",
       "      <td>118000.0</td>\n",
       "      <td>388000.0</td>\n",
       "      <td>-0.185310</td>\n",
       "      <td>-22000.0</td>\n",
       "      <td>1.388763</td>\n",
       "      <td>9657.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14.898300</td>\n",
       "      <td>220000.0</td>\n",
       "      <td>-9.252136</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>39132687.0</td>\n",
       "      <td>0.067433</td>\n",
       "      <td>0.713153</td>\n",
       "      <td>-0.277003</td>\n",
       "      <td>-0.243668</td>\n",
       "      <td>0.126239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144871</td>\n",
       "      <td>337000.0</td>\n",
       "      <td>-0.165310</td>\n",
       "      <td>208000.0</td>\n",
       "      <td>-159000.0</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-170000.0</td>\n",
       "      <td>-0.957763</td>\n",
       "      <td>9704.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.060300</td>\n",
       "      <td>311000.0</td>\n",
       "      <td>-5.356476</td>\n",
       "      <td>91000.0</td>\n",
       "      <td>20054077.0</td>\n",
       "      <td>0.022470</td>\n",
       "      <td>0.267426</td>\n",
       "      <td>0.046168</td>\n",
       "      <td>-0.081190</td>\n",
       "      <td>0.168318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126766</td>\n",
       "      <td>1594000.0</td>\n",
       "      <td>0.909203</td>\n",
       "      <td>1257000.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>1216000.0</td>\n",
       "      <td>-0.463276</td>\n",
       "      <td>998000.0</td>\n",
       "      <td>0.469299</td>\n",
       "      <td>9708.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15.060300</td>\n",
       "      <td>273000.0</td>\n",
       "      <td>-2.921692</td>\n",
       "      <td>-38000.0</td>\n",
       "      <td>21390593.0</td>\n",
       "      <td>-0.022470</td>\n",
       "      <td>1.069725</td>\n",
       "      <td>0.092333</td>\n",
       "      <td>-0.243668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018115</td>\n",
       "      <td>1951000.0</td>\n",
       "      <td>0.206635</td>\n",
       "      <td>357000.0</td>\n",
       "      <td>78000.0</td>\n",
       "      <td>359000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-857000.0</td>\n",
       "      <td>0.009583</td>\n",
       "      <td>9799.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.938800</td>\n",
       "      <td>206000.0</td>\n",
       "      <td>3.408661</td>\n",
       "      <td>-67000.0</td>\n",
       "      <td>26350272.0</td>\n",
       "      <td>0.029968</td>\n",
       "      <td>0.891430</td>\n",
       "      <td>0.046167</td>\n",
       "      <td>-0.243763</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>578000.0</td>\n",
       "      <td>-0.165310</td>\n",
       "      <td>-1373000.0</td>\n",
       "      <td>-69000.0</td>\n",
       "      <td>301000.0</td>\n",
       "      <td>0.138981</td>\n",
       "      <td>-58000.0</td>\n",
       "      <td>-0.478882</td>\n",
       "      <td>9770.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15.343700</td>\n",
       "      <td>738000.0</td>\n",
       "      <td>18.991180</td>\n",
       "      <td>532000.0</td>\n",
       "      <td>26167635.0</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>-0.713150</td>\n",
       "      <td>0.046169</td>\n",
       "      <td>0.568621</td>\n",
       "      <td>0.168316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045263</td>\n",
       "      <td>307000.0</td>\n",
       "      <td>0.082655</td>\n",
       "      <td>-271000.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>192000.0</td>\n",
       "      <td>-0.092653</td>\n",
       "      <td>-109000.0</td>\n",
       "      <td>-0.478882</td>\n",
       "      <td>9763.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.222200</td>\n",
       "      <td>226000.0</td>\n",
       "      <td>4.869538</td>\n",
       "      <td>-512000.0</td>\n",
       "      <td>15877728.0</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>-0.713142</td>\n",
       "      <td>0.277004</td>\n",
       "      <td>0.243668</td>\n",
       "      <td>-0.126238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018106</td>\n",
       "      <td>548000.0</td>\n",
       "      <td>-0.247963</td>\n",
       "      <td>241000.0</td>\n",
       "      <td>-273000.0</td>\n",
       "      <td>268000.0</td>\n",
       "      <td>0.092653</td>\n",
       "      <td>76000.0</td>\n",
       "      <td>-0.478882</td>\n",
       "      <td>9786.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15.222200</td>\n",
       "      <td>226000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15877728.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>548000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>268000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9766.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.181700</td>\n",
       "      <td>198000.0</td>\n",
       "      <td>6.817322</td>\n",
       "      <td>-28000.0</td>\n",
       "      <td>17941621.0</td>\n",
       "      <td>-0.014980</td>\n",
       "      <td>0.445720</td>\n",
       "      <td>0.046166</td>\n",
       "      <td>-0.081190</td>\n",
       "      <td>-0.126236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262576</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>0.206636</td>\n",
       "      <td>-219000.0</td>\n",
       "      <td>77000.0</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>0.046329</td>\n",
       "      <td>-73000.0</td>\n",
       "      <td>3.352173</td>\n",
       "      <td>9767.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15.060300</td>\n",
       "      <td>161000.0</td>\n",
       "      <td>-5.843445</td>\n",
       "      <td>-37000.0</td>\n",
       "      <td>20051965.0</td>\n",
       "      <td>-0.037458</td>\n",
       "      <td>0.178284</td>\n",
       "      <td>0.507839</td>\n",
       "      <td>0.121881</td>\n",
       "      <td>0.168316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126758</td>\n",
       "      <td>3253000.0</td>\n",
       "      <td>0.909199</td>\n",
       "      <td>2924000.0</td>\n",
       "      <td>-124000.0</td>\n",
       "      <td>123000.0</td>\n",
       "      <td>-0.046329</td>\n",
       "      <td>-72000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>26.950001</td>\n",
       "      <td>811601.0</td>\n",
       "      <td>-45.000000</td>\n",
       "      <td>233579.0</td>\n",
       "      <td>25587617.0</td>\n",
       "      <td>0.424494</td>\n",
       "      <td>0.049999</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>631068.0</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>84152.0</td>\n",
       "      <td>246000.0</td>\n",
       "      <td>1654793.0</td>\n",
       "      <td>1.050001</td>\n",
       "      <td>852129.0</td>\n",
       "      <td>-0.800003</td>\n",
       "      <td>10610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>26.500000</td>\n",
       "      <td>471897.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>-339704.0</td>\n",
       "      <td>24146271.0</td>\n",
       "      <td>-0.471660</td>\n",
       "      <td>-0.450001</td>\n",
       "      <td>-0.649997</td>\n",
       "      <td>-0.450001</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>713926.0</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>82858.0</td>\n",
       "      <td>-339000.0</td>\n",
       "      <td>863020.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-791773.0</td>\n",
       "      <td>-2.400002</td>\n",
       "      <td>10719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>26.700001</td>\n",
       "      <td>448750.0</td>\n",
       "      <td>-45.000000</td>\n",
       "      <td>-23147.0</td>\n",
       "      <td>9743362.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450001</td>\n",
       "      <td>-0.100003</td>\n",
       "      <td>-0.049999</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060000</td>\n",
       "      <td>438662.0</td>\n",
       "      <td>0.700001</td>\n",
       "      <td>-275264.0</td>\n",
       "      <td>-206908.0</td>\n",
       "      <td>4617277.0</td>\n",
       "      <td>2.450001</td>\n",
       "      <td>3754257.0</td>\n",
       "      <td>4.400002</td>\n",
       "      <td>10749.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>26.549999</td>\n",
       "      <td>898636.0</td>\n",
       "      <td>-25.000000</td>\n",
       "      <td>449886.0</td>\n",
       "      <td>19206723.0</td>\n",
       "      <td>-0.188664</td>\n",
       "      <td>-0.049999</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>1246340.0</td>\n",
       "      <td>1.299999</td>\n",
       "      <td>807678.0</td>\n",
       "      <td>34908.0</td>\n",
       "      <td>3451285.0</td>\n",
       "      <td>-0.350001</td>\n",
       "      <td>-1165992.0</td>\n",
       "      <td>-5.300003</td>\n",
       "      <td>10833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>26.700001</td>\n",
       "      <td>524443.0</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>-374193.0</td>\n",
       "      <td>8046112.0</td>\n",
       "      <td>0.047167</td>\n",
       "      <td>-0.349998</td>\n",
       "      <td>0.049999</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>433616.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-812724.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>838497.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-2612788.0</td>\n",
       "      <td>1.300003</td>\n",
       "      <td>10894.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>26.799999</td>\n",
       "      <td>1001368.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>476925.0</td>\n",
       "      <td>13488401.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300004</td>\n",
       "      <td>0.049999</td>\n",
       "      <td>0.149999</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190000</td>\n",
       "      <td>811804.0</td>\n",
       "      <td>0.599998</td>\n",
       "      <td>378188.0</td>\n",
       "      <td>159505.0</td>\n",
       "      <td>553027.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-285470.0</td>\n",
       "      <td>-1.199997</td>\n",
       "      <td>10982.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>744703.0</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>-256665.0</td>\n",
       "      <td>15471557.0</td>\n",
       "      <td>0.235829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349998</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160000</td>\n",
       "      <td>830193.0</td>\n",
       "      <td>0.700005</td>\n",
       "      <td>18389.0</td>\n",
       "      <td>-131545.0</td>\n",
       "      <td>714415.0</td>\n",
       "      <td>-0.100001</td>\n",
       "      <td>161388.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>26.850000</td>\n",
       "      <td>685623.0</td>\n",
       "      <td>-65.000000</td>\n",
       "      <td>-59080.0</td>\n",
       "      <td>23098623.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.449996</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>677768.0</td>\n",
       "      <td>-0.300003</td>\n",
       "      <td>-152425.0</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>1037450.0</td>\n",
       "      <td>-0.649999</td>\n",
       "      <td>323035.0</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>10914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>1328553.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>642930.0</td>\n",
       "      <td>9595061.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.299999</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030000</td>\n",
       "      <td>1527715.0</td>\n",
       "      <td>-0.599998</td>\n",
       "      <td>849947.0</td>\n",
       "      <td>-13673.0</td>\n",
       "      <td>484398.0</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>-553052.0</td>\n",
       "      <td>-0.200005</td>\n",
       "      <td>10749.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>26.799999</td>\n",
       "      <td>1824392.0</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>495839.0</td>\n",
       "      <td>15217268.0</td>\n",
       "      <td>-0.094332</td>\n",
       "      <td>-1.049999</td>\n",
       "      <td>-0.300004</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>622568.0</td>\n",
       "      <td>-0.600003</td>\n",
       "      <td>-905147.0</td>\n",
       "      <td>229673.0</td>\n",
       "      <td>714552.0</td>\n",
       "      <td>-0.600001</td>\n",
       "      <td>230154.0</td>\n",
       "      <td>-5.900001</td>\n",
       "      <td>10817.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>26.799999</td>\n",
       "      <td>1846423.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>22031.0</td>\n",
       "      <td>14641631.0</td>\n",
       "      <td>-0.047166</td>\n",
       "      <td>-0.200001</td>\n",
       "      <td>-0.099998</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.149999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>429308.0</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>-193260.0</td>\n",
       "      <td>127200.0</td>\n",
       "      <td>1882483.0</td>\n",
       "      <td>-0.950000</td>\n",
       "      <td>1167931.0</td>\n",
       "      <td>-1.199997</td>\n",
       "      <td>10761.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>1301057.0</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>-545366.0</td>\n",
       "      <td>17251340.0</td>\n",
       "      <td>-0.141498</td>\n",
       "      <td>0.450001</td>\n",
       "      <td>0.299999</td>\n",
       "      <td>-0.049999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>458316.0</td>\n",
       "      <td>0.299999</td>\n",
       "      <td>29008.0</td>\n",
       "      <td>-333200.0</td>\n",
       "      <td>938282.0</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>-944201.0</td>\n",
       "      <td>-1.800003</td>\n",
       "      <td>10868.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>26.900000</td>\n",
       "      <td>967316.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>-333741.0</td>\n",
       "      <td>15853559.0</td>\n",
       "      <td>0.188664</td>\n",
       "      <td>1.049999</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.199999</td>\n",
       "      <td>0.049999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>321430.0</td>\n",
       "      <td>0.299999</td>\n",
       "      <td>-136886.0</td>\n",
       "      <td>77000.0</td>\n",
       "      <td>576417.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>-361865.0</td>\n",
       "      <td>2.300003</td>\n",
       "      <td>10887.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>26.900000</td>\n",
       "      <td>1155881.0</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>188565.0</td>\n",
       "      <td>11721091.0</td>\n",
       "      <td>-0.047166</td>\n",
       "      <td>0.400002</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>838091.0</td>\n",
       "      <td>1.300003</td>\n",
       "      <td>516661.0</td>\n",
       "      <td>-138000.0</td>\n",
       "      <td>498475.0</td>\n",
       "      <td>0.299999</td>\n",
       "      <td>-77942.0</td>\n",
       "      <td>0.900002</td>\n",
       "      <td>10953.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>26.900000</td>\n",
       "      <td>703069.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-452812.0</td>\n",
       "      <td>12948424.0</td>\n",
       "      <td>0.141498</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049999</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1209613.0</td>\n",
       "      <td>0.599999</td>\n",
       "      <td>371522.0</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>547553.0</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>49078.0</td>\n",
       "      <td>1.099998</td>\n",
       "      <td>10744.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>939428.0</td>\n",
       "      <td>-110.000000</td>\n",
       "      <td>236359.0</td>\n",
       "      <td>23235451.0</td>\n",
       "      <td>-0.235829</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>-0.649997</td>\n",
       "      <td>-0.200001</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>1215191.0</td>\n",
       "      <td>0.199996</td>\n",
       "      <td>5578.0</td>\n",
       "      <td>56000.0</td>\n",
       "      <td>987104.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>439551.0</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>10821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>26.549999</td>\n",
       "      <td>942202.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>2774.0</td>\n",
       "      <td>13139503.0</td>\n",
       "      <td>0.047166</td>\n",
       "      <td>-0.049999</td>\n",
       "      <td>1.849998</td>\n",
       "      <td>0.400002</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1665096.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>449905.0</td>\n",
       "      <td>48164.0</td>\n",
       "      <td>1229060.0</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>241956.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>26.900000</td>\n",
       "      <td>561002.0</td>\n",
       "      <td>-25.000000</td>\n",
       "      <td>-381200.0</td>\n",
       "      <td>23251680.0</td>\n",
       "      <td>0.141497</td>\n",
       "      <td>1.299999</td>\n",
       "      <td>0.950001</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>1173693.0</td>\n",
       "      <td>0.200005</td>\n",
       "      <td>-491403.0</td>\n",
       "      <td>306836.0</td>\n",
       "      <td>889251.0</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-339809.0</td>\n",
       "      <td>-0.599998</td>\n",
       "      <td>10973.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>26.900000</td>\n",
       "      <td>1352202.0</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>791200.0</td>\n",
       "      <td>18489721.0</td>\n",
       "      <td>0.047166</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>1.049999</td>\n",
       "      <td>0.349998</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>577668.0</td>\n",
       "      <td>-0.100006</td>\n",
       "      <td>-596025.0</td>\n",
       "      <td>210436.0</td>\n",
       "      <td>762791.0</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>-126460.0</td>\n",
       "      <td>0.900001</td>\n",
       "      <td>10911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>1535897.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>183695.0</td>\n",
       "      <td>24566305.0</td>\n",
       "      <td>-0.047166</td>\n",
       "      <td>-0.450001</td>\n",
       "      <td>-0.849998</td>\n",
       "      <td>-0.449999</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>684194.0</td>\n",
       "      <td>-0.899994</td>\n",
       "      <td>106526.0</td>\n",
       "      <td>-541914.0</td>\n",
       "      <td>1027349.0</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>264558.0</td>\n",
       "      <td>-2.200005</td>\n",
       "      <td>10919.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>26.650000</td>\n",
       "      <td>6815730.0</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>5279833.0</td>\n",
       "      <td>65473348.0</td>\n",
       "      <td>0.047166</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>1393645.0</td>\n",
       "      <td>1.699997</td>\n",
       "      <td>709451.0</td>\n",
       "      <td>-113522.0</td>\n",
       "      <td>1361100.0</td>\n",
       "      <td>0.550001</td>\n",
       "      <td>333751.0</td>\n",
       "      <td>0.700005</td>\n",
       "      <td>11057.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>1032411.0</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-5783319.0</td>\n",
       "      <td>24763639.0</td>\n",
       "      <td>0.141498</td>\n",
       "      <td>0.950001</td>\n",
       "      <td>0.599998</td>\n",
       "      <td>0.299999</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2203520.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>809875.0</td>\n",
       "      <td>62000.0</td>\n",
       "      <td>451729.0</td>\n",
       "      <td>-0.100001</td>\n",
       "      <td>-909371.0</td>\n",
       "      <td>1.899994</td>\n",
       "      <td>11107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>974377.0</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>-58034.0</td>\n",
       "      <td>27310317.0</td>\n",
       "      <td>0.141498</td>\n",
       "      <td>0.049999</td>\n",
       "      <td>-0.149997</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>1450623.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-752897.0</td>\n",
       "      <td>73000.0</td>\n",
       "      <td>477030.0</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>25301.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11283.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>27.799999</td>\n",
       "      <td>934209.0</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>-40168.0</td>\n",
       "      <td>38923943.0</td>\n",
       "      <td>0.094332</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>1172014.0</td>\n",
       "      <td>0.900002</td>\n",
       "      <td>-278609.0</td>\n",
       "      <td>472860.0</td>\n",
       "      <td>1133250.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>656220.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11358.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>27.799999</td>\n",
       "      <td>1053542.0</td>\n",
       "      <td>-90.000000</td>\n",
       "      <td>119333.0</td>\n",
       "      <td>29718283.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060000</td>\n",
       "      <td>856469.0</td>\n",
       "      <td>-0.200005</td>\n",
       "      <td>-315545.0</td>\n",
       "      <td>-450638.0</td>\n",
       "      <td>465006.0</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>-668244.0</td>\n",
       "      <td>0.600006</td>\n",
       "      <td>11457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>27.900000</td>\n",
       "      <td>620428.0</td>\n",
       "      <td>-30.000000</td>\n",
       "      <td>-433114.0</td>\n",
       "      <td>28718273.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>675764.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-180705.0</td>\n",
       "      <td>-111222.0</td>\n",
       "      <td>1289886.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>824880.0</td>\n",
       "      <td>0.699997</td>\n",
       "      <td>11571.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>474621.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>-145807.0</td>\n",
       "      <td>39305131.0</td>\n",
       "      <td>0.094333</td>\n",
       "      <td>0.450001</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1224209.0</td>\n",
       "      <td>0.800003</td>\n",
       "      <td>548445.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>852600.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-437286.0</td>\n",
       "      <td>-0.400002</td>\n",
       "      <td>11610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>27.950001</td>\n",
       "      <td>622418.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147797.0</td>\n",
       "      <td>41393105.0</td>\n",
       "      <td>-0.047167</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030000</td>\n",
       "      <td>955433.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-268776.0</td>\n",
       "      <td>14960.0</td>\n",
       "      <td>498267.0</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-354333.0</td>\n",
       "      <td>-0.599998</td>\n",
       "      <td>11706.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>28.100000</td>\n",
       "      <td>813983.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>191565.0</td>\n",
       "      <td>73457321.0</td>\n",
       "      <td>0.094333</td>\n",
       "      <td>0.599998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.299999</td>\n",
       "      <td>-0.100001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>911740.0</td>\n",
       "      <td>0.800003</td>\n",
       "      <td>-43693.0</td>\n",
       "      <td>79840.0</td>\n",
       "      <td>787657.0</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>289390.0</td>\n",
       "      <td>-0.599998</td>\n",
       "      <td>11491.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>661000.0</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>-152983.0</td>\n",
       "      <td>103319000.0</td>\n",
       "      <td>-0.200001</td>\n",
       "      <td>-1.299999</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.799999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120000</td>\n",
       "      <td>1685000.0</td>\n",
       "      <td>-2.900001</td>\n",
       "      <td>773260.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>2491000.0</td>\n",
       "      <td>0.299999</td>\n",
       "      <td>1703343.0</td>\n",
       "      <td>-3.300004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>835 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Low_2884  Volume_6415   價格漲幅_6415  成交量變動_6415  Volume_2890  價格漲幅_2890  \\\n",
       "0    14.857800     626000.0   17.530304    423000.0    6583345.0  -0.022478   \n",
       "1    14.898300     171000.0    5.843475   -455000.0   12639896.0   0.067425   \n",
       "2    14.898300     506000.0    1.947784    335000.0   10379642.0   0.029969   \n",
       "3    14.776900     200000.0  -11.199920   -306000.0    8408650.0  -0.029969   \n",
       "4    14.736400      78000.0   -1.460846   -122000.0    8668352.0  -0.029967   \n",
       "5    14.776900     155000.0    6.817352     77000.0    9286992.0   0.044956   \n",
       "6    14.857800     153000.0   -2.921722     -2000.0   14103096.0   0.022470   \n",
       "7    14.857800     156000.0   -8.765168      3000.0    8483605.0  -0.014980   \n",
       "8    14.817400     194000.0   -5.843444     38000.0    8759142.0   0.000000   \n",
       "9    14.857800      81000.0    6.330413   -113000.0    6391207.0   0.007490   \n",
       "10   14.857800     234000.0   10.712952    153000.0    9353502.0  -0.037459   \n",
       "11   14.776900     139000.0   -1.460876    -95000.0   11534578.0  -0.029967   \n",
       "12   14.776900     269000.0   -0.486939    130000.0    7953643.0   0.014980   \n",
       "13   14.979300     137000.0   -8.278198   -132000.0   15221082.0   0.074924   \n",
       "14   14.898300     148000.0    3.895630     11000.0   14719625.0   0.014980   \n",
       "15   14.776900     476000.0    9.739044    328000.0   24740329.0  -0.007490   \n",
       "16   14.776900     111000.0    4.869507   -365000.0   21384259.0  -0.037458   \n",
       "17   14.817400     170000.0    4.869568     59000.0   21882549.0   0.052438   \n",
       "18   14.776900     589000.0   14.608612    419000.0   19173623.0   0.044956   \n",
       "19   14.776900     333000.0    6.817322   -256000.0   20684330.0   0.029968   \n",
       "20   14.817400     180000.0  -16.556397   -153000.0   24039344.0   0.052439   \n",
       "21   14.898300     220000.0   -9.252136     40000.0   39132687.0   0.067433   \n",
       "22   15.060300     311000.0   -5.356476     91000.0   20054077.0   0.022470   \n",
       "23   15.060300     273000.0   -2.921692    -38000.0   21390593.0  -0.022470   \n",
       "24   14.938800     206000.0    3.408661    -67000.0   26350272.0   0.029968   \n",
       "25   15.343700     738000.0   18.991180    532000.0   26167635.0   0.014981   \n",
       "26   15.222200     226000.0    4.869538   -512000.0   15877728.0   0.007489   \n",
       "27   15.222200     226000.0    0.000000         0.0   15877728.0   0.000000   \n",
       "28   15.181700     198000.0    6.817322    -28000.0   17941621.0  -0.014980   \n",
       "29   15.060300     161000.0   -5.843445    -37000.0   20051965.0  -0.037458   \n",
       "..         ...          ...         ...         ...          ...        ...   \n",
       "805  26.950001     811601.0  -45.000000    233579.0   25587617.0   0.424494   \n",
       "806  26.500000     471897.0   20.000000   -339704.0   24146271.0  -0.471660   \n",
       "807  26.700001     448750.0  -45.000000    -23147.0    9743362.0   0.000000   \n",
       "808  26.549999     898636.0  -25.000000    449886.0   19206723.0  -0.188664   \n",
       "809  26.700001     524443.0   55.000000   -374193.0    8046112.0   0.047167   \n",
       "810  26.799999    1001368.0   80.000000    476925.0   13488401.0   0.000000   \n",
       "811  27.000000     744703.0   85.000000   -256665.0   15471557.0   0.235829   \n",
       "812  26.850000     685623.0  -65.000000    -59080.0   23098623.0   0.000000   \n",
       "813  26.750000    1328553.0   25.000000    642930.0    9595061.0   0.000000   \n",
       "814  26.799999    1824392.0   45.000000    495839.0   15217268.0  -0.094332   \n",
       "815  26.799999    1846423.0  100.000000     22031.0   14641631.0  -0.047166   \n",
       "816  26.750000    1301057.0  -20.000000   -545366.0   17251340.0  -0.141498   \n",
       "817  26.900000     967316.0   35.000000   -333741.0   15853559.0   0.188664   \n",
       "818  26.900000    1155881.0   -5.000000    188565.0   11721091.0  -0.047166   \n",
       "819  26.900000     703069.0    0.000000   -452812.0   12948424.0   0.141498   \n",
       "820  26.750000     939428.0 -110.000000    236359.0   23235451.0  -0.235829   \n",
       "821  26.549999     942202.0  105.000000      2774.0   13139503.0   0.047166   \n",
       "822  26.900000     561002.0  -25.000000   -381200.0   23251680.0   0.141497   \n",
       "823  26.900000    1352202.0   55.000000    791200.0   18489721.0   0.047166   \n",
       "824  26.750000    1535897.0   40.000000    183695.0   24566305.0  -0.047166   \n",
       "825  26.650000    6815730.0   75.000000   5279833.0   65473348.0   0.047166   \n",
       "826  26.750000    1032411.0  -70.000000  -5783319.0   24763639.0   0.141498   \n",
       "827  27.000000     974377.0   30.000000    -58034.0   27310317.0   0.141498   \n",
       "828  27.799999     934209.0  110.000000    -40168.0   38923943.0   0.094332   \n",
       "829  27.799999    1053542.0  -90.000000    119333.0   29718283.0   0.000000   \n",
       "830  27.900000     620428.0  -30.000000   -433114.0   28718273.0   0.000000   \n",
       "831  28.000000     474621.0   70.000000   -145807.0   39305131.0   0.094333   \n",
       "832  27.950001     622418.0    0.000000    147797.0   41393105.0  -0.047167   \n",
       "833  28.100000     813983.0    5.000000    191565.0   73457321.0   0.094333   \n",
       "834  28.000000     661000.0  -40.000000   -152983.0  103319000.0  -0.200001   \n",
       "\n",
       "     價格漲幅_2105  價格漲幅_2633  價格漲幅_2823  價格漲幅_2324  ...  價格漲幅_1714  Volume_2480  \\\n",
       "0     0.267430  -0.046169   0.000000   0.000000  ...   0.000000     329000.0   \n",
       "1     0.000000   0.369341   0.081284   0.126235  ...  -0.027159     148000.0   \n",
       "2     0.802292   0.184668  -0.446740  -0.042077  ...   0.000000     243000.0   \n",
       "3     0.534858  -0.138502   0.203072  -0.084158  ...   0.054326     211000.0   \n",
       "4    -0.802291   0.046165   0.040596   0.042079  ...  -0.027167     189000.0   \n",
       "5     1.337154  -0.046165  -0.121881  -0.042079  ...   0.036219     313000.0   \n",
       "6     0.000000   0.046165  -0.527930  -0.126237  ...   0.190144     146000.0   \n",
       "7    -0.178291   0.046169  -0.731096   0.210395  ...   0.000000     341000.0   \n",
       "8    -0.534859   0.000000  -0.284264   0.210395  ...  -0.280689     118000.0   \n",
       "9     0.267429  -0.046169   0.121787   0.000000  ...   0.027167     148000.0   \n",
       "10   -0.802284  -0.138498   0.365549  -0.042077  ...   0.009053      81000.0   \n",
       "11    0.089138  -0.138502  -0.162478  -0.042081  ...  -0.036220     111000.0   \n",
       "12   -0.624000   0.000000   0.203074  -0.042077  ...   0.018105     126000.0   \n",
       "13    0.534862   0.092332   0.121881   0.042077  ...   0.027167     149000.0   \n",
       "14   -0.178291   0.000000   0.081190  -0.042077  ...   0.072432      93000.0   \n",
       "15   -0.624005  -0.092332  -0.487335  -0.210399  ...   0.208259      99000.0   \n",
       "16    0.534859   0.092332   0.162477   0.000000  ...   0.018105     190000.0   \n",
       "17    0.178288   0.092337   0.609215   0.084160  ...   0.162977     227000.0   \n",
       "18    1.248012   0.092331   0.081192   0.042079  ...   0.000000     231000.0   \n",
       "19   -0.802296   0.323172   0.081190  -0.042079  ...  -0.054327     236000.0   \n",
       "20    0.713147   0.000000  -0.609216  -0.084160  ...   0.045274     129000.0   \n",
       "21    0.713153  -0.277003  -0.243668   0.126239  ...   0.144871     337000.0   \n",
       "22    0.267426   0.046168  -0.081190   0.168318  ...   0.126766    1594000.0   \n",
       "23    1.069725   0.092333  -0.243668   0.000000  ...  -0.018115    1951000.0   \n",
       "24    0.891430   0.046167  -0.243763   0.168317  ...   0.009052     578000.0   \n",
       "25   -0.713150   0.046169   0.568621   0.168316  ...  -0.045263     307000.0   \n",
       "26   -0.713142   0.277004   0.243668  -0.126238  ...   0.018106     548000.0   \n",
       "27    0.000000   0.000000   0.000000   0.000000  ...   0.000000     548000.0   \n",
       "28    0.445720   0.046166  -0.081190  -0.126236  ...  -0.262576     329000.0   \n",
       "29    0.178284   0.507839   0.121881   0.168316  ...   0.126758    3253000.0   \n",
       "..         ...        ...        ...        ...  ...        ...          ...   \n",
       "805   0.049999   0.099998   0.350000   0.000000  ...  -0.040000     631068.0   \n",
       "806  -0.450001  -0.649997  -0.450001  -0.500000  ...   0.000000     713926.0   \n",
       "807   0.450001  -0.100003  -0.049999   0.050000  ...  -0.060000     438662.0   \n",
       "808  -0.049999   0.050004  -0.100000   0.000000  ...   0.290000    1246340.0   \n",
       "809  -0.349998   0.049999   0.050001   0.050001  ...   0.060000     433616.0   \n",
       "810  -0.300004   0.049999   0.149999   0.099998  ...  -0.190000     811804.0   \n",
       "811   0.000000   0.349998   0.550000   0.000000  ...  -0.160000     830193.0   \n",
       "812  -0.500000  -0.449996  -0.150000   0.000000  ...  -0.010000     677768.0   \n",
       "813  -0.299999   0.500000  -0.100000   0.200001  ...  -0.030000    1527715.0   \n",
       "814  -1.049999  -0.300004  -0.400000  -0.400000  ...  -0.230000     622568.0   \n",
       "815  -0.200001  -0.099998  -0.250000  -0.149999  ...   0.000000     429308.0   \n",
       "816   0.450001   0.299999  -0.049999   0.000000  ...   0.210000     458316.0   \n",
       "817   1.049999   0.750000   0.199999   0.049999  ...   0.040000     321430.0   \n",
       "818   0.400002   0.549999   0.050001   0.000000  ...  -0.040000     838091.0   \n",
       "819   0.099998   1.000000   0.049999   0.100000  ...   0.060000    1209613.0   \n",
       "820  -1.250000  -0.649997  -0.200001  -0.200000  ...  -0.150000    1215191.0   \n",
       "821  -0.049999   1.849998   0.400002   0.050001  ...   0.020000    1665096.0   \n",
       "822   1.299999   0.950001   0.500000   0.099998  ...  -0.010000    1173693.0   \n",
       "823   0.200001   1.049999   0.349998   0.100001  ...   0.140000     577668.0   \n",
       "824  -0.450001  -0.849998  -0.449999  -0.250000  ...  -0.090000     684194.0   \n",
       "825  -0.750000  -0.750000   0.150000   0.700000  ...   0.140000    1393645.0   \n",
       "826   0.950001   0.599998   0.299999  -0.200000  ...   0.000000    2203520.0   \n",
       "827   0.049999  -0.149997   0.200001   0.100000  ...  -0.080000    1450623.0   \n",
       "828   0.799999   0.250000   0.549999   0.000000  ...   0.070000    1172014.0   \n",
       "829   0.150002   0.000000   0.000000   0.049999  ...  -0.060000     856469.0   \n",
       "830   0.000000   0.250000   0.200001   0.050001  ...   0.080000     675764.0   \n",
       "831   0.450001   0.099998   0.400000   0.100001  ...   0.040000    1224209.0   \n",
       "832   0.200001  -0.250000   0.100000   0.000000  ...  -0.030000     955433.0   \n",
       "833   0.599998   0.000000   0.299999  -0.100001  ...  -0.020000     911740.0   \n",
       "834  -1.299999  -0.500000  -0.799999   0.000000  ...  -0.120000    1685000.0   \n",
       "\n",
       "     價格漲幅_2480  成交量變動_2480  成交量變動_1321  Volume_1519  價格漲幅_1519  成交量變動_1519  \\\n",
       "0     0.371946    268000.0     45000.0     176000.0   0.092657    -22000.0   \n",
       "1    -0.041328   -181000.0    -81000.0     351000.0   0.046328    175000.0   \n",
       "2     0.000000     95000.0     42000.0     731000.0   0.138985    380000.0   \n",
       "3    -0.041327    -32000.0    -43000.0     302000.0  -0.092657   -429000.0   \n",
       "4     0.000000    -22000.0     12000.0     220000.0  -0.092656    -82000.0   \n",
       "5     0.041327    124000.0    -30000.0     795000.0   0.000000    575000.0   \n",
       "6     0.000000   -167000.0     16000.0     574000.0   0.000000   -221000.0   \n",
       "7     0.082655    195000.0     15000.0     172000.0  -0.092657   -402000.0   \n",
       "8    -0.082655   -223000.0    -27000.0     228000.0  -0.185310     56000.0   \n",
       "9    -0.165307     30000.0     21000.0     271000.0   0.046328     43000.0   \n",
       "10    0.165307    -67000.0     75000.0     272000.0   0.046329      1000.0   \n",
       "11    0.041328     30000.0    -47000.0     154000.0  -0.092657   -118000.0   \n",
       "12    0.082655     15000.0     39000.0     167000.0   0.000000     13000.0   \n",
       "13   -0.123983     23000.0    430000.0     311000.0   0.138985    144000.0   \n",
       "14    0.041328    -56000.0   -347000.0     325000.0   0.231638     14000.0   \n",
       "15    0.041327      6000.0    532000.0    1676000.0   0.416952   1351000.0   \n",
       "16    0.000000     91000.0   -162000.0     663000.0  -0.231642  -1013000.0   \n",
       "17    0.123981     37000.0    795000.0     700000.0   0.277967     37000.0   \n",
       "18    0.082657      4000.0  -1007000.0     339000.0  -0.046325   -361000.0   \n",
       "19    0.082653      5000.0    -77000.0     410000.0  -0.046332     71000.0   \n",
       "20   -0.041327   -107000.0    118000.0     388000.0  -0.185310    -22000.0   \n",
       "21   -0.165310    208000.0   -159000.0     218000.0   0.000000   -170000.0   \n",
       "22    0.909203   1257000.0      7000.0    1216000.0  -0.463276    998000.0   \n",
       "23    0.206635    357000.0     78000.0     359000.0   0.000000   -857000.0   \n",
       "24   -0.165310  -1373000.0    -69000.0     301000.0   0.138981    -58000.0   \n",
       "25    0.082655   -271000.0    135000.0     192000.0  -0.092653   -109000.0   \n",
       "26   -0.247963    241000.0   -273000.0     268000.0   0.092653     76000.0   \n",
       "27    0.000000         0.0         0.0     268000.0   0.000000         0.0   \n",
       "28    0.206636   -219000.0     77000.0     195000.0   0.046329    -73000.0   \n",
       "29    0.909199   2924000.0   -124000.0     123000.0  -0.046329    -72000.0   \n",
       "..         ...         ...         ...          ...        ...         ...   \n",
       "805   0.200001     84152.0    246000.0    1654793.0   1.050001    852129.0   \n",
       "806   0.099998     82858.0   -339000.0     863020.0  -0.600000   -791773.0   \n",
       "807   0.700001   -275264.0   -206908.0    4617277.0   2.450001   3754257.0   \n",
       "808   1.299999    807678.0     34908.0    3451285.0  -0.350001  -1165992.0   \n",
       "809   0.000000   -812724.0        40.0     838497.0   0.050000  -2612788.0   \n",
       "810   0.599998    378188.0    159505.0     553027.0   0.000000   -285470.0   \n",
       "811   0.700005     18389.0   -131545.0     714415.0  -0.100001    161388.0   \n",
       "812  -0.300003   -152425.0     -1000.0    1037450.0  -0.649999    323035.0   \n",
       "813  -0.599998    849947.0    -13673.0     484398.0   0.050001   -553052.0   \n",
       "814  -0.600003   -905147.0    229673.0     714552.0  -0.600001    230154.0   \n",
       "815   0.100003   -193260.0    127200.0    1882483.0  -0.950000   1167931.0   \n",
       "816   0.299999     29008.0   -333200.0     938282.0  -0.050000   -944201.0   \n",
       "817   0.299999   -136886.0     77000.0     576417.0   0.150000   -361865.0   \n",
       "818   1.300003    516661.0   -138000.0     498475.0   0.299999    -77942.0   \n",
       "819   0.599999    371522.0     32000.0     547553.0   0.200001     49078.0   \n",
       "820   0.199996      5578.0     56000.0     987104.0  -1.000000    439551.0   \n",
       "821   2.500000    449905.0     48164.0    1229060.0  -0.400000    241956.0   \n",
       "822   0.200005   -491403.0    306836.0     889251.0  -0.100000   -339809.0   \n",
       "823  -0.100006   -596025.0    210436.0     762791.0   0.799999   -126460.0   \n",
       "824  -0.899994    106526.0   -541914.0    1027349.0   0.100001    264558.0   \n",
       "825   1.699997    709451.0   -113522.0    1361100.0   0.550001    333751.0   \n",
       "826   3.000000    809875.0     62000.0     451729.0  -0.100001   -909371.0   \n",
       "827   1.000000   -752897.0     73000.0     477030.0  -0.450000     25301.0   \n",
       "828   0.900002   -278609.0    472860.0    1133250.0   0.250000    656220.0   \n",
       "829  -0.200005   -315545.0   -450638.0     465006.0  -0.050000   -668244.0   \n",
       "830   0.500000   -180705.0   -111222.0    1289886.0   0.750000    824880.0   \n",
       "831   0.800003    548445.0     16200.0     852600.0   0.000000   -437286.0   \n",
       "832  -1.000000   -268776.0     14960.0     498267.0  -0.200000   -354333.0   \n",
       "833   0.800003    -43693.0     79840.0     787657.0  -0.150000    289390.0   \n",
       "834  -2.900001    773260.0     21000.0    2491000.0   0.299999   1703343.0   \n",
       "\n",
       "     價格漲幅_3530   Target  \n",
       "0    -1.436646   9342.0  \n",
       "1    -0.383110   9365.0  \n",
       "2    -0.095772   9342.0  \n",
       "3     2.873291   9341.0  \n",
       "4    -0.469299   9337.0  \n",
       "5     5.669960   9411.0  \n",
       "6    -3.754433   9375.0  \n",
       "7     0.000000   9285.0  \n",
       "8    -0.469314   9340.0  \n",
       "9     0.938614   9340.0  \n",
       "10   -0.478890   9304.0  \n",
       "11   -2.375259   9330.0  \n",
       "12   -0.019142   9407.0  \n",
       "13    3.198928   9442.0  \n",
       "14   -3.103157   9375.0  \n",
       "15    0.861992   9437.0  \n",
       "16   -0.957763   9505.0  \n",
       "17    0.000000   9520.0  \n",
       "18   -0.871567   9515.0  \n",
       "19   -0.038315   9574.0  \n",
       "20    1.388763   9657.0  \n",
       "21   -0.957763   9704.0  \n",
       "22    0.469299   9708.0  \n",
       "23    0.009583   9799.0  \n",
       "24   -0.478882   9770.0  \n",
       "25   -0.478882   9763.0  \n",
       "26   -0.478882   9786.0  \n",
       "27    0.000000   9766.0  \n",
       "28    3.352173   9767.0  \n",
       "29    0.000000   9783.0  \n",
       "..         ...      ...  \n",
       "805  -0.800003  10610.0  \n",
       "806  -2.400002  10719.0  \n",
       "807   4.400002  10749.0  \n",
       "808  -5.300003  10833.0  \n",
       "809   1.300003  10894.0  \n",
       "810  -1.199997  10982.0  \n",
       "811   0.500000  10865.0  \n",
       "812  -1.500000  10914.0  \n",
       "813  -0.200005  10749.0  \n",
       "814  -5.900001  10817.0  \n",
       "815  -1.199997  10761.0  \n",
       "816  -1.800003  10868.0  \n",
       "817   2.300003  10887.0  \n",
       "818   0.900002  10953.0  \n",
       "819   1.099998  10744.0  \n",
       "820  -3.500000  10821.0  \n",
       "821   2.000000  10984.0  \n",
       "822  -0.599998  10973.0  \n",
       "823   0.900001  10911.0  \n",
       "824  -2.200005  10919.0  \n",
       "825   0.700005  11057.0  \n",
       "826   1.899994  11107.0  \n",
       "827   0.000000  11283.0  \n",
       "828   1.000000  11358.0  \n",
       "829   0.600006  11457.0  \n",
       "830   0.699997  11571.0  \n",
       "831  -0.400002  11610.0  \n",
       "832  -0.599998  11706.0  \n",
       "833  -0.599998  11491.0  \n",
       "834  -3.300004      NaN  \n",
       "\n",
       "[835 rows x 101 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feature= Significant_factor_test\n",
    "all_feature = all_feature.drop(columns=[\"Date\"])\n",
    "all_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastDay=30, futureDay=5):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "        Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"Target\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X,Y,rate):\n",
    "    X_train = X[int(X.shape[0]*rate):]\n",
    "    Y_train = Y[int(Y.shape[0]*rate):]\n",
    "    X_val = X[:int(X.shape[0]*rate)]\n",
    "    Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shuffle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-269-a57907ab876f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# shuffle the data, and random seed is 10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# split training data and validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shuffle' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# build Data, use last 30 days to predict next 5 days\n",
    "X_train, Y_train = buildTrain(all_feature, 30, 5)\n",
    "\n",
    "# split training data and validation data\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "# X_trian: (5710, 30, 10)\n",
    "# Y_train: (5710, 5, 1)\n",
    "# X_val: (634, 30, 10)\n",
    "# Y_val: (634, 5, 1)\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"Y_train\",Y_train.shape)\n",
    "print(\"X_val\",X_val.shape)\n",
    "print(\"Y_val\",Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildOneToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(250, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "    model.add(LSTM(100,return_sequences=True))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
    "    model.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 5)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 250)            352000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 1, 100)            140400    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 1, 1)              101       \n",
      "=================================================================\n",
      "Total params: 492,501\n",
      "Trainable params: 492,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 750 samples, validate on 83 samples\n",
      "Epoch 1/10000\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 10663.0970 - val_loss: 9716.2041\n",
      "Epoch 2/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10662.4953 - val_loss: 9715.4033\n",
      "Epoch 3/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10661.7781 - val_loss: 9714.4453\n",
      "Epoch 4/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10660.9246 - val_loss: 9713.3467\n",
      "Epoch 5/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10659.9425 - val_loss: 9712.1338\n",
      "Epoch 6/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 10658.8535 - val_loss: 9710.8672\n",
      "Epoch 7/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10657.6985 - val_loss: 9709.5762\n",
      "Epoch 8/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10656.5039 - val_loss: 9708.3008\n",
      "Epoch 9/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10655.3040 - val_loss: 9707.0713\n",
      "Epoch 10/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10654.1246 - val_loss: 9705.8965\n",
      "Epoch 11/10000\n",
      "750/750 [==============================] - 0s 203us/step - loss: 10652.9797 - val_loss: 9704.7881\n",
      "Epoch 12/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 10651.8804 - val_loss: 9703.7383\n",
      "Epoch 13/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10650.8311 - val_loss: 9702.7549\n",
      "Epoch 14/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10649.8317 - val_loss: 9701.8311\n",
      "Epoch 15/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10648.8772 - val_loss: 9700.9541\n",
      "Epoch 16/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10647.9680 - val_loss: 9700.1133\n",
      "Epoch 17/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10647.0974 - val_loss: 9699.3135\n",
      "Epoch 18/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10646.2604 - val_loss: 9698.5391\n",
      "Epoch 19/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10645.4542 - val_loss: 9697.7920\n",
      "Epoch 20/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 10644.6733 - val_loss: 9697.0645\n",
      "Epoch 21/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 10643.9154 - val_loss: 9696.3545\n",
      "Epoch 22/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10643.1769 - val_loss: 9695.6611\n",
      "Epoch 23/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10642.4567 - val_loss: 9694.9814\n",
      "Epoch 24/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10641.7514 - val_loss: 9694.3135\n",
      "Epoch 25/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 10641.0634 - val_loss: 9693.6592\n",
      "Epoch 26/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 10640.3882 - val_loss: 9693.0166\n",
      "Epoch 27/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10639.7238 - val_loss: 9692.3809\n",
      "Epoch 28/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10639.0707 - val_loss: 9691.7539\n",
      "Epoch 29/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10638.4273 - val_loss: 9691.1348\n",
      "Epoch 30/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 10637.7922 - val_loss: 9690.5244\n",
      "Epoch 31/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10637.1650 - val_loss: 9689.9180\n",
      "Epoch 32/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10636.5465 - val_loss: 9689.3193\n",
      "Epoch 33/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10635.9335 - val_loss: 9688.7256\n",
      "Epoch 34/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 10635.3280 - val_loss: 9688.1367\n",
      "Epoch 35/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10634.7271 - val_loss: 9687.5518\n",
      "Epoch 36/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 10634.1330 - val_loss: 9686.9727\n",
      "Epoch 37/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10633.5427 - val_loss: 9686.3984\n",
      "Epoch 38/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10632.9576 - val_loss: 9685.8271\n",
      "Epoch 39/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10632.3773 - val_loss: 9685.2588\n",
      "Epoch 40/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10631.8007 - val_loss: 9684.6953\n",
      "Epoch 41/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 10631.2282 - val_loss: 9684.1338\n",
      "Epoch 42/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10630.6592 - val_loss: 9683.5762\n",
      "Epoch 43/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 10630.0932 - val_loss: 9683.0215\n",
      "Epoch 44/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 10629.5314 - val_loss: 9682.4688\n",
      "Epoch 45/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10628.9718 - val_loss: 9681.9189\n",
      "Epoch 46/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10628.4152 - val_loss: 9681.3711\n",
      "Epoch 47/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10627.8615 - val_loss: 9680.8271\n",
      "Epoch 48/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10627.3102 - val_loss: 9680.2842\n",
      "Epoch 49/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10626.7622 - val_loss: 9679.7441\n",
      "Epoch 50/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10626.2158 - val_loss: 9679.2061\n",
      "Epoch 51/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10625.6721 - val_loss: 9678.6699\n",
      "Epoch 52/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10625.1305 - val_loss: 9678.1348\n",
      "Epoch 53/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10624.5905 - val_loss: 9677.6025\n",
      "Epoch 54/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10624.0527 - val_loss: 9677.0703\n",
      "Epoch 55/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10623.5167 - val_loss: 9676.5400\n",
      "Epoch 56/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10622.9824 - val_loss: 9676.0127\n",
      "Epoch 57/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10622.4503 - val_loss: 9675.4854\n",
      "Epoch 58/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10621.9193 - val_loss: 9674.9600\n",
      "Epoch 59/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10621.3900 - val_loss: 9674.4375\n",
      "Epoch 60/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10620.8626 - val_loss: 9673.9141\n",
      "Epoch 61/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10620.3362 - val_loss: 9673.3926\n",
      "Epoch 62/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 10619.8109 - val_loss: 9672.8730\n",
      "Epoch 63/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10619.2875 - val_loss: 9672.3545\n",
      "Epoch 64/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 10618.7655 - val_loss: 9671.8359\n",
      "Epoch 65/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10618.2445 - val_loss: 9671.3193\n",
      "Epoch 66/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 10617.7249 - val_loss: 9670.8037\n",
      "Epoch 67/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 10617.2060 - val_loss: 9670.2900\n",
      "Epoch 68/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10616.6887 - val_loss: 9669.7754\n",
      "Epoch 69/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10616.1721 - val_loss: 9669.2637\n",
      "Epoch 70/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10615.6562 - val_loss: 9668.7520\n",
      "Epoch 71/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10615.1423 - val_loss: 9668.2412\n",
      "Epoch 72/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10614.6286 - val_loss: 9667.7305\n",
      "Epoch 73/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10614.1164 - val_loss: 9667.2217\n",
      "Epoch 74/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10613.6050 - val_loss: 9666.7139\n",
      "Epoch 75/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 10613.0940 - val_loss: 9666.2061\n",
      "Epoch 76/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10612.5844 - val_loss: 9665.6992\n",
      "Epoch 77/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 10612.0753 - val_loss: 9665.1943\n",
      "Epoch 78/10000\n",
      "750/750 [==============================] - 0s 103us/step - loss: 10611.5674 - val_loss: 9664.6885\n",
      "Epoch 79/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10611.0601 - val_loss: 9664.1846\n",
      "Epoch 80/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10610.5537 - val_loss: 9663.6807\n",
      "Epoch 81/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10610.0475 - val_loss: 9663.1777\n",
      "Epoch 82/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10609.5424 - val_loss: 9662.6748\n",
      "Epoch 83/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10609.0382 - val_loss: 9662.1729\n",
      "Epoch 84/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10608.5346 - val_loss: 9661.6709\n",
      "Epoch 85/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 10608.0315 - val_loss: 9661.1719\n",
      "Epoch 86/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10607.5289 - val_loss: 9660.6709\n",
      "Epoch 87/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10607.0275 - val_loss: 9660.1719\n",
      "Epoch 88/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10606.5261 - val_loss: 9659.6729\n",
      "Epoch 89/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 10606.0254 - val_loss: 9659.1738\n",
      "Epoch 90/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10605.5251 - val_loss: 9658.6768\n",
      "Epoch 91/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10605.0266 - val_loss: 9658.1797\n",
      "Epoch 92/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10604.5274 - val_loss: 9657.6826\n",
      "Epoch 93/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10604.0289 - val_loss: 9657.1865\n",
      "Epoch 94/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10603.5314 - val_loss: 9656.6904\n",
      "Epoch 95/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 10603.0341 - val_loss: 9656.1953\n",
      "Epoch 96/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10602.5373 - val_loss: 9655.7002\n",
      "Epoch 97/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10602.0409 - val_loss: 9655.2061\n",
      "Epoch 98/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10601.5453 - val_loss: 9654.7119\n",
      "Epoch 99/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10601.0499 - val_loss: 9654.2188\n",
      "Epoch 100/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10600.5553 - val_loss: 9653.7256\n",
      "Epoch 101/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10600.0608 - val_loss: 9653.2314\n",
      "Epoch 102/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 10599.5667 - val_loss: 9652.7402\n",
      "Epoch 103/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10599.0735 - val_loss: 9652.2480\n",
      "Epoch 104/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10598.5802 - val_loss: 9651.7578\n",
      "Epoch 105/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 10598.0872 - val_loss: 9651.2656\n",
      "Epoch 106/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10597.5951 - val_loss: 9650.7744\n",
      "Epoch 107/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10597.1039 - val_loss: 9650.2842\n",
      "Epoch 108/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10596.6120 - val_loss: 9649.7939\n",
      "Epoch 109/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10596.1208 - val_loss: 9649.3047\n",
      "Epoch 110/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10595.6299 - val_loss: 9648.8154\n",
      "Epoch 111/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10595.1394 - val_loss: 9648.3262\n",
      "Epoch 112/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10594.6494 - val_loss: 9647.8369\n",
      "Epoch 113/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 10594.1594 - val_loss: 9647.3496\n",
      "Epoch 114/10000\n",
      "750/750 [==============================] - 0s 162us/step - loss: 10593.6703 - val_loss: 9646.8604\n",
      "Epoch 115/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10593.1812 - val_loss: 9646.3730\n",
      "Epoch 116/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10592.6924 - val_loss: 9645.8857\n",
      "Epoch 117/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10592.2041 - val_loss: 9645.3984\n",
      "Epoch 118/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 10591.7158 - val_loss: 9644.9121\n",
      "Epoch 119/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 10591.2285 - val_loss: 9644.4248\n",
      "Epoch 120/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10590.7408 - val_loss: 9643.9395\n",
      "Epoch 121/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10590.2536 - val_loss: 9643.4531\n",
      "Epoch 122/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10589.7668 - val_loss: 9642.9668\n",
      "Epoch 123/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10589.2799 - val_loss: 9642.4814\n",
      "Epoch 124/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10588.7941 - val_loss: 9641.9961\n",
      "Epoch 125/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10588.3080 - val_loss: 9641.5117\n",
      "Epoch 126/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10587.8221 - val_loss: 9641.0273\n",
      "Epoch 127/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10587.3366 - val_loss: 9640.5420\n",
      "Epoch 128/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10586.8515 - val_loss: 9640.0576\n",
      "Epoch 129/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10586.3663 - val_loss: 9639.5742\n",
      "Epoch 130/10000\n",
      "750/750 [==============================] - 0s 166us/step - loss: 10585.8822 - val_loss: 9639.0908\n",
      "Epoch 131/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 10585.3973 - val_loss: 9638.6064\n",
      "Epoch 132/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10584.9128 - val_loss: 9638.1230\n",
      "Epoch 133/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10584.4294 - val_loss: 9637.6396\n",
      "Epoch 134/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10583.9454 - val_loss: 9637.1572\n",
      "Epoch 135/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10583.4617 - val_loss: 9636.6748\n",
      "Epoch 136/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10582.9781 - val_loss: 9636.1924\n",
      "Epoch 137/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10582.4951 - val_loss: 9635.7100\n",
      "Epoch 138/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 120us/step - loss: 10582.0126 - val_loss: 9635.2285\n",
      "Epoch 139/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 10581.5299 - val_loss: 9634.7451\n",
      "Epoch 140/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 10581.0475 - val_loss: 9634.2646\n",
      "Epoch 141/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10580.5652 - val_loss: 9633.7832\n",
      "Epoch 142/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 10580.0833 - val_loss: 9633.3018\n",
      "Epoch 143/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10579.6015 - val_loss: 9632.8203\n",
      "Epoch 144/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10579.1199 - val_loss: 9632.3408\n",
      "Epoch 145/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10578.6389 - val_loss: 9631.8594\n",
      "Epoch 146/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10578.1572 - val_loss: 9631.3789\n",
      "Epoch 147/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10577.6767 - val_loss: 9630.8994\n",
      "Epoch 148/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10577.1955 - val_loss: 9630.4189\n",
      "Epoch 149/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 10576.7151 - val_loss: 9629.9385\n",
      "Epoch 150/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10576.2349 - val_loss: 9629.4590\n",
      "Epoch 151/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10575.7544 - val_loss: 9628.9795\n",
      "Epoch 152/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10575.2744 - val_loss: 9628.5000\n",
      "Epoch 153/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10574.7944 - val_loss: 9628.0205\n",
      "Epoch 154/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10574.3150 - val_loss: 9627.5410\n",
      "Epoch 155/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 10573.8351 - val_loss: 9627.0635\n",
      "Epoch 156/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10573.3552 - val_loss: 9626.5840\n",
      "Epoch 157/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10572.8763 - val_loss: 9626.1055\n",
      "Epoch 158/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10572.3976 - val_loss: 9625.6270\n",
      "Epoch 159/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10571.9183 - val_loss: 9625.1484\n",
      "Epoch 160/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10571.4397 - val_loss: 9624.6709\n",
      "Epoch 161/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10570.9609 - val_loss: 9624.1924\n",
      "Epoch 162/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10570.4826 - val_loss: 9623.7148\n",
      "Epoch 163/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10570.0042 - val_loss: 9623.2373\n",
      "Epoch 164/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10569.5262 - val_loss: 9622.7588\n",
      "Epoch 165/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10569.0476 - val_loss: 9622.2812\n",
      "Epoch 166/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10568.5702 - val_loss: 9621.8047\n",
      "Epoch 167/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10568.0927 - val_loss: 9621.3271\n",
      "Epoch 168/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10567.6150 - val_loss: 9620.8496\n",
      "Epoch 169/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 10567.1377 - val_loss: 9620.3730\n",
      "Epoch 170/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10566.6605 - val_loss: 9619.8965\n",
      "Epoch 171/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10566.1830 - val_loss: 9619.4189\n",
      "Epoch 172/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 10565.7061 - val_loss: 9618.9434\n",
      "Epoch 173/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10565.2296 - val_loss: 9618.4668\n",
      "Epoch 174/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10564.7529 - val_loss: 9617.9912\n",
      "Epoch 175/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10564.2759 - val_loss: 9617.5146\n",
      "Epoch 176/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 10563.7999 - val_loss: 9617.0381\n",
      "Epoch 177/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10563.3232 - val_loss: 9616.5625\n",
      "Epoch 178/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10562.8466 - val_loss: 9616.0869\n",
      "Epoch 179/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10562.3707 - val_loss: 9615.6113\n",
      "Epoch 180/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10561.8947 - val_loss: 9615.1357\n",
      "Epoch 181/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10561.4189 - val_loss: 9614.6602\n",
      "Epoch 182/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10560.9430 - val_loss: 9614.1855\n",
      "Epoch 183/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 10560.4672 - val_loss: 9613.7109\n",
      "Epoch 184/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10559.9920 - val_loss: 9613.2354\n",
      "Epoch 185/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10559.5160 - val_loss: 9612.7607\n",
      "Epoch 186/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10559.0411 - val_loss: 9612.2852\n",
      "Epoch 187/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10558.5662 - val_loss: 9611.8105\n",
      "Epoch 188/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10558.0912 - val_loss: 9611.3359\n",
      "Epoch 189/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10557.6162 - val_loss: 9610.8604\n",
      "Epoch 190/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10557.1409 - val_loss: 9610.3867\n",
      "Epoch 191/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10556.6659 - val_loss: 9609.9121\n",
      "Epoch 192/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 10556.1918 - val_loss: 9609.4385\n",
      "Epoch 193/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 10555.7169 - val_loss: 9608.9648\n",
      "Epoch 194/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 10555.2427 - val_loss: 9608.4893\n",
      "Epoch 195/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 10554.7685 - val_loss: 9608.0146\n",
      "Epoch 196/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10554.2942 - val_loss: 9607.5420\n",
      "Epoch 197/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 10553.8196 - val_loss: 9607.0674\n",
      "Epoch 198/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10553.3461 - val_loss: 9606.5938\n",
      "Epoch 199/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10552.8721 - val_loss: 9606.1201\n",
      "Epoch 200/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 10552.3985 - val_loss: 9605.6475\n",
      "Epoch 201/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10551.9240 - val_loss: 9605.1729\n",
      "Epoch 202/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10551.4506 - val_loss: 9604.6992\n",
      "Epoch 203/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10550.9774 - val_loss: 9604.2285\n",
      "Epoch 204/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 10550.5033 - val_loss: 9603.7549\n",
      "Epoch 205/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10550.0302 - val_loss: 9603.2812\n",
      "Epoch 206/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10549.5568 - val_loss: 9602.8076\n",
      "Epoch 207/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10549.0837 - val_loss: 9602.3350\n",
      "Epoch 208/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10548.6104 - val_loss: 9601.8623\n",
      "Epoch 209/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10548.1372 - val_loss: 9601.3896\n",
      "Epoch 210/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 130us/step - loss: 10547.6643 - val_loss: 9600.9170\n",
      "Epoch 211/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 10547.1916 - val_loss: 9600.4443\n",
      "Epoch 212/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10546.7186 - val_loss: 9599.9717\n",
      "Epoch 213/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10546.2460 - val_loss: 9599.5000\n",
      "Epoch 214/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10545.7736 - val_loss: 9599.0283\n",
      "Epoch 215/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10545.3010 - val_loss: 9598.5547\n",
      "Epoch 216/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10544.8284 - val_loss: 9598.0830\n",
      "Epoch 217/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10544.3565 - val_loss: 9597.6104\n",
      "Epoch 218/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 10543.8840 - val_loss: 9597.1396\n",
      "Epoch 219/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 10543.4119 - val_loss: 9596.6670\n",
      "Epoch 220/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10542.9395 - val_loss: 9596.1943\n",
      "Epoch 221/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10542.4672 - val_loss: 9595.7236\n",
      "Epoch 222/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10541.9957 - val_loss: 9595.2520\n",
      "Epoch 223/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10541.5234 - val_loss: 9594.7793\n",
      "Epoch 224/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 10541.0515 - val_loss: 9594.3086\n",
      "Epoch 225/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10540.5795 - val_loss: 9593.8359\n",
      "Epoch 226/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10540.1076 - val_loss: 9593.3662\n",
      "Epoch 227/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10539.6362 - val_loss: 9592.8936\n",
      "Epoch 228/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 10539.1647 - val_loss: 9592.4219\n",
      "Epoch 229/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10538.6930 - val_loss: 9591.9512\n",
      "Epoch 230/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10538.2212 - val_loss: 9591.4795\n",
      "Epoch 231/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10537.7501 - val_loss: 9591.0078\n",
      "Epoch 232/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10537.2788 - val_loss: 9590.5381\n",
      "Epoch 233/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10536.8078 - val_loss: 9590.0664\n",
      "Epoch 234/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10536.3363 - val_loss: 9589.5957\n",
      "Epoch 235/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10535.8653 - val_loss: 9589.1250\n",
      "Epoch 236/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10535.3944 - val_loss: 9588.6533\n",
      "Epoch 237/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10534.9232 - val_loss: 9588.1826\n",
      "Epoch 238/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10534.4520 - val_loss: 9587.7119\n",
      "Epoch 239/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 10533.9815 - val_loss: 9587.2422\n",
      "Epoch 240/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10533.5107 - val_loss: 9586.7715\n",
      "Epoch 241/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 10533.0400 - val_loss: 9586.3008\n",
      "Epoch 242/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10532.5692 - val_loss: 9585.8311\n",
      "Epoch 243/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10532.0989 - val_loss: 9585.3594\n",
      "Epoch 244/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10531.6282 - val_loss: 9584.8896\n",
      "Epoch 245/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10531.1578 - val_loss: 9584.4189\n",
      "Epoch 246/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10530.6872 - val_loss: 9583.9492\n",
      "Epoch 247/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10530.2167 - val_loss: 9583.4785\n",
      "Epoch 248/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 10529.7463 - val_loss: 9583.0088\n",
      "Epoch 249/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 10529.2763 - val_loss: 9582.5381\n",
      "Epoch 250/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10528.8064 - val_loss: 9582.0693\n",
      "Epoch 251/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10528.3356 - val_loss: 9581.5986\n",
      "Epoch 252/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10527.8658 - val_loss: 9581.1279\n",
      "Epoch 253/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10527.3956 - val_loss: 9580.6592\n",
      "Epoch 254/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10526.9255 - val_loss: 9580.1895\n",
      "Epoch 255/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10526.4559 - val_loss: 9579.7188\n",
      "Epoch 256/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10525.9856 - val_loss: 9579.2500\n",
      "Epoch 257/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10525.5160 - val_loss: 9578.7803\n",
      "Epoch 258/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10525.0466 - val_loss: 9578.3105\n",
      "Epoch 259/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10524.5769 - val_loss: 9577.8408\n",
      "Epoch 260/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 10524.1070 - val_loss: 9577.3711\n",
      "Epoch 261/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10523.6373 - val_loss: 9576.9023\n",
      "Epoch 262/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 10523.1678 - val_loss: 9576.4326\n",
      "Epoch 263/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10522.6982 - val_loss: 9575.9629\n",
      "Epoch 264/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10522.2287 - val_loss: 9575.4941\n",
      "Epoch 265/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10521.7591 - val_loss: 9575.0254\n",
      "Epoch 266/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10521.2905 - val_loss: 9574.5566\n",
      "Epoch 267/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10520.8205 - val_loss: 9574.0859\n",
      "Epoch 268/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10520.3514 - val_loss: 9573.6172\n",
      "Epoch 269/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 10519.8821 - val_loss: 9573.1475\n",
      "Epoch 270/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10519.4126 - val_loss: 9572.6787\n",
      "Epoch 271/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 10518.9439 - val_loss: 9572.2090\n",
      "Epoch 272/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 10518.4745 - val_loss: 9571.7412\n",
      "Epoch 273/10000\n",
      "750/750 [==============================] - 0s 187us/step - loss: 10518.0056 - val_loss: 9571.2725\n",
      "Epoch 274/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 10517.5363 - val_loss: 9570.8037\n",
      "Epoch 275/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10517.0677 - val_loss: 9570.3340\n",
      "Epoch 276/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 10516.5985 - val_loss: 9569.8662\n",
      "Epoch 277/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 10516.1296 - val_loss: 9569.3975\n",
      "Epoch 278/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 10515.6610 - val_loss: 9568.9287\n",
      "Epoch 279/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 10515.1921 - val_loss: 9568.4600\n",
      "Epoch 280/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 10514.7237 - val_loss: 9567.9912\n",
      "Epoch 281/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 10514.2547 - val_loss: 9567.5225\n",
      "Epoch 282/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 131us/step - loss: 10513.7860 - val_loss: 9567.0547\n",
      "Epoch 283/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 10513.3175 - val_loss: 9566.5859\n",
      "Epoch 284/10000\n",
      "750/750 [==============================] - 0s 169us/step - loss: 10512.8490 - val_loss: 9566.1172\n",
      "Epoch 285/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10512.3800 - val_loss: 9565.6494\n",
      "Epoch 286/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 10511.9118 - val_loss: 9565.1807\n",
      "Epoch 287/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10511.4434 - val_loss: 9564.7119\n",
      "Epoch 288/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10510.9749 - val_loss: 9564.2441\n",
      "Epoch 289/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 10510.5059 - val_loss: 9563.7764\n",
      "Epoch 290/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 10510.0379 - val_loss: 9563.3076\n",
      "Epoch 291/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 10509.5695 - val_loss: 9562.8398\n",
      "Epoch 292/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 10509.1013 - val_loss: 9562.3711\n",
      "Epoch 293/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10508.6330 - val_loss: 9561.9033\n",
      "Epoch 294/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10508.1654 - val_loss: 9561.4355\n",
      "Epoch 295/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10507.6970 - val_loss: 9560.9668\n",
      "Epoch 296/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10507.2288 - val_loss: 9560.4980\n",
      "Epoch 297/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10506.7605 - val_loss: 9560.0312\n",
      "Epoch 298/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 10506.2924 - val_loss: 9559.5635\n",
      "Epoch 299/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10505.8251 - val_loss: 9559.0957\n",
      "Epoch 300/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 10505.3574 - val_loss: 9558.6270\n",
      "Epoch 301/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10504.8892 - val_loss: 9558.1592\n",
      "Epoch 302/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10504.4212 - val_loss: 9557.6924\n",
      "Epoch 303/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 10503.9528 - val_loss: 9557.2236\n",
      "Epoch 304/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10503.4848 - val_loss: 9556.7568\n",
      "Epoch 305/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10503.0172 - val_loss: 9556.2891\n",
      "Epoch 306/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10502.5497 - val_loss: 9555.8203\n",
      "Epoch 307/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10502.0814 - val_loss: 9555.3535\n",
      "Epoch 308/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10501.6144 - val_loss: 9554.8867\n",
      "Epoch 309/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10501.1465 - val_loss: 9554.4189\n",
      "Epoch 310/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10500.6789 - val_loss: 9553.9512\n",
      "Epoch 311/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10500.2115 - val_loss: 9553.4834\n",
      "Epoch 312/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10499.7443 - val_loss: 9553.0156\n",
      "Epoch 313/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10499.2764 - val_loss: 9552.5488\n",
      "Epoch 314/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10498.8088 - val_loss: 9552.0811\n",
      "Epoch 315/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10498.3417 - val_loss: 9551.6143\n",
      "Epoch 316/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10497.8740 - val_loss: 9551.1475\n",
      "Epoch 317/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10497.4061 - val_loss: 9550.6787\n",
      "Epoch 318/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10496.9388 - val_loss: 9550.2119\n",
      "Epoch 319/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10496.4718 - val_loss: 9549.7451\n",
      "Epoch 320/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 10496.0043 - val_loss: 9549.2783\n",
      "Epoch 321/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10495.5372 - val_loss: 9548.8105\n",
      "Epoch 322/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10495.0701 - val_loss: 9548.3428\n",
      "Epoch 323/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10494.6029 - val_loss: 9547.8770\n",
      "Epoch 324/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10494.1358 - val_loss: 9547.4092\n",
      "Epoch 325/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10493.6683 - val_loss: 9546.9424\n",
      "Epoch 326/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10493.2009 - val_loss: 9546.4746\n",
      "Epoch 327/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10492.7337 - val_loss: 9546.0078\n",
      "Epoch 328/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10492.2670 - val_loss: 9545.5410\n",
      "Epoch 329/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10491.7999 - val_loss: 9545.0742\n",
      "Epoch 330/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10491.3330 - val_loss: 9544.6064\n",
      "Epoch 331/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 10490.8659 - val_loss: 9544.1396\n",
      "Epoch 332/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 10490.3990 - val_loss: 9543.6738\n",
      "Epoch 333/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10489.9321 - val_loss: 9543.2070\n",
      "Epoch 334/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10489.4650 - val_loss: 9542.7393\n",
      "Epoch 335/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10488.9980 - val_loss: 9542.2734\n",
      "Epoch 336/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 10488.5311 - val_loss: 9541.8066\n",
      "Epoch 337/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10488.0646 - val_loss: 9541.3398\n",
      "Epoch 338/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10487.5976 - val_loss: 9540.8730\n",
      "Epoch 339/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 10487.1311 - val_loss: 9540.4062\n",
      "Epoch 340/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 10486.6644 - val_loss: 9539.9395\n",
      "Epoch 341/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 10486.1973 - val_loss: 9539.4717\n",
      "Epoch 342/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10485.7306 - val_loss: 9539.0068\n",
      "Epoch 343/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10485.2641 - val_loss: 9538.5400\n",
      "Epoch 344/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10484.7970 - val_loss: 9538.0732\n",
      "Epoch 345/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 10484.3307 - val_loss: 9537.6064\n",
      "Epoch 346/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10483.8640 - val_loss: 9537.1396\n",
      "Epoch 347/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10483.3975 - val_loss: 9536.6729\n",
      "Epoch 348/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10482.9308 - val_loss: 9536.2061\n",
      "Epoch 349/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10482.4644 - val_loss: 9535.7412\n",
      "Epoch 350/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 10481.9982 - val_loss: 9535.2744\n",
      "Epoch 351/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10481.5315 - val_loss: 9534.8076\n",
      "Epoch 352/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10481.0653 - val_loss: 9534.3408\n",
      "Epoch 353/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10480.5985 - val_loss: 9533.8750\n",
      "Epoch 354/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 114us/step - loss: 10480.1320 - val_loss: 9533.4082\n",
      "Epoch 355/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10479.6655 - val_loss: 9532.9424\n",
      "Epoch 356/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 10479.1995 - val_loss: 9532.4766\n",
      "Epoch 357/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10478.7334 - val_loss: 9532.0098\n",
      "Epoch 358/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 10478.2669 - val_loss: 9531.5430\n",
      "Epoch 359/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 10477.8005 - val_loss: 9531.0781\n",
      "Epoch 360/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10477.3340 - val_loss: 9530.6104\n",
      "Epoch 361/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10476.8679 - val_loss: 9530.1465\n",
      "Epoch 362/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10476.4019 - val_loss: 9529.6787\n",
      "Epoch 363/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10475.9358 - val_loss: 9529.2129\n",
      "Epoch 364/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10475.4697 - val_loss: 9528.7471\n",
      "Epoch 365/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10475.0032 - val_loss: 9528.2803\n",
      "Epoch 366/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10474.5368 - val_loss: 9527.8145\n",
      "Epoch 367/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 10474.0710 - val_loss: 9527.3496\n",
      "Epoch 368/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 10473.6047 - val_loss: 9526.8818\n",
      "Epoch 369/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 10473.1389 - val_loss: 9526.4160\n",
      "Epoch 370/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10472.6730 - val_loss: 9525.9502\n",
      "Epoch 371/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 10472.2063 - val_loss: 9525.4834\n",
      "Epoch 372/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 10471.7406 - val_loss: 9525.0195\n",
      "Epoch 373/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10471.2749 - val_loss: 9524.5537\n",
      "Epoch 374/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10470.8087 - val_loss: 9524.0869\n",
      "Epoch 375/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10470.3428 - val_loss: 9523.6211\n",
      "Epoch 376/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10469.8768 - val_loss: 9523.1553\n",
      "Epoch 377/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 10469.4106 - val_loss: 9522.6895\n",
      "Epoch 378/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10468.9443 - val_loss: 9522.2246\n",
      "Epoch 379/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10468.4790 - val_loss: 9521.7568\n",
      "Epoch 380/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10468.0136 - val_loss: 9521.2910\n",
      "Epoch 381/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 10467.5475 - val_loss: 9520.8262\n",
      "Epoch 382/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 10467.0814 - val_loss: 9520.3604\n",
      "Epoch 383/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10466.6149 - val_loss: 9519.8945\n",
      "Epoch 384/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10466.1499 - val_loss: 9519.4277\n",
      "Epoch 385/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10465.6844 - val_loss: 9518.9629\n",
      "Epoch 386/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10465.2185 - val_loss: 9518.4980\n",
      "Epoch 387/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 10464.7526 - val_loss: 9518.0312\n",
      "Epoch 388/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10464.2863 - val_loss: 9517.5664\n",
      "Epoch 389/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10463.8215 - val_loss: 9517.1006\n",
      "Epoch 390/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10463.3557 - val_loss: 9516.6348\n",
      "Epoch 391/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10462.8901 - val_loss: 9516.1689\n",
      "Epoch 392/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10462.4243 - val_loss: 9515.7041\n",
      "Epoch 393/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10461.9578 - val_loss: 9515.2383\n",
      "Epoch 394/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10461.4932 - val_loss: 9514.7725\n",
      "Epoch 395/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10461.0277 - val_loss: 9514.3076\n",
      "Epoch 396/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10460.5619 - val_loss: 9513.8408\n",
      "Epoch 397/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10460.0961 - val_loss: 9513.3760\n",
      "Epoch 398/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10459.6307 - val_loss: 9512.9102\n",
      "Epoch 399/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10459.1651 - val_loss: 9512.4443\n",
      "Epoch 400/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10458.6999 - val_loss: 9511.9805\n",
      "Epoch 401/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10458.2340 - val_loss: 9511.5146\n",
      "Epoch 402/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10457.7686 - val_loss: 9511.0488\n",
      "Epoch 403/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 10457.3036 - val_loss: 9510.5840\n",
      "Epoch 404/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10456.8380 - val_loss: 9510.1191\n",
      "Epoch 405/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10456.3725 - val_loss: 9509.6533\n",
      "Epoch 406/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10455.9070 - val_loss: 9509.1875\n",
      "Epoch 407/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10455.4416 - val_loss: 9508.7227\n",
      "Epoch 408/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10454.9763 - val_loss: 9508.2549\n",
      "Epoch 409/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10454.5113 - val_loss: 9507.7910\n",
      "Epoch 410/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10454.0456 - val_loss: 9507.3262\n",
      "Epoch 411/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10453.5802 - val_loss: 9506.8604\n",
      "Epoch 412/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10453.1151 - val_loss: 9506.3955\n",
      "Epoch 413/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10452.6500 - val_loss: 9505.9307\n",
      "Epoch 414/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10452.1846 - val_loss: 9505.4648\n",
      "Epoch 415/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10451.7186 - val_loss: 9504.9990\n",
      "Epoch 416/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10451.2537 - val_loss: 9504.5342\n",
      "Epoch 417/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10450.7887 - val_loss: 9504.0684\n",
      "Epoch 418/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10450.3236 - val_loss: 9503.6045\n",
      "Epoch 419/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10449.8581 - val_loss: 9503.1387\n",
      "Epoch 420/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10449.3930 - val_loss: 9502.6738\n",
      "Epoch 421/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10448.9278 - val_loss: 9502.2080\n",
      "Epoch 422/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 10448.4634 - val_loss: 9501.7441\n",
      "Epoch 423/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 10447.9972 - val_loss: 9501.2783\n",
      "Epoch 424/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10447.5320 - val_loss: 9500.8135\n",
      "Epoch 425/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10447.0671 - val_loss: 9500.3477\n",
      "Epoch 426/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 120us/step - loss: 10446.6024 - val_loss: 9499.8828\n",
      "Epoch 427/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10446.1370 - val_loss: 9499.4180\n",
      "Epoch 428/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10445.6716 - val_loss: 9498.9541\n",
      "Epoch 429/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10445.2067 - val_loss: 9498.4883\n",
      "Epoch 430/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10444.7419 - val_loss: 9498.0234\n",
      "Epoch 431/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 10444.2766 - val_loss: 9497.5576\n",
      "Epoch 432/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 10443.8114 - val_loss: 9497.0938\n",
      "Epoch 433/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 10443.3464 - val_loss: 9496.6270\n",
      "Epoch 434/10000\n",
      "750/750 [==============================] - 0s 107us/step - loss: 10442.8816 - val_loss: 9496.1631\n",
      "Epoch 435/10000\n",
      "750/750 [==============================] - 0s 172us/step - loss: 10442.4170 - val_loss: 9495.6982\n",
      "Epoch 436/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10441.9512 - val_loss: 9495.2334\n",
      "Epoch 437/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 10441.4864 - val_loss: 9494.7686\n",
      "Epoch 438/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10441.0222 - val_loss: 9494.3047\n",
      "Epoch 439/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10440.5568 - val_loss: 9493.8379\n",
      "Epoch 440/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10440.0915 - val_loss: 9493.3730\n",
      "Epoch 441/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 10439.6266 - val_loss: 9492.9092\n",
      "Epoch 442/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10439.1619 - val_loss: 9492.4443\n",
      "Epoch 443/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 10438.6971 - val_loss: 9491.9785\n",
      "Epoch 444/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 10438.2320 - val_loss: 9491.5146\n",
      "Epoch 445/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10437.7673 - val_loss: 9491.0488\n",
      "Epoch 446/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 10437.3024 - val_loss: 9490.5850\n",
      "Epoch 447/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10436.8381 - val_loss: 9490.1201\n",
      "Epoch 448/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10436.3726 - val_loss: 9489.6562\n",
      "Epoch 449/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 10435.9078 - val_loss: 9489.1895\n",
      "Epoch 450/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 10435.4437 - val_loss: 9488.7256\n",
      "Epoch 451/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10434.9782 - val_loss: 9488.2607\n",
      "Epoch 452/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 10434.5131 - val_loss: 9487.7959\n",
      "Epoch 453/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 10434.0485 - val_loss: 9487.3311\n",
      "Epoch 454/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 10433.5842 - val_loss: 9486.8672\n",
      "Epoch 455/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10433.1190 - val_loss: 9486.4004\n",
      "Epoch 456/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10432.6542 - val_loss: 9485.9365\n",
      "Epoch 457/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10432.1900 - val_loss: 9485.4717\n",
      "Epoch 458/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 10431.7252 - val_loss: 9485.0068\n",
      "Epoch 459/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10431.2600 - val_loss: 9484.5430\n",
      "Epoch 460/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10430.7955 - val_loss: 9484.0791\n",
      "Epoch 461/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10430.3310 - val_loss: 9483.6133\n",
      "Epoch 462/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10429.8663 - val_loss: 9483.1494\n",
      "Epoch 463/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10429.4016 - val_loss: 9482.6846\n",
      "Epoch 464/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10428.9372 - val_loss: 9482.2197\n",
      "Epoch 465/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10428.4727 - val_loss: 9481.7549\n",
      "Epoch 466/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10428.0076 - val_loss: 9481.2900\n",
      "Epoch 467/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10427.5428 - val_loss: 9480.8262\n",
      "Epoch 468/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10427.0784 - val_loss: 9480.3604\n",
      "Epoch 469/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10426.6139 - val_loss: 9479.8975\n",
      "Epoch 470/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 10426.1495 - val_loss: 9479.4326\n",
      "Epoch 471/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10425.6844 - val_loss: 9478.9688\n",
      "Epoch 472/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 10425.2202 - val_loss: 9478.5020\n",
      "Epoch 473/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10424.7560 - val_loss: 9478.0381\n",
      "Epoch 474/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10424.2907 - val_loss: 9477.5742\n",
      "Epoch 475/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10423.8263 - val_loss: 9477.1104\n",
      "Epoch 476/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10423.3624 - val_loss: 9476.6465\n",
      "Epoch 477/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10422.8980 - val_loss: 9476.1807\n",
      "Epoch 478/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10422.4325 - val_loss: 9475.7168\n",
      "Epoch 479/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10421.9683 - val_loss: 9475.2510\n",
      "Epoch 480/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10421.5042 - val_loss: 9474.7881\n",
      "Epoch 481/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10421.0386 - val_loss: 9474.3232\n",
      "Epoch 482/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10420.5746 - val_loss: 9473.8584\n",
      "Epoch 483/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10420.1107 - val_loss: 9473.3936\n",
      "Epoch 484/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10419.6465 - val_loss: 9472.9297\n",
      "Epoch 485/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10419.1813 - val_loss: 9472.4658\n",
      "Epoch 486/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10418.7171 - val_loss: 9472.0010\n",
      "Epoch 487/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10418.2529 - val_loss: 9471.5371\n",
      "Epoch 488/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10417.7881 - val_loss: 9471.0713\n",
      "Epoch 489/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10417.3242 - val_loss: 9470.6074\n",
      "Epoch 490/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10416.8591 - val_loss: 9470.1436\n",
      "Epoch 491/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10416.3955 - val_loss: 9469.6797\n",
      "Epoch 492/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10415.9302 - val_loss: 9469.2139\n",
      "Epoch 493/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10415.4661 - val_loss: 9468.7490\n",
      "Epoch 494/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10415.0025 - val_loss: 9468.2871\n",
      "Epoch 495/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10414.5378 - val_loss: 9467.8213\n",
      "Epoch 496/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10414.0731 - val_loss: 9467.3574\n",
      "Epoch 497/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10413.6091 - val_loss: 9466.8926\n",
      "Epoch 498/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10413.1449 - val_loss: 9466.4287\n",
      "Epoch 499/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10412.6798 - val_loss: 9465.9629\n",
      "Epoch 500/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10412.2158 - val_loss: 9465.4990\n",
      "Epoch 501/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10411.7522 - val_loss: 9465.0361\n",
      "Epoch 502/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 10411.2871 - val_loss: 9464.5713\n",
      "Epoch 503/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10410.8226 - val_loss: 9464.1064\n",
      "Epoch 504/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10410.3586 - val_loss: 9463.6436\n",
      "Epoch 505/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10409.8948 - val_loss: 9463.1797\n",
      "Epoch 506/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10409.4296 - val_loss: 9462.7158\n",
      "Epoch 507/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10408.9660 - val_loss: 9462.2490\n",
      "Epoch 508/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10408.5019 - val_loss: 9461.7871\n",
      "Epoch 509/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10408.0371 - val_loss: 9461.3213\n",
      "Epoch 510/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10407.5734 - val_loss: 9460.8564\n",
      "Epoch 511/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 10407.1091 - val_loss: 9460.3936\n",
      "Epoch 512/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10406.6452 - val_loss: 9459.9297\n",
      "Epoch 513/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10406.1802 - val_loss: 9459.4658\n",
      "Epoch 514/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10405.7160 - val_loss: 9459.0000\n",
      "Epoch 515/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10405.2526 - val_loss: 9458.5371\n",
      "Epoch 516/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10404.7876 - val_loss: 9458.0732\n",
      "Epoch 517/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10404.3234 - val_loss: 9457.6074\n",
      "Epoch 518/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10403.8599 - val_loss: 9457.1445\n",
      "Epoch 519/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10403.3953 - val_loss: 9456.6797\n",
      "Epoch 520/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10402.9309 - val_loss: 9456.2168\n",
      "Epoch 521/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10402.4672 - val_loss: 9455.7520\n",
      "Epoch 522/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10402.0034 - val_loss: 9455.2871\n",
      "Epoch 523/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10401.5386 - val_loss: 9454.8242\n",
      "Epoch 524/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 10401.0748 - val_loss: 9454.3584\n",
      "Epoch 525/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10400.6106 - val_loss: 9453.8965\n",
      "Epoch 526/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10400.1463 - val_loss: 9453.4307\n",
      "Epoch 527/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10399.6820 - val_loss: 9452.9668\n",
      "Epoch 528/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10399.2189 - val_loss: 9452.5039\n",
      "Epoch 529/10000\n",
      "750/750 [==============================] - 0s 103us/step - loss: 10398.7540 - val_loss: 9452.0381\n",
      "Epoch 530/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10398.2897 - val_loss: 9451.5762\n",
      "Epoch 531/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10397.8262 - val_loss: 9451.1113\n",
      "Epoch 532/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10397.3623 - val_loss: 9450.6465\n",
      "Epoch 533/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10396.8971 - val_loss: 9450.1836\n",
      "Epoch 534/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10396.4341 - val_loss: 9449.7188\n",
      "Epoch 535/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10395.9699 - val_loss: 9449.2559\n",
      "Epoch 536/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10395.5055 - val_loss: 9448.7900\n",
      "Epoch 537/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10395.0415 - val_loss: 9448.3262\n",
      "Epoch 538/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10394.5780 - val_loss: 9447.8633\n",
      "Epoch 539/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10394.1133 - val_loss: 9447.3975\n",
      "Epoch 540/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10393.6493 - val_loss: 9446.9355\n",
      "Epoch 541/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10393.1862 - val_loss: 9446.4717\n",
      "Epoch 542/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10392.7220 - val_loss: 9446.0068\n",
      "Epoch 543/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10392.2575 - val_loss: 9445.5430\n",
      "Epoch 544/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10391.7936 - val_loss: 9445.0781\n",
      "Epoch 545/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10391.3301 - val_loss: 9444.6152\n",
      "Epoch 546/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10390.8650 - val_loss: 9444.1514\n",
      "Epoch 547/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10390.4014 - val_loss: 9443.6865\n",
      "Epoch 548/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 10389.9380 - val_loss: 9443.2227\n",
      "Epoch 549/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10389.4733 - val_loss: 9442.7588\n",
      "Epoch 550/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10389.0097 - val_loss: 9442.2949\n",
      "Epoch 551/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10388.5461 - val_loss: 9441.8311\n",
      "Epoch 552/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10388.0815 - val_loss: 9441.3672\n",
      "Epoch 553/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10387.6177 - val_loss: 9440.9033\n",
      "Epoch 554/10000\n",
      "750/750 [==============================] - 0s 201us/step - loss: 10387.1543 - val_loss: 9440.4395\n",
      "Epoch 555/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10386.6896 - val_loss: 9439.9746\n",
      "Epoch 556/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 10386.2258 - val_loss: 9439.5117\n",
      "Epoch 557/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 10385.7625 - val_loss: 9439.0479\n",
      "Epoch 558/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10385.2985 - val_loss: 9438.5840\n",
      "Epoch 559/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 10384.8340 - val_loss: 9438.1201\n",
      "Epoch 560/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10384.3708 - val_loss: 9437.6562\n",
      "Epoch 561/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10383.9070 - val_loss: 9437.1914\n",
      "Epoch 562/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10383.4420 - val_loss: 9436.7285\n",
      "Epoch 563/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10382.9788 - val_loss: 9436.2637\n",
      "Epoch 564/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10382.5151 - val_loss: 9435.8008\n",
      "Epoch 565/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10382.0504 - val_loss: 9435.3369\n",
      "Epoch 566/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10381.5871 - val_loss: 9434.8730\n",
      "Epoch 567/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10381.1236 - val_loss: 9434.4092\n",
      "Epoch 568/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10380.6596 - val_loss: 9433.9453\n",
      "Epoch 569/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10380.1952 - val_loss: 9433.4805\n",
      "Epoch 570/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 134us/step - loss: 10379.7323 - val_loss: 9433.0186\n",
      "Epoch 571/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 10379.2678 - val_loss: 9432.5527\n",
      "Epoch 572/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10378.8037 - val_loss: 9432.0908\n",
      "Epoch 573/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10378.3408 - val_loss: 9431.6270\n",
      "Epoch 574/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 10377.8764 - val_loss: 9431.1621\n",
      "Epoch 575/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 10377.4124 - val_loss: 9430.6992\n",
      "Epoch 576/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10376.9490 - val_loss: 9430.2354\n",
      "Epoch 577/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10376.4852 - val_loss: 9429.7715\n",
      "Epoch 578/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10376.0211 - val_loss: 9429.3076\n",
      "Epoch 579/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10375.5578 - val_loss: 9428.8438\n",
      "Epoch 580/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10375.0938 - val_loss: 9428.3799\n",
      "Epoch 581/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10374.6295 - val_loss: 9427.9160\n",
      "Epoch 582/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10374.1664 - val_loss: 9427.4512\n",
      "Epoch 583/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10373.7027 - val_loss: 9426.9883\n",
      "Epoch 584/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 10373.2383 - val_loss: 9426.5254\n",
      "Epoch 585/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 10372.7748 - val_loss: 9426.0605\n",
      "Epoch 586/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 10372.3113 - val_loss: 9425.5977\n",
      "Epoch 587/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 10371.8469 - val_loss: 9425.1338\n",
      "Epoch 588/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 10371.3834 - val_loss: 9424.6689\n",
      "Epoch 589/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 10370.9198 - val_loss: 9424.2061\n",
      "Epoch 590/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10370.4554 - val_loss: 9423.7422\n",
      "Epoch 591/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 10369.9922 - val_loss: 9423.2783\n",
      "Epoch 592/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10369.5287 - val_loss: 9422.8145\n",
      "Epoch 593/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10369.0647 - val_loss: 9422.3506\n",
      "Epoch 594/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10368.6012 - val_loss: 9421.8867\n",
      "Epoch 595/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10368.1378 - val_loss: 9421.4248\n",
      "Epoch 596/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10367.6736 - val_loss: 9420.9600\n",
      "Epoch 597/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10367.2096 - val_loss: 9420.4961\n",
      "Epoch 598/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10366.7463 - val_loss: 9420.0322\n",
      "Epoch 599/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10366.2825 - val_loss: 9419.5693\n",
      "Epoch 600/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10365.8187 - val_loss: 9419.1055\n",
      "Epoch 601/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10365.3551 - val_loss: 9418.6416\n",
      "Epoch 602/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 10364.8914 - val_loss: 9418.1777\n",
      "Epoch 603/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10364.4275 - val_loss: 9417.7148\n",
      "Epoch 604/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10363.9642 - val_loss: 9417.2510\n",
      "Epoch 605/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10363.5005 - val_loss: 9416.7861\n",
      "Epoch 606/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10363.0365 - val_loss: 9416.3232\n",
      "Epoch 607/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10362.5735 - val_loss: 9415.8604\n",
      "Epoch 608/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10362.1090 - val_loss: 9415.3955\n",
      "Epoch 609/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10361.6454 - val_loss: 9414.9326\n",
      "Epoch 610/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10361.1824 - val_loss: 9414.4688\n",
      "Epoch 611/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10360.7182 - val_loss: 9414.0049\n",
      "Epoch 612/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10360.2541 - val_loss: 9413.5410\n",
      "Epoch 613/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 10349.527 - 0s 118us/step - loss: 10359.7910 - val_loss: 9413.0771\n",
      "Epoch 614/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10359.3277 - val_loss: 9412.6133\n",
      "Epoch 615/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10358.8636 - val_loss: 9412.1514\n",
      "Epoch 616/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10358.4001 - val_loss: 9411.6865\n",
      "Epoch 617/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10357.9366 - val_loss: 9411.2227\n",
      "Epoch 618/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10357.4726 - val_loss: 9410.7607\n",
      "Epoch 619/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10357.0096 - val_loss: 9410.2969\n",
      "Epoch 620/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10356.5459 - val_loss: 9409.8320\n",
      "Epoch 621/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10356.0819 - val_loss: 9409.3691\n",
      "Epoch 622/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 10355.6190 - val_loss: 9408.9062\n",
      "Epoch 623/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10355.1551 - val_loss: 9408.4424\n",
      "Epoch 624/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10354.6911 - val_loss: 9407.9785\n",
      "Epoch 625/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10354.2282 - val_loss: 9407.5146\n",
      "Epoch 626/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10353.7645 - val_loss: 9407.0508\n",
      "Epoch 627/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10353.3006 - val_loss: 9406.5879\n",
      "Epoch 628/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10352.8368 - val_loss: 9406.1240\n",
      "Epoch 629/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10352.3738 - val_loss: 9405.6592\n",
      "Epoch 630/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 10351.9093 - val_loss: 9405.1973\n",
      "Epoch 631/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10351.4462 - val_loss: 9404.7344\n",
      "Epoch 632/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10350.9830 - val_loss: 9404.2686\n",
      "Epoch 633/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10350.5190 - val_loss: 9403.8057\n",
      "Epoch 634/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10350.0561 - val_loss: 9403.3438\n",
      "Epoch 635/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10349.5925 - val_loss: 9402.8789\n",
      "Epoch 636/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10349.1281 - val_loss: 9402.4150\n",
      "Epoch 637/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10348.6650 - val_loss: 9401.9521\n",
      "Epoch 638/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10348.2015 - val_loss: 9401.4883\n",
      "Epoch 639/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 10347.7373 - val_loss: 9401.0244\n",
      "Epoch 640/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10347.2744 - val_loss: 9400.5615\n",
      "Epoch 641/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 10346.8106 - val_loss: 9400.0967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 642/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10346.3466 - val_loss: 9399.6338\n",
      "Epoch 643/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 10345.8837 - val_loss: 9399.1719\n",
      "Epoch 644/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 10345.4200 - val_loss: 9398.7061\n",
      "Epoch 645/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10344.9560 - val_loss: 9398.2432\n",
      "Epoch 646/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10344.4929 - val_loss: 9397.7812\n",
      "Epoch 647/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 10344.0293 - val_loss: 9397.3164\n",
      "Epoch 648/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10343.5653 - val_loss: 9396.8525\n",
      "Epoch 649/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10343.1022 - val_loss: 9396.3896\n",
      "Epoch 650/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10342.6389 - val_loss: 9395.9258\n",
      "Epoch 651/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10342.1748 - val_loss: 9395.4619\n",
      "Epoch 652/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10341.7122 - val_loss: 9394.9990\n",
      "Epoch 653/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10341.2484 - val_loss: 9394.5352\n",
      "Epoch 654/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10340.7843 - val_loss: 9394.0713\n",
      "Epoch 655/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10340.3215 - val_loss: 9393.6094\n",
      "Epoch 656/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 10339.8578 - val_loss: 9393.1445\n",
      "Epoch 657/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10339.3939 - val_loss: 9392.6816\n",
      "Epoch 658/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10338.9313 - val_loss: 9392.2188\n",
      "Epoch 659/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10338.4668 - val_loss: 9391.7549\n",
      "Epoch 660/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10338.0036 - val_loss: 9391.2910\n",
      "Epoch 661/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10337.5409 - val_loss: 9390.8271\n",
      "Epoch 662/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10337.0766 - val_loss: 9390.3633\n",
      "Epoch 663/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 10336.6132 - val_loss: 9389.9004\n",
      "Epoch 664/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10336.1502 - val_loss: 9389.4375\n",
      "Epoch 665/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10335.6868 - val_loss: 9388.9736\n",
      "Epoch 666/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10335.2226 - val_loss: 9388.5107\n",
      "Epoch 667/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10334.7606 - val_loss: 9388.0479\n",
      "Epoch 668/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10334.2957 - val_loss: 9387.5840\n",
      "Epoch 669/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10333.8326 - val_loss: 9387.1201\n",
      "Epoch 670/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10333.3702 - val_loss: 9386.6572\n",
      "Epoch 671/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10332.9060 - val_loss: 9386.1934\n",
      "Epoch 672/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10332.4424 - val_loss: 9385.7285\n",
      "Epoch 673/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10331.9794 - val_loss: 9385.2666\n",
      "Epoch 674/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10331.5155 - val_loss: 9384.8047\n",
      "Epoch 675/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10331.0525 - val_loss: 9384.3408\n",
      "Epoch 676/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10330.5896 - val_loss: 9383.8770\n",
      "Epoch 677/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10330.1249 - val_loss: 9383.4131\n",
      "Epoch 678/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10329.6624 - val_loss: 9382.9492\n",
      "Epoch 679/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10329.1992 - val_loss: 9382.4863\n",
      "Epoch 680/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10328.7351 - val_loss: 9382.0244\n",
      "Epoch 681/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10328.2724 - val_loss: 9381.5586\n",
      "Epoch 682/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10327.8086 - val_loss: 9381.0967\n",
      "Epoch 683/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10327.3445 - val_loss: 9380.6328\n",
      "Epoch 684/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10326.8820 - val_loss: 9380.1699\n",
      "Epoch 685/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10326.4191 - val_loss: 9379.7061\n",
      "Epoch 686/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10325.9550 - val_loss: 9379.2422\n",
      "Epoch 687/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10325.4924 - val_loss: 9378.7793\n",
      "Epoch 688/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10325.0281 - val_loss: 9378.3154\n",
      "Epoch 689/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10324.5643 - val_loss: 9377.8525\n",
      "Epoch 690/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10324.1019 - val_loss: 9377.3896\n",
      "Epoch 691/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10323.6384 - val_loss: 9376.9258\n",
      "Epoch 692/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10323.1743 - val_loss: 9376.4629\n",
      "Epoch 693/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10322.7122 - val_loss: 9375.9990\n",
      "Epoch 694/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10322.2479 - val_loss: 9375.5352\n",
      "Epoch 695/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10321.7842 - val_loss: 9375.0723\n",
      "Epoch 696/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10321.3216 - val_loss: 9374.6094\n",
      "Epoch 697/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10320.8575 - val_loss: 9374.1445\n",
      "Epoch 698/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10320.3938 - val_loss: 9373.6826\n",
      "Epoch 699/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10319.9317 - val_loss: 9373.2188\n",
      "Epoch 700/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10319.4676 - val_loss: 9372.7549\n",
      "Epoch 701/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10319.0041 - val_loss: 9372.2910\n",
      "Epoch 702/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10318.5413 - val_loss: 9371.8271\n",
      "Epoch 703/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10318.0772 - val_loss: 9371.3652\n",
      "Epoch 704/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10317.6143 - val_loss: 9370.9014\n",
      "Epoch 705/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10317.1511 - val_loss: 9370.4385\n",
      "Epoch 706/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10316.6872 - val_loss: 9369.9746\n",
      "Epoch 707/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10316.2240 - val_loss: 9369.5107\n",
      "Epoch 708/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10315.7613 - val_loss: 9369.0479\n",
      "Epoch 709/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10315.2966 - val_loss: 9368.5859\n",
      "Epoch 710/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10314.8339 - val_loss: 9368.1211\n",
      "Epoch 711/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 10314.3709 - val_loss: 9367.6582\n",
      "Epoch 712/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10313.9066 - val_loss: 9367.1953\n",
      "Epoch 713/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10313.4442 - val_loss: 9366.7314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 714/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10312.9806 - val_loss: 9366.2676\n",
      "Epoch 715/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10312.5165 - val_loss: 9365.8047\n",
      "Epoch 716/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10312.0540 - val_loss: 9365.3418\n",
      "Epoch 717/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10311.5907 - val_loss: 9364.8779\n",
      "Epoch 718/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10311.1263 - val_loss: 9364.4150\n",
      "Epoch 719/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10310.6639 - val_loss: 9363.9512\n",
      "Epoch 720/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10310.2003 - val_loss: 9363.4873\n",
      "Epoch 721/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10309.7366 - val_loss: 9363.0244\n",
      "Epoch 722/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10309.2740 - val_loss: 9362.5615\n",
      "Epoch 723/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10308.8102 - val_loss: 9362.0977\n",
      "Epoch 724/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10308.3466 - val_loss: 9361.6348\n",
      "Epoch 725/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10307.8841 - val_loss: 9361.1719\n",
      "Epoch 726/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10307.4198 - val_loss: 9360.7080\n",
      "Epoch 727/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10306.9569 - val_loss: 9360.2451\n",
      "Epoch 728/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10306.4938 - val_loss: 9359.7812\n",
      "Epoch 729/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10306.0297 - val_loss: 9359.3184\n",
      "Epoch 730/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10305.5668 - val_loss: 9358.8535\n",
      "Epoch 731/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10305.1035 - val_loss: 9358.3916\n",
      "Epoch 732/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10304.6398 - val_loss: 9357.9297\n",
      "Epoch 733/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10304.1769 - val_loss: 9357.4648\n",
      "Epoch 734/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10303.7136 - val_loss: 9357.0020\n",
      "Epoch 735/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10303.2494 - val_loss: 9356.5381\n",
      "Epoch 736/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 10302.7868 - val_loss: 9356.0742\n",
      "Epoch 737/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10302.3237 - val_loss: 9355.6104\n",
      "Epoch 738/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10301.8597 - val_loss: 9355.1484\n",
      "Epoch 739/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10301.3970 - val_loss: 9354.6855\n",
      "Epoch 740/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10300.9335 - val_loss: 9354.2217\n",
      "Epoch 741/10000\n",
      "750/750 [==============================] - 0s 103us/step - loss: 10300.4700 - val_loss: 9353.7588\n",
      "Epoch 742/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10300.0077 - val_loss: 9353.2949\n",
      "Epoch 743/10000\n",
      "750/750 [==============================] - 0s 189us/step - loss: 10299.5433 - val_loss: 9352.8311\n",
      "Epoch 744/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10299.0801 - val_loss: 9352.3682\n",
      "Epoch 745/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10298.6176 - val_loss: 9351.9062\n",
      "Epoch 746/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10298.1536 - val_loss: 9351.4424\n",
      "Epoch 747/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 10297.6902 - val_loss: 9350.9775\n",
      "Epoch 748/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10297.2278 - val_loss: 9350.5146\n",
      "Epoch 749/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10296.7638 - val_loss: 9350.0527\n",
      "Epoch 750/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10296.3009 - val_loss: 9349.5889\n",
      "Epoch 751/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10295.8381 - val_loss: 9349.1270\n",
      "Epoch 752/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10295.3739 - val_loss: 9348.6631\n",
      "Epoch 753/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 10291.483 - 0s 121us/step - loss: 10294.9115 - val_loss: 9348.1992\n",
      "Epoch 754/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10294.4485 - val_loss: 9347.7354\n",
      "Epoch 755/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10293.9844 - val_loss: 9347.2734\n",
      "Epoch 756/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10293.5216 - val_loss: 9346.8105\n",
      "Epoch 757/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10293.0580 - val_loss: 9346.3467\n",
      "Epoch 758/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10292.5948 - val_loss: 9345.8828\n",
      "Epoch 759/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10292.1327 - val_loss: 9345.4199\n",
      "Epoch 760/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10291.6685 - val_loss: 9344.9570\n",
      "Epoch 761/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10291.2054 - val_loss: 9344.4932\n",
      "Epoch 762/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10290.7428 - val_loss: 9344.0312\n",
      "Epoch 763/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10290.2788 - val_loss: 9343.5684\n",
      "Epoch 764/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10289.8161 - val_loss: 9343.1035\n",
      "Epoch 765/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 10289.3536 - val_loss: 9342.6406\n",
      "Epoch 766/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10288.8891 - val_loss: 9342.1787\n",
      "Epoch 767/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10288.4269 - val_loss: 9341.7158\n",
      "Epoch 768/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10287.9638 - val_loss: 9341.2520\n",
      "Epoch 769/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10287.4993 - val_loss: 9340.7881\n",
      "Epoch 770/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10287.0371 - val_loss: 9340.3262\n",
      "Epoch 771/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10286.5738 - val_loss: 9339.8623\n",
      "Epoch 772/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10286.1098 - val_loss: 9339.3994\n",
      "Epoch 773/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10285.6475 - val_loss: 9338.9355\n",
      "Epoch 774/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10285.1844 - val_loss: 9338.4727\n",
      "Epoch 775/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10284.7212 - val_loss: 9338.0088\n",
      "Epoch 776/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10284.2582 - val_loss: 9337.5469\n",
      "Epoch 777/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10283.7940 - val_loss: 9337.0840\n",
      "Epoch 778/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 10283.3316 - val_loss: 9336.6201\n",
      "Epoch 779/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10282.8687 - val_loss: 9336.1572\n",
      "Epoch 780/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10282.4047 - val_loss: 9335.6943\n",
      "Epoch 781/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10281.9425 - val_loss: 9335.2314\n",
      "Epoch 782/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 10281.4788 - val_loss: 9334.7666\n",
      "Epoch 783/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10281.0152 - val_loss: 9334.3047\n",
      "Epoch 784/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10280.5536 - val_loss: 9333.8418\n",
      "Epoch 785/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10280.0896 - val_loss: 9333.3779\n",
      "Epoch 786/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10279.6261 - val_loss: 9332.9150\n",
      "Epoch 787/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10279.1637 - val_loss: 9332.4521\n",
      "Epoch 788/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10278.6995 - val_loss: 9331.9883\n",
      "Epoch 789/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10278.2364 - val_loss: 9331.5244\n",
      "Epoch 790/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10277.7741 - val_loss: 9331.0615\n",
      "Epoch 791/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10277.3098 - val_loss: 9330.5996\n",
      "Epoch 792/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10276.8476 - val_loss: 9330.1357\n",
      "Epoch 793/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10276.3844 - val_loss: 9329.6729\n",
      "Epoch 794/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10275.9206 - val_loss: 9329.2109\n",
      "Epoch 795/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10275.4581 - val_loss: 9328.7461\n",
      "Epoch 796/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10274.9947 - val_loss: 9328.2832\n",
      "Epoch 797/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10274.5308 - val_loss: 9327.8203\n",
      "Epoch 798/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10274.0692 - val_loss: 9327.3574\n",
      "Epoch 799/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10273.6048 - val_loss: 9326.8936\n",
      "Epoch 800/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10273.1419 - val_loss: 9326.4307\n",
      "Epoch 801/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10272.6795 - val_loss: 9325.9678\n",
      "Epoch 802/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10272.2151 - val_loss: 9325.5049\n",
      "Epoch 803/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10271.7524 - val_loss: 9325.0410\n",
      "Epoch 804/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10271.2902 - val_loss: 9324.5771\n",
      "Epoch 805/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10270.8258 - val_loss: 9324.1152\n",
      "Epoch 806/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10270.3634 - val_loss: 9323.6523\n",
      "Epoch 807/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10269.9001 - val_loss: 9323.1885\n",
      "Epoch 808/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10269.4363 - val_loss: 9322.7256\n",
      "Epoch 809/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10268.9737 - val_loss: 9322.2617\n",
      "Epoch 810/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10268.5102 - val_loss: 9321.7988\n",
      "Epoch 811/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10268.0465 - val_loss: 9321.3359\n",
      "Epoch 812/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10267.5842 - val_loss: 9320.8730\n",
      "Epoch 813/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10267.1208 - val_loss: 9320.4092\n",
      "Epoch 814/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10266.6574 - val_loss: 9319.9453\n",
      "Epoch 815/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10266.1951 - val_loss: 9319.4824\n",
      "Epoch 816/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 10265.7310 - val_loss: 9319.0215\n",
      "Epoch 817/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 10265.2681 - val_loss: 9318.5566\n",
      "Epoch 818/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10264.8057 - val_loss: 9318.0938\n",
      "Epoch 819/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10264.3420 - val_loss: 9317.6309\n",
      "Epoch 820/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 10263.8790 - val_loss: 9317.1680\n",
      "Epoch 821/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10263.4160 - val_loss: 9316.7041\n",
      "Epoch 822/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10262.9518 - val_loss: 9316.2412\n",
      "Epoch 823/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10262.4901 - val_loss: 9315.7783\n",
      "Epoch 824/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10262.0260 - val_loss: 9315.3145\n",
      "Epoch 825/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10261.5627 - val_loss: 9314.8506\n",
      "Epoch 826/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10261.1008 - val_loss: 9314.3887\n",
      "Epoch 827/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10260.6362 - val_loss: 9313.9258\n",
      "Epoch 828/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10260.1733 - val_loss: 9313.4619\n",
      "Epoch 829/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10259.7111 - val_loss: 9312.9980\n",
      "Epoch 830/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10259.2472 - val_loss: 9312.5361\n",
      "Epoch 831/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10258.7840 - val_loss: 9312.0723\n",
      "Epoch 832/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10258.3214 - val_loss: 9311.6104\n",
      "Epoch 833/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10257.8574 - val_loss: 9311.1465\n",
      "Epoch 834/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 10257.3949 - val_loss: 9310.6826\n",
      "Epoch 835/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10256.9312 - val_loss: 9310.2197\n",
      "Epoch 836/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10256.4675 - val_loss: 9309.7568\n",
      "Epoch 837/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 10256.0058 - val_loss: 9309.2949\n",
      "Epoch 838/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10255.5422 - val_loss: 9308.8301\n",
      "Epoch 839/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 10255.0789 - val_loss: 9308.3672\n",
      "Epoch 840/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10254.6161 - val_loss: 9307.9043\n",
      "Epoch 841/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10254.1521 - val_loss: 9307.4414\n",
      "Epoch 842/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10253.6893 - val_loss: 9306.9775\n",
      "Epoch 843/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10253.2263 - val_loss: 9306.5146\n",
      "Epoch 844/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 10252.7625 - val_loss: 9306.0527\n",
      "Epoch 845/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10252.3009 - val_loss: 9305.5889\n",
      "Epoch 846/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10251.8369 - val_loss: 9305.1260\n",
      "Epoch 847/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 10251.3730 - val_loss: 9304.6621\n",
      "Epoch 848/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10250.9110 - val_loss: 9304.1992\n",
      "Epoch 849/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10250.4477 - val_loss: 9303.7363\n",
      "Epoch 850/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10249.9837 - val_loss: 9303.2725\n",
      "Epoch 851/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10249.5215 - val_loss: 9302.8105\n",
      "Epoch 852/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10249.0578 - val_loss: 9302.3457\n",
      "Epoch 853/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10248.5944 - val_loss: 9301.8828\n",
      "Epoch 854/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10248.1323 - val_loss: 9301.4199\n",
      "Epoch 855/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10247.6678 - val_loss: 9300.9580\n",
      "Epoch 856/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 10247.2054 - val_loss: 9300.4941\n",
      "Epoch 857/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 86us/step - loss: 10246.7423 - val_loss: 9300.0303\n",
      "Epoch 858/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10246.2783 - val_loss: 9299.5684\n",
      "Epoch 859/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10245.8154 - val_loss: 9299.1045\n",
      "Epoch 860/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10245.3527 - val_loss: 9298.6406\n",
      "Epoch 861/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10244.8890 - val_loss: 9298.1777\n",
      "Epoch 862/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10244.4268 - val_loss: 9297.7158\n",
      "Epoch 863/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10243.9634 - val_loss: 9297.2520\n",
      "Epoch 864/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10243.4996 - val_loss: 9296.7881\n",
      "Epoch 865/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10243.0375 - val_loss: 9296.3262\n",
      "Epoch 866/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10242.5737 - val_loss: 9295.8623\n",
      "Epoch 867/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10242.1106 - val_loss: 9295.3994\n",
      "Epoch 868/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10241.6477 - val_loss: 9294.9355\n",
      "Epoch 869/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 10241.1833 - val_loss: 9294.4736\n",
      "Epoch 870/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10240.7211 - val_loss: 9294.0088\n",
      "Epoch 871/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10240.2583 - val_loss: 9293.5469\n",
      "Epoch 872/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10239.7943 - val_loss: 9293.0840\n",
      "Epoch 873/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10239.3318 - val_loss: 9292.6201\n",
      "Epoch 874/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10238.8685 - val_loss: 9292.1562\n",
      "Epoch 875/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10238.4045 - val_loss: 9291.6943\n",
      "Epoch 876/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10237.9425 - val_loss: 9291.2314\n",
      "Epoch 877/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10237.4787 - val_loss: 9290.7676\n",
      "Epoch 878/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 10237.0153 - val_loss: 9290.3047\n",
      "Epoch 879/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10236.5530 - val_loss: 9289.8418\n",
      "Epoch 880/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10236.0890 - val_loss: 9289.3799\n",
      "Epoch 881/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10235.6264 - val_loss: 9288.9141\n",
      "Epoch 882/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 10235.1638 - val_loss: 9288.4512\n",
      "Epoch 883/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10234.6993 - val_loss: 9287.9893\n",
      "Epoch 884/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 10234.2371 - val_loss: 9287.5264\n",
      "Epoch 885/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 10233.7740 - val_loss: 9287.0625\n",
      "Epoch 886/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10233.3100 - val_loss: 9286.5996\n",
      "Epoch 887/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10232.8480 - val_loss: 9286.1367\n",
      "Epoch 888/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10232.3844 - val_loss: 9285.6729\n",
      "Epoch 889/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10231.9207 - val_loss: 9285.2090\n",
      "Epoch 890/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10231.4584 - val_loss: 9284.7471\n",
      "Epoch 891/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10230.9945 - val_loss: 9284.2832\n",
      "Epoch 892/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10230.5310 - val_loss: 9283.8203\n",
      "Epoch 893/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10230.0691 - val_loss: 9283.3574\n",
      "Epoch 894/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10229.6049 - val_loss: 9282.8945\n",
      "Epoch 895/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10229.1422 - val_loss: 9282.4307\n",
      "Epoch 896/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10228.6798 - val_loss: 9281.9678\n",
      "Epoch 897/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10228.2154 - val_loss: 9281.5049\n",
      "Epoch 898/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10227.7528 - val_loss: 9281.0420\n",
      "Epoch 899/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10227.2897 - val_loss: 9280.5771\n",
      "Epoch 900/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10226.8255 - val_loss: 9280.1152\n",
      "Epoch 901/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10226.3636 - val_loss: 9279.6533\n",
      "Epoch 902/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10225.8997 - val_loss: 9279.1895\n",
      "Epoch 903/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10225.4362 - val_loss: 9278.7256\n",
      "Epoch 904/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10224.9745 - val_loss: 9278.2637\n",
      "Epoch 905/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10224.5104 - val_loss: 9277.7988\n",
      "Epoch 906/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10224.0470 - val_loss: 9277.3359\n",
      "Epoch 907/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10223.5848 - val_loss: 9276.8730\n",
      "Epoch 908/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10223.1209 - val_loss: 9276.4102\n",
      "Epoch 909/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10222.6576 - val_loss: 9275.9463\n",
      "Epoch 910/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10222.1953 - val_loss: 9275.4834\n",
      "Epoch 911/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10221.7315 - val_loss: 9275.0215\n",
      "Epoch 912/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10221.2687 - val_loss: 9274.5576\n",
      "Epoch 913/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10220.8057 - val_loss: 9274.0938\n",
      "Epoch 914/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10220.3420 - val_loss: 9273.6309\n",
      "Epoch 915/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10219.8797 - val_loss: 9273.1689\n",
      "Epoch 916/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10219.4159 - val_loss: 9272.7041\n",
      "Epoch 917/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10218.9524 - val_loss: 9272.2422\n",
      "Epoch 918/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10218.4904 - val_loss: 9271.7793\n",
      "Epoch 919/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10218.0264 - val_loss: 9271.3164\n",
      "Epoch 920/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10217.5638 - val_loss: 9270.8506\n",
      "Epoch 921/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10217.1004 - val_loss: 9270.3887\n",
      "Epoch 922/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10216.6363 - val_loss: 9269.9268\n",
      "Epoch 923/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10216.1741 - val_loss: 9269.4639\n",
      "Epoch 924/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10215.7117 - val_loss: 9269.0000\n",
      "Epoch 925/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10215.2469 - val_loss: 9268.5371\n",
      "Epoch 926/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10214.7850 - val_loss: 9268.0742\n",
      "Epoch 927/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10214.3217 - val_loss: 9267.6104\n",
      "Epoch 928/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10213.8571 - val_loss: 9267.1465\n",
      "Epoch 929/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10213.3958 - val_loss: 9266.6846\n",
      "Epoch 930/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10212.9317 - val_loss: 9266.2207\n",
      "Epoch 931/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10212.4690 - val_loss: 9265.7578\n",
      "Epoch 932/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10212.0064 - val_loss: 9265.2949\n",
      "Epoch 933/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10211.5420 - val_loss: 9264.8330\n",
      "Epoch 934/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10211.0791 - val_loss: 9264.3682\n",
      "Epoch 935/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 10210.6164 - val_loss: 9263.9043\n",
      "Epoch 936/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10210.1526 - val_loss: 9263.4424\n",
      "Epoch 937/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10209.6902 - val_loss: 9262.9795\n",
      "Epoch 938/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10209.2268 - val_loss: 9262.5146\n",
      "Epoch 939/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10208.7632 - val_loss: 9262.0527\n",
      "Epoch 940/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10208.3012 - val_loss: 9261.5908\n",
      "Epoch 941/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10207.8376 - val_loss: 9261.1270\n",
      "Epoch 942/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10207.3734 - val_loss: 9260.6631\n",
      "Epoch 943/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10206.9117 - val_loss: 9260.2012\n",
      "Epoch 944/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10206.4475 - val_loss: 9259.7373\n",
      "Epoch 945/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10205.9847 - val_loss: 9259.2734\n",
      "Epoch 946/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10205.5216 - val_loss: 9258.8105\n",
      "Epoch 947/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10205.0580 - val_loss: 9258.3477\n",
      "Epoch 948/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10204.5953 - val_loss: 9257.8857\n",
      "Epoch 949/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10204.1332 - val_loss: 9257.4219\n",
      "Epoch 950/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10203.6685 - val_loss: 9256.9590\n",
      "Epoch 951/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10203.2063 - val_loss: 9256.4951\n",
      "Epoch 952/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10202.7429 - val_loss: 9256.0322\n",
      "Epoch 953/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10202.2796 - val_loss: 9255.5684\n",
      "Epoch 954/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10201.8173 - val_loss: 9255.1064\n",
      "Epoch 955/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10201.3531 - val_loss: 9254.6416\n",
      "Epoch 956/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10200.8903 - val_loss: 9254.1797\n",
      "Epoch 957/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10200.4280 - val_loss: 9253.7168\n",
      "Epoch 958/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10199.9641 - val_loss: 9253.2549\n",
      "Epoch 959/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10199.5009 - val_loss: 9252.7910\n",
      "Epoch 960/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10199.0384 - val_loss: 9252.3262\n",
      "Epoch 961/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10198.5747 - val_loss: 9251.8643\n",
      "Epoch 962/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10198.1123 - val_loss: 9251.4014\n",
      "Epoch 963/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10197.6491 - val_loss: 9250.9375\n",
      "Epoch 964/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10197.1852 - val_loss: 9250.4746\n",
      "Epoch 965/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10196.7232 - val_loss: 9250.0117\n",
      "Epoch 966/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10196.2597 - val_loss: 9249.5479\n",
      "Epoch 967/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10195.7965 - val_loss: 9249.0859\n",
      "Epoch 968/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10195.3345 - val_loss: 9248.6230\n",
      "Epoch 969/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10194.8702 - val_loss: 9248.1602\n",
      "Epoch 970/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10194.4073 - val_loss: 9247.6963\n",
      "Epoch 971/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10193.9452 - val_loss: 9247.2344\n",
      "Epoch 972/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10193.4804 - val_loss: 9246.7715\n",
      "Epoch 973/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10193.0186 - val_loss: 9246.3076\n",
      "Epoch 974/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10192.5557 - val_loss: 9245.8438\n",
      "Epoch 975/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 10192.0921 - val_loss: 9245.3809\n",
      "Epoch 976/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10191.6296 - val_loss: 9244.9189\n",
      "Epoch 977/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10191.1659 - val_loss: 9244.4561\n",
      "Epoch 978/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10190.7032 - val_loss: 9243.9922\n",
      "Epoch 979/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10190.2414 - val_loss: 9243.5293\n",
      "Epoch 980/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10189.7772 - val_loss: 9243.0674\n",
      "Epoch 981/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10189.3145 - val_loss: 9242.6035\n",
      "Epoch 982/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10188.8513 - val_loss: 9242.1396\n",
      "Epoch 983/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10188.3877 - val_loss: 9241.6777\n",
      "Epoch 984/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10187.9263 - val_loss: 9241.2158\n",
      "Epoch 985/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10187.4622 - val_loss: 9240.7520\n",
      "Epoch 986/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10186.9992 - val_loss: 9240.2881\n",
      "Epoch 987/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10186.5371 - val_loss: 9239.8262\n",
      "Epoch 988/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10186.0728 - val_loss: 9239.3633\n",
      "Epoch 989/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10185.6112 - val_loss: 9238.9004\n",
      "Epoch 990/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10185.1483 - val_loss: 9238.4375\n",
      "Epoch 991/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10184.6839 - val_loss: 9237.9736\n",
      "Epoch 992/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10184.2219 - val_loss: 9237.5117\n",
      "Epoch 993/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10183.7591 - val_loss: 9237.0479\n",
      "Epoch 994/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10183.2956 - val_loss: 9236.5840\n",
      "Epoch 995/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10182.8338 - val_loss: 9236.1211\n",
      "Epoch 996/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10182.3693 - val_loss: 9235.6592\n",
      "Epoch 997/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10181.9070 - val_loss: 9235.1953\n",
      "Epoch 998/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10181.4445 - val_loss: 9234.7324\n",
      "Epoch 999/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10180.9807 - val_loss: 9234.2705\n",
      "Epoch 1000/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10180.5189 - val_loss: 9233.8076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1001/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10180.0552 - val_loss: 9233.3438\n",
      "Epoch 1002/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10179.5922 - val_loss: 9232.8809\n",
      "Epoch 1003/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10179.1299 - val_loss: 9232.4189\n",
      "Epoch 1004/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10178.6658 - val_loss: 9231.9561\n",
      "Epoch 1005/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10178.2034 - val_loss: 9231.4922\n",
      "Epoch 1006/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 10177.7413 - val_loss: 9231.0293\n",
      "Epoch 1007/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10177.2770 - val_loss: 9230.5674\n",
      "Epoch 1008/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10176.8149 - val_loss: 9230.1045\n",
      "Epoch 1009/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10176.3519 - val_loss: 9229.6406\n",
      "Epoch 1010/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10175.8886 - val_loss: 9229.1777\n",
      "Epoch 1011/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10175.4266 - val_loss: 9228.7158\n",
      "Epoch 1012/10000\n",
      "750/750 [==============================] - 0s 107us/step - loss: 10174.9625 - val_loss: 9228.2529\n",
      "Epoch 1013/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 10174.4999 - val_loss: 9227.7891\n",
      "Epoch 1014/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10174.0379 - val_loss: 9227.3262\n",
      "Epoch 1015/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10173.5740 - val_loss: 9226.8633\n",
      "Epoch 1016/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10173.1118 - val_loss: 9226.4014\n",
      "Epoch 1017/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10172.6483 - val_loss: 9225.9375\n",
      "Epoch 1018/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10172.1851 - val_loss: 9225.4746\n",
      "Epoch 1019/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10171.7231 - val_loss: 9225.0117\n",
      "Epoch 1020/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10171.2589 - val_loss: 9224.5479\n",
      "Epoch 1021/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10170.7967 - val_loss: 9224.0859\n",
      "Epoch 1022/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10170.3344 - val_loss: 9223.6230\n",
      "Epoch 1023/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10169.8704 - val_loss: 9223.1602\n",
      "Epoch 1024/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10169.4078 - val_loss: 9222.6963\n",
      "Epoch 1025/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10168.9453 - val_loss: 9222.2344\n",
      "Epoch 1026/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10168.4815 - val_loss: 9221.7715\n",
      "Epoch 1027/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10168.0196 - val_loss: 9221.3086\n",
      "Epoch 1028/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10167.5560 - val_loss: 9220.8447\n",
      "Epoch 1029/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10167.0927 - val_loss: 9220.3828\n",
      "Epoch 1030/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10166.6307 - val_loss: 9219.9189\n",
      "Epoch 1031/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10166.1666 - val_loss: 9219.4580\n",
      "Epoch 1032/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10165.7043 - val_loss: 9218.9932\n",
      "Epoch 1033/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10165.2421 - val_loss: 9218.5303\n",
      "Epoch 1034/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10164.7782 - val_loss: 9218.0674\n",
      "Epoch 1035/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10164.3158 - val_loss: 9217.6064\n",
      "Epoch 1036/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10163.8524 - val_loss: 9217.1416\n",
      "Epoch 1037/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10163.3897 - val_loss: 9216.6777\n",
      "Epoch 1038/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10162.9275 - val_loss: 9216.2158\n",
      "Epoch 1039/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10162.4633 - val_loss: 9215.7549\n",
      "Epoch 1040/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10162.0010 - val_loss: 9215.2910\n",
      "Epoch 1041/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 10161.5383 - val_loss: 9214.8262\n",
      "Epoch 1042/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10161.0748 - val_loss: 9214.3633\n",
      "Epoch 1043/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10160.6128 - val_loss: 9213.9033\n",
      "Epoch 1044/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 10160.1487 - val_loss: 9213.4385\n",
      "Epoch 1045/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10159.6861 - val_loss: 9212.9746\n",
      "Epoch 1046/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10159.2239 - val_loss: 9212.5117\n",
      "Epoch 1047/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10158.7598 - val_loss: 9212.0508\n",
      "Epoch 1048/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10158.2975 - val_loss: 9211.5869\n",
      "Epoch 1049/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10157.8355 - val_loss: 9211.1230\n",
      "Epoch 1050/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10157.3712 - val_loss: 9210.6602\n",
      "Epoch 1051/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10156.9091 - val_loss: 9210.1992\n",
      "Epoch 1052/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10156.4461 - val_loss: 9209.7344\n",
      "Epoch 1053/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10155.9821 - val_loss: 9209.2715\n",
      "Epoch 1054/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10155.5207 - val_loss: 9208.8086\n",
      "Epoch 1055/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10155.0565 - val_loss: 9208.3467\n",
      "Epoch 1056/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10154.5936 - val_loss: 9207.8828\n",
      "Epoch 1057/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10154.1317 - val_loss: 9207.4199\n",
      "Epoch 1058/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10153.6675 - val_loss: 9206.9580\n",
      "Epoch 1059/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10153.2053 - val_loss: 9206.4951\n",
      "Epoch 1060/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10152.7426 - val_loss: 9206.0312\n",
      "Epoch 1061/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10152.2792 - val_loss: 9205.5684\n",
      "Epoch 1062/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10151.8171 - val_loss: 9205.1064\n",
      "Epoch 1063/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10151.3529 - val_loss: 9204.6436\n",
      "Epoch 1064/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10150.8904 - val_loss: 9204.1797\n",
      "Epoch 1065/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10150.4284 - val_loss: 9203.7168\n",
      "Epoch 1066/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10149.9641 - val_loss: 9203.2549\n",
      "Epoch 1067/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10149.5021 - val_loss: 9202.7920\n",
      "Epoch 1068/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10149.0390 - val_loss: 9202.3271\n",
      "Epoch 1069/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10148.5756 - val_loss: 9201.8652\n",
      "Epoch 1070/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10148.1140 - val_loss: 9201.4033\n",
      "Epoch 1071/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10147.6498 - val_loss: 9200.9404\n",
      "Epoch 1072/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10147.1873 - val_loss: 9200.4756\n",
      "Epoch 1073/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 10146.7251 - val_loss: 9200.0137\n",
      "Epoch 1074/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10146.2609 - val_loss: 9199.5508\n",
      "Epoch 1075/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10145.7990 - val_loss: 9199.0889\n",
      "Epoch 1076/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10145.3357 - val_loss: 9198.6250\n",
      "Epoch 1077/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10144.8719 - val_loss: 9198.1602\n",
      "Epoch 1078/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10144.4099 - val_loss: 9197.6992\n",
      "Epoch 1079/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10143.9463 - val_loss: 9197.2354\n",
      "Epoch 1080/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10143.4837 - val_loss: 9196.7725\n",
      "Epoch 1081/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10143.0215 - val_loss: 9196.3086\n",
      "Epoch 1082/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10142.5574 - val_loss: 9195.8477\n",
      "Epoch 1083/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10142.0952 - val_loss: 9195.3838\n",
      "Epoch 1084/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10141.6322 - val_loss: 9194.9199\n",
      "Epoch 1085/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10141.1686 - val_loss: 9194.4580\n",
      "Epoch 1086/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10140.7066 - val_loss: 9193.9961\n",
      "Epoch 1087/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10140.2425 - val_loss: 9193.5312\n",
      "Epoch 1088/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10139.7799 - val_loss: 9193.0684\n",
      "Epoch 1089/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 10139.3183 - val_loss: 9192.6064\n",
      "Epoch 1090/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10138.8536 - val_loss: 9192.1455\n",
      "Epoch 1091/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10138.3915 - val_loss: 9191.6807\n",
      "Epoch 1092/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10137.9290 - val_loss: 9191.2168\n",
      "Epoch 1093/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10137.4649 - val_loss: 9190.7549\n",
      "Epoch 1094/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10137.0030 - val_loss: 9190.2939\n",
      "Epoch 1095/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10136.5394 - val_loss: 9189.8271\n",
      "Epoch 1096/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10136.0766 - val_loss: 9189.3652\n",
      "Epoch 1097/10000\n",
      "750/750 [==============================] - 0s 107us/step - loss: 10135.6146 - val_loss: 9188.9033\n",
      "Epoch 1098/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10135.1505 - val_loss: 9188.4424\n",
      "Epoch 1099/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10134.6878 - val_loss: 9187.9785\n",
      "Epoch 1100/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10134.2258 - val_loss: 9187.5137\n",
      "Epoch 1101/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10133.7618 - val_loss: 9187.0508\n",
      "Epoch 1102/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10133.3003 - val_loss: 9186.5898\n",
      "Epoch 1103/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10132.8358 - val_loss: 9186.1250\n",
      "Epoch 1104/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10132.3732 - val_loss: 9185.6621\n",
      "Epoch 1105/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10131.9115 - val_loss: 9185.1992\n",
      "Epoch 1106/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10131.4470 - val_loss: 9184.7383\n",
      "Epoch 1107/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10130.9850 - val_loss: 9184.2744\n",
      "Epoch 1108/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10130.5225 - val_loss: 9183.8105\n",
      "Epoch 1109/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10130.0585 - val_loss: 9183.3477\n",
      "Epoch 1110/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10129.5961 - val_loss: 9182.8857\n",
      "Epoch 1111/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10129.1325 - val_loss: 9182.4219\n",
      "Epoch 1112/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10128.6698 - val_loss: 9181.9590\n",
      "Epoch 1113/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10128.2080 - val_loss: 9181.4971\n",
      "Epoch 1114/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10127.7435 - val_loss: 9181.0342\n",
      "Epoch 1115/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 10127.2810 - val_loss: 9180.5703\n",
      "Epoch 1116/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10126.8188 - val_loss: 9180.1074\n",
      "Epoch 1117/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10126.3546 - val_loss: 9179.6455\n",
      "Epoch 1118/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10125.8926 - val_loss: 9179.1826\n",
      "Epoch 1119/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10125.4293 - val_loss: 9178.7188\n",
      "Epoch 1120/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10124.9660 - val_loss: 9178.2559\n",
      "Epoch 1121/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10124.5045 - val_loss: 9177.7939\n",
      "Epoch 1122/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10124.0405 - val_loss: 9177.3311\n",
      "Epoch 1123/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10123.5774 - val_loss: 9176.8672\n",
      "Epoch 1124/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10123.1154 - val_loss: 9176.4043\n",
      "Epoch 1125/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10122.6517 - val_loss: 9175.9424\n",
      "Epoch 1126/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10122.1893 - val_loss: 9175.4795\n",
      "Epoch 1127/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10121.7262 - val_loss: 9175.0146\n",
      "Epoch 1128/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10121.2623 - val_loss: 9174.5518\n",
      "Epoch 1129/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10120.8010 - val_loss: 9174.0908\n",
      "Epoch 1130/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10120.3371 - val_loss: 9173.6270\n",
      "Epoch 1131/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10119.8745 - val_loss: 9173.1641\n",
      "Epoch 1132/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10119.4124 - val_loss: 9172.7002\n",
      "Epoch 1133/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10118.9480 - val_loss: 9172.2383\n",
      "Epoch 1134/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10118.4861 - val_loss: 9171.7754\n",
      "Epoch 1135/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10118.0232 - val_loss: 9171.3125\n",
      "Epoch 1136/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10117.5595 - val_loss: 9170.8477\n",
      "Epoch 1137/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10117.0977 - val_loss: 9170.3867\n",
      "Epoch 1138/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10116.6335 - val_loss: 9169.9238\n",
      "Epoch 1139/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10116.1707 - val_loss: 9169.4590\n",
      "Epoch 1140/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10115.7085 - val_loss: 9168.9971\n",
      "Epoch 1141/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10115.2445 - val_loss: 9168.5342\n",
      "Epoch 1142/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10114.7826 - val_loss: 9168.0713\n",
      "Epoch 1143/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 117us/step - loss: 10114.3195 - val_loss: 9167.6074\n",
      "Epoch 1144/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10113.8556 - val_loss: 9167.1455\n",
      "Epoch 1145/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10113.3941 - val_loss: 9166.6826\n",
      "Epoch 1146/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10112.9300 - val_loss: 9166.2188\n",
      "Epoch 1147/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 10112.4675 - val_loss: 9165.7559\n",
      "Epoch 1148/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10112.0051 - val_loss: 9165.2939\n",
      "Epoch 1149/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10111.5410 - val_loss: 9164.8320\n",
      "Epoch 1150/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10111.0789 - val_loss: 9164.3682\n",
      "Epoch 1151/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10110.6165 - val_loss: 9163.9043\n",
      "Epoch 1152/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10110.1527 - val_loss: 9163.4424\n",
      "Epoch 1153/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10109.6905 - val_loss: 9162.9805\n",
      "Epoch 1154/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10109.2267 - val_loss: 9162.5146\n",
      "Epoch 1155/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10108.7640 - val_loss: 9162.0527\n",
      "Epoch 1156/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10108.3019 - val_loss: 9161.5908\n",
      "Epoch 1157/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 10107.8375 - val_loss: 9161.1289\n",
      "Epoch 1158/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10107.3756 - val_loss: 9160.6660\n",
      "Epoch 1159/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10106.9130 - val_loss: 9160.2012\n",
      "Epoch 1160/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10106.4490 - val_loss: 9159.7383\n",
      "Epoch 1161/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10105.9873 - val_loss: 9159.2773\n",
      "Epoch 1162/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10105.5233 - val_loss: 9158.8125\n",
      "Epoch 1163/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10105.0601 - val_loss: 9158.3496\n",
      "Epoch 1164/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10104.5983 - val_loss: 9157.8867\n",
      "Epoch 1165/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10104.1341 - val_loss: 9157.4258\n",
      "Epoch 1166/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10103.6724 - val_loss: 9156.9619\n",
      "Epoch 1167/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10103.2095 - val_loss: 9156.4980\n",
      "Epoch 1168/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10102.7458 - val_loss: 9156.0352\n",
      "Epoch 1169/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10102.2835 - val_loss: 9155.5732\n",
      "Epoch 1170/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10101.8200 - val_loss: 9155.1094\n",
      "Epoch 1171/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10101.3572 - val_loss: 9154.6465\n",
      "Epoch 1172/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10100.8946 - val_loss: 9154.1836\n",
      "Epoch 1173/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10100.4305 - val_loss: 9153.7217\n",
      "Epoch 1174/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10099.9680 - val_loss: 9153.2578\n",
      "Epoch 1175/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10099.5065 - val_loss: 9152.7939\n",
      "Epoch 1176/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10099.0419 - val_loss: 9152.3330\n",
      "Epoch 1177/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10098.5800 - val_loss: 9151.8691\n",
      "Epoch 1178/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10098.1167 - val_loss: 9151.4062\n",
      "Epoch 1179/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10097.6529 - val_loss: 9150.9424\n",
      "Epoch 1180/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10097.1911 - val_loss: 9150.4814\n",
      "Epoch 1181/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10096.7273 - val_loss: 9150.0186\n",
      "Epoch 1182/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10096.2651 - val_loss: 9149.5547\n",
      "Epoch 1183/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10095.8022 - val_loss: 9149.0908\n",
      "Epoch 1184/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10095.3387 - val_loss: 9148.6299\n",
      "Epoch 1185/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10094.8763 - val_loss: 9148.1660\n",
      "Epoch 1186/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 10094.4134 - val_loss: 9147.7021\n",
      "Epoch 1187/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10093.9500 - val_loss: 9147.2393\n",
      "Epoch 1188/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10093.4879 - val_loss: 9146.7783\n",
      "Epoch 1189/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10093.0239 - val_loss: 9146.3145\n",
      "Epoch 1190/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10092.5617 - val_loss: 9145.8506\n",
      "Epoch 1191/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 10092.0988 - val_loss: 9145.3877\n",
      "Epoch 1192/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10091.6352 - val_loss: 9144.9258\n",
      "Epoch 1193/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10091.1731 - val_loss: 9144.4629\n",
      "Epoch 1194/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10090.7099 - val_loss: 9143.9990\n",
      "Epoch 1195/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10090.2467 - val_loss: 9143.5361\n",
      "Epoch 1196/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10089.7850 - val_loss: 9143.0742\n",
      "Epoch 1197/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10089.3205 - val_loss: 9142.6104\n",
      "Epoch 1198/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10088.8576 - val_loss: 9142.1465\n",
      "Epoch 1199/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10088.3961 - val_loss: 9141.6855\n",
      "Epoch 1200/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10087.9322 - val_loss: 9141.2217\n",
      "Epoch 1201/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 10087.4698 - val_loss: 9140.7588\n",
      "Epoch 1202/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10087.0068 - val_loss: 9140.2949\n",
      "Epoch 1203/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10086.5429 - val_loss: 9139.8330\n",
      "Epoch 1204/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 10086.0812 - val_loss: 9139.3701\n",
      "Epoch 1205/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 10085.6173 - val_loss: 9138.9062\n",
      "Epoch 1206/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10085.1546 - val_loss: 9138.4434\n",
      "Epoch 1207/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 10084.6922 - val_loss: 9137.9814\n",
      "Epoch 1208/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10084.2280 - val_loss: 9137.5195\n",
      "Epoch 1209/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10083.7662 - val_loss: 9137.0557\n",
      "Epoch 1210/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 10083.3035 - val_loss: 9136.5918\n",
      "Epoch 1211/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10082.8395 - val_loss: 9136.1299\n",
      "Epoch 1212/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10082.3773 - val_loss: 9135.6680\n",
      "Epoch 1213/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10081.9139 - val_loss: 9135.2021\n",
      "Epoch 1214/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10081.4512 - val_loss: 9134.7402\n",
      "Epoch 1215/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10080.9892 - val_loss: 9134.2783\n",
      "Epoch 1216/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10080.5249 - val_loss: 9133.8164\n",
      "Epoch 1217/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10080.0625 - val_loss: 9133.3535\n",
      "Epoch 1218/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10079.6001 - val_loss: 9132.8887\n",
      "Epoch 1219/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10079.1365 - val_loss: 9132.4258\n",
      "Epoch 1220/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10078.6744 - val_loss: 9131.9648\n",
      "Epoch 1221/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10078.2105 - val_loss: 9131.5000\n",
      "Epoch 1222/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10077.7475 - val_loss: 9131.0371\n",
      "Epoch 1223/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 10077.2859 - val_loss: 9130.5742\n",
      "Epoch 1224/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10076.8219 - val_loss: 9130.1133\n",
      "Epoch 1225/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10076.3592 - val_loss: 9129.6494\n",
      "Epoch 1226/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10075.8969 - val_loss: 9129.1855\n",
      "Epoch 1227/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10075.4331 - val_loss: 9128.7227\n",
      "Epoch 1228/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10074.9712 - val_loss: 9128.2617\n",
      "Epoch 1229/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 10074.5076 - val_loss: 9127.7969\n",
      "Epoch 1230/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10074.0441 - val_loss: 9127.3340\n",
      "Epoch 1231/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10073.5826 - val_loss: 9126.8721\n",
      "Epoch 1232/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 10073.1190 - val_loss: 9126.4092\n",
      "Epoch 1233/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 10072.6557 - val_loss: 9125.9453\n",
      "Epoch 1234/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10072.1931 - val_loss: 9125.4814\n",
      "Epoch 1235/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10071.7294 - val_loss: 9125.0205\n",
      "Epoch 1236/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 10071.2679 - val_loss: 9124.5576\n",
      "Epoch 1237/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 10070.8039 - val_loss: 9124.0938\n",
      "Epoch 1238/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 10070.3401 - val_loss: 9123.6299\n",
      "Epoch 1239/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 10069.8787 - val_loss: 9123.1689\n",
      "Epoch 1240/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10069.4148 - val_loss: 9122.7051\n",
      "Epoch 1241/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10068.9522 - val_loss: 9122.2412\n",
      "Epoch 1242/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 10068.4898 - val_loss: 9121.7783\n",
      "Epoch 1243/10000\n",
      "750/750 [==============================] - 0s 103us/step - loss: 10068.0258 - val_loss: 9121.3174\n",
      "Epoch 1244/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 10067.5636 - val_loss: 9120.8535\n",
      "Epoch 1245/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 10067.1010 - val_loss: 9120.3896\n",
      "Epoch 1246/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10066.6373 - val_loss: 9119.9268\n",
      "Epoch 1247/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 10066.1754 - val_loss: 9119.4648\n",
      "Epoch 1248/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 10065.7110 - val_loss: 9119.0020\n",
      "Epoch 1249/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 10065.2488 - val_loss: 9118.5381\n",
      "Epoch 1250/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10064.7867 - val_loss: 9118.0752\n",
      "Epoch 1251/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10064.3229 - val_loss: 9117.6133\n",
      "Epoch 1252/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 10063.8603 - val_loss: 9117.1504\n",
      "Epoch 1253/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 10063.3973 - val_loss: 9116.6865\n",
      "Epoch 1254/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10062.9342 - val_loss: 9116.2236\n",
      "Epoch 1255/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10062.4723 - val_loss: 9115.7617\n",
      "Epoch 1256/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10062.0079 - val_loss: 9115.2979\n",
      "Epoch 1257/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10061.5453 - val_loss: 9114.8350\n",
      "Epoch 1258/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10061.0832 - val_loss: 9114.3730\n",
      "Epoch 1259/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10060.6195 - val_loss: 9113.9092\n",
      "Epoch 1260/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10060.1570 - val_loss: 9113.4473\n",
      "Epoch 1261/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10059.6942 - val_loss: 9112.9824\n",
      "Epoch 1262/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10059.2299 - val_loss: 9112.5205\n",
      "Epoch 1263/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10058.7680 - val_loss: 9112.0576\n",
      "Epoch 1264/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10058.3043 - val_loss: 9111.5938\n",
      "Epoch 1265/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10057.8420 - val_loss: 9111.1309\n",
      "Epoch 1266/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10057.3793 - val_loss: 9110.6689\n",
      "Epoch 1267/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10056.9155 - val_loss: 9110.2070\n",
      "Epoch 1268/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10056.4533 - val_loss: 9109.7422\n",
      "Epoch 1269/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10055.9912 - val_loss: 9109.2783\n",
      "Epoch 1270/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10055.5266 - val_loss: 9108.8174\n",
      "Epoch 1271/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10055.0648 - val_loss: 9108.3555\n",
      "Epoch 1272/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10054.6009 - val_loss: 9107.8896\n",
      "Epoch 1273/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10054.1382 - val_loss: 9107.4268\n",
      "Epoch 1274/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10053.6763 - val_loss: 9106.9648\n",
      "Epoch 1275/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10053.2121 - val_loss: 9106.5029\n",
      "Epoch 1276/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10052.7497 - val_loss: 9106.0400\n",
      "Epoch 1277/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10052.2876 - val_loss: 9105.5752\n",
      "Epoch 1278/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10051.8235 - val_loss: 9105.1133\n",
      "Epoch 1279/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10051.3614 - val_loss: 9104.6514\n",
      "Epoch 1280/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 10050.8976 - val_loss: 9104.1865\n",
      "Epoch 1281/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10050.4344 - val_loss: 9103.7236\n",
      "Epoch 1282/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10049.9723 - val_loss: 9103.2617\n",
      "Epoch 1283/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10049.5087 - val_loss: 9102.7988\n",
      "Epoch 1284/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10049.0455 - val_loss: 9102.3350\n",
      "Epoch 1285/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10048.5833 - val_loss: 9101.8730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1286/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10048.1196 - val_loss: 9101.4092\n",
      "Epoch 1287/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10047.6572 - val_loss: 9100.9473\n",
      "Epoch 1288/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10047.1938 - val_loss: 9100.4824\n",
      "Epoch 1289/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10046.7301 - val_loss: 9100.0205\n",
      "Epoch 1290/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10046.2683 - val_loss: 9099.5576\n",
      "Epoch 1291/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10045.8043 - val_loss: 9099.0938\n",
      "Epoch 1292/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10045.3412 - val_loss: 9098.6299\n",
      "Epoch 1293/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10044.8793 - val_loss: 9098.1680\n",
      "Epoch 1294/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10044.4151 - val_loss: 9097.7061\n",
      "Epoch 1295/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10043.9525 - val_loss: 9097.2422\n",
      "Epoch 1296/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 10043.4905 - val_loss: 9096.7783\n",
      "Epoch 1297/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10043.0256 - val_loss: 9096.3164\n",
      "Epoch 1298/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10042.5635 - val_loss: 9095.8535\n",
      "Epoch 1299/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 10042.1003 - val_loss: 9095.3896\n",
      "Epoch 1300/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10041.6363 - val_loss: 9094.9258\n",
      "Epoch 1301/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10041.1743 - val_loss: 9094.4648\n",
      "Epoch 1302/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10040.7107 - val_loss: 9094.0000\n",
      "Epoch 1303/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10040.2472 - val_loss: 9093.5361\n",
      "Epoch 1304/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10039.7851 - val_loss: 9093.0742\n",
      "Epoch 1305/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10039.3212 - val_loss: 9092.6113\n",
      "Epoch 1306/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10038.8583 - val_loss: 9092.1475\n",
      "Epoch 1307/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10038.3958 - val_loss: 9091.6855\n",
      "Epoch 1308/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10037.9319 - val_loss: 9091.2217\n",
      "Epoch 1309/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10037.4697 - val_loss: 9090.7578\n",
      "Epoch 1310/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10037.0061 - val_loss: 9090.2939\n",
      "Epoch 1311/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10036.5421 - val_loss: 9089.8320\n",
      "Epoch 1312/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10036.0803 - val_loss: 9089.3701\n",
      "Epoch 1313/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10035.6166 - val_loss: 9088.9053\n",
      "Epoch 1314/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10035.1530 - val_loss: 9088.4424\n",
      "Epoch 1315/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10034.6906 - val_loss: 9087.9805\n",
      "Epoch 1316/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10034.2266 - val_loss: 9087.5186\n",
      "Epoch 1317/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10033.7640 - val_loss: 9087.0518\n",
      "Epoch 1318/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10033.3017 - val_loss: 9086.5898\n",
      "Epoch 1319/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10032.8370 - val_loss: 9086.1279\n",
      "Epoch 1320/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10032.3742 - val_loss: 9085.6650\n",
      "Epoch 1321/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10031.9122 - val_loss: 9085.2002\n",
      "Epoch 1322/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10031.4477 - val_loss: 9084.7383\n",
      "Epoch 1323/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10030.9850 - val_loss: 9084.2754\n",
      "Epoch 1324/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10030.5220 - val_loss: 9083.8115\n",
      "Epoch 1325/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 10030.0585 - val_loss: 9083.3467\n",
      "Epoch 1326/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 10029.5963 - val_loss: 9082.8857\n",
      "Epoch 1327/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 10029.1325 - val_loss: 9082.4219\n",
      "Epoch 1328/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 10028.6697 - val_loss: 9081.9590\n",
      "Epoch 1329/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10028.2072 - val_loss: 9081.4951\n",
      "Epoch 1330/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10027.7425 - val_loss: 9081.0332\n",
      "Epoch 1331/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 10027.2797 - val_loss: 9080.5693\n",
      "Epoch 1332/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10026.8175 - val_loss: 9080.1064\n",
      "Epoch 1333/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10026.3530 - val_loss: 9079.6436\n",
      "Epoch 1334/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10025.8906 - val_loss: 9079.1797\n",
      "Epoch 1335/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10025.4278 - val_loss: 9078.7158\n",
      "Epoch 1336/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 10024.9640 - val_loss: 9078.2539\n",
      "Epoch 1337/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10024.5015 - val_loss: 9077.7910\n",
      "Epoch 1338/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 10024.0380 - val_loss: 9077.3271\n",
      "Epoch 1339/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10023.5739 - val_loss: 9076.8643\n",
      "Epoch 1340/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10023.1126 - val_loss: 9076.4023\n",
      "Epoch 1341/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10022.6480 - val_loss: 9075.9375\n",
      "Epoch 1342/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10022.1846 - val_loss: 9075.4736\n",
      "Epoch 1343/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10021.7229 - val_loss: 9075.0117\n",
      "Epoch 1344/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10021.2587 - val_loss: 9074.5488\n",
      "Epoch 1345/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10020.7955 - val_loss: 9074.0850\n",
      "Epoch 1346/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10020.3334 - val_loss: 9073.6221\n",
      "Epoch 1347/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 10019.8694 - val_loss: 9073.1592\n",
      "Epoch 1348/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10019.4071 - val_loss: 9072.6953\n",
      "Epoch 1349/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10018.9433 - val_loss: 9072.2314\n",
      "Epoch 1350/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10018.4797 - val_loss: 9071.7695\n",
      "Epoch 1351/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10018.0180 - val_loss: 9071.3076\n",
      "Epoch 1352/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10017.5542 - val_loss: 9070.8428\n",
      "Epoch 1353/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10017.0903 - val_loss: 9070.3799\n",
      "Epoch 1354/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 10016.6280 - val_loss: 9069.9180\n",
      "Epoch 1355/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 10016.1642 - val_loss: 9069.4521\n",
      "Epoch 1356/10000\n",
      "750/750 [==============================] - 0s 99us/step - loss: 10015.7010 - val_loss: 9068.9893\n",
      "Epoch 1357/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 10015.2389 - val_loss: 9068.5273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1358/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 10014.7745 - val_loss: 9068.0654\n",
      "Epoch 1359/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 10014.3114 - val_loss: 9067.6025\n",
      "Epoch 1360/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 10013.8494 - val_loss: 9067.1367\n",
      "Epoch 1361/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 10013.3850 - val_loss: 9066.6758\n",
      "Epoch 1362/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 10012.9226 - val_loss: 9066.2119\n",
      "Epoch 1363/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 10012.4594 - val_loss: 9065.7490\n",
      "Epoch 1364/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10011.9956 - val_loss: 9065.2842\n",
      "Epoch 1365/10000\n",
      "750/750 [==============================] - 0s 107us/step - loss: 10011.5331 - val_loss: 9064.8232\n",
      "Epoch 1366/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10011.0699 - val_loss: 9064.3594\n",
      "Epoch 1367/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 10010.6072 - val_loss: 9063.8965\n",
      "Epoch 1368/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 10010.1440 - val_loss: 9063.4326\n",
      "Epoch 1369/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 10009.6798 - val_loss: 9062.9697\n",
      "Epoch 1370/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 10009.2173 - val_loss: 9062.5068\n",
      "Epoch 1371/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 10008.7550 - val_loss: 9062.0439\n",
      "Epoch 1372/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 10008.2903 - val_loss: 9061.5811\n",
      "Epoch 1373/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 10007.8277 - val_loss: 9061.1172\n",
      "Epoch 1374/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 10007.3652 - val_loss: 9060.6533\n",
      "Epoch 1375/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 10006.9010 - val_loss: 9060.1914\n",
      "Epoch 1376/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10006.4386 - val_loss: 9059.7285\n",
      "Epoch 1377/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 10005.9755 - val_loss: 9059.2646\n",
      "Epoch 1378/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 10005.5115 - val_loss: 9058.8008\n",
      "Epoch 1379/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 10005.0494 - val_loss: 9058.3389\n",
      "Epoch 1380/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 10004.5855 - val_loss: 9057.8750\n",
      "Epoch 1381/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 10004.1221 - val_loss: 9057.4111\n",
      "Epoch 1382/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10003.6601 - val_loss: 9056.9492\n",
      "Epoch 1383/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 10003.1958 - val_loss: 9056.4863\n",
      "Epoch 1384/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10002.7330 - val_loss: 9056.0225\n",
      "Epoch 1385/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10002.2706 - val_loss: 9055.5596\n",
      "Epoch 1386/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 10001.8068 - val_loss: 9055.0967\n",
      "Epoch 1387/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 10001.3439 - val_loss: 9054.6328\n",
      "Epoch 1388/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 10000.8808 - val_loss: 9054.1689\n",
      "Epoch 1389/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 10000.4171 - val_loss: 9053.7070\n",
      "Epoch 1390/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9999.9551 - val_loss: 9053.2451\n",
      "Epoch 1391/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9999.4913 - val_loss: 9052.7803\n",
      "Epoch 1392/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9999.0278 - val_loss: 9052.3174\n",
      "Epoch 1393/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9998.5655 - val_loss: 9051.8555\n",
      "Epoch 1394/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9998.1015 - val_loss: 9051.3896\n",
      "Epoch 1395/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9997.6381 - val_loss: 9050.9268\n",
      "Epoch 1396/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9997.1760 - val_loss: 9050.4648\n",
      "Epoch 1397/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9996.7117 - val_loss: 9050.0029\n",
      "Epoch 1398/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9996.2491 - val_loss: 9049.5400\n",
      "Epoch 1399/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9995.7867 - val_loss: 9049.0742\n",
      "Epoch 1400/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9995.3224 - val_loss: 9048.6133\n",
      "Epoch 1401/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9994.8598 - val_loss: 9048.1494\n",
      "Epoch 1402/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9994.3970 - val_loss: 9047.6855\n",
      "Epoch 1403/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9993.9329 - val_loss: 9047.2217\n",
      "Epoch 1404/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9993.4708 - val_loss: 9046.7607\n",
      "Epoch 1405/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9993.0073 - val_loss: 9046.2959\n",
      "Epoch 1406/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9992.5437 - val_loss: 9045.8330\n",
      "Epoch 1407/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9992.0810 - val_loss: 9045.3701\n",
      "Epoch 1408/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9991.6171 - val_loss: 9044.9072\n",
      "Epoch 1409/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9991.1546 - val_loss: 9044.4434\n",
      "Epoch 1410/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9990.6921 - val_loss: 9043.9805\n",
      "Epoch 1411/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9990.2278 - val_loss: 9043.5186\n",
      "Epoch 1412/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9989.7653 - val_loss: 9043.0547\n",
      "Epoch 1413/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9989.3027 - val_loss: 9042.5908\n",
      "Epoch 1414/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9988.8383 - val_loss: 9042.1289\n",
      "Epoch 1415/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9988.3758 - val_loss: 9041.6660\n",
      "Epoch 1416/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9987.9130 - val_loss: 9041.2012\n",
      "Epoch 1417/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9987.4494 - val_loss: 9040.7383\n",
      "Epoch 1418/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9986.9868 - val_loss: 9040.2764\n",
      "Epoch 1419/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9986.5229 - val_loss: 9039.8125\n",
      "Epoch 1420/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9986.0595 - val_loss: 9039.3477\n",
      "Epoch 1421/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9985.5977 - val_loss: 9038.8867\n",
      "Epoch 1422/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9985.1330 - val_loss: 9038.4229\n",
      "Epoch 1423/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9984.6703 - val_loss: 9037.9590\n",
      "Epoch 1424/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9984.2081 - val_loss: 9037.4951\n",
      "Epoch 1425/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9983.7438 - val_loss: 9037.0342\n",
      "Epoch 1426/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9983.2816 - val_loss: 9036.5703\n",
      "Epoch 1427/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9982.8186 - val_loss: 9036.1064\n",
      "Epoch 1428/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9982.3543 - val_loss: 9035.6445\n",
      "Epoch 1429/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9981.8919 - val_loss: 9035.1816\n",
      "Epoch 1430/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 84us/step - loss: 9981.4286 - val_loss: 9034.7168\n",
      "Epoch 1431/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9980.9650 - val_loss: 9034.2549\n",
      "Epoch 1432/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9980.5030 - val_loss: 9033.7920\n",
      "Epoch 1433/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9980.0388 - val_loss: 9033.3271\n",
      "Epoch 1434/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9979.5757 - val_loss: 9032.8643\n",
      "Epoch 1435/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9979.1135 - val_loss: 9032.4023\n",
      "Epoch 1436/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9978.6490 - val_loss: 9031.9404\n",
      "Epoch 1437/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9978.1861 - val_loss: 9031.4756\n",
      "Epoch 1438/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 9977.7241 - val_loss: 9031.0117\n",
      "Epoch 1439/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 9977.2599 - val_loss: 9030.5498\n",
      "Epoch 1440/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 9976.7972 - val_loss: 9030.0869\n",
      "Epoch 1441/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 9976.3341 - val_loss: 9029.6230\n",
      "Epoch 1442/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9975.8701 - val_loss: 9029.1592\n",
      "Epoch 1443/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9975.4080 - val_loss: 9028.6982\n",
      "Epoch 1444/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9974.9449 - val_loss: 9028.2334\n",
      "Epoch 1445/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 9974.4804 - val_loss: 9027.7705\n",
      "Epoch 1446/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9974.0186 - val_loss: 9027.3076\n",
      "Epoch 1447/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9973.5545 - val_loss: 9026.8447\n",
      "Epoch 1448/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9973.0921 - val_loss: 9026.3809\n",
      "Epoch 1449/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9972.6296 - val_loss: 9025.9180\n",
      "Epoch 1450/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9972.1649 - val_loss: 9025.4561\n",
      "Epoch 1451/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9971.7024 - val_loss: 9024.9922\n",
      "Epoch 1452/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9971.2401 - val_loss: 9024.5283\n",
      "Epoch 1453/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9970.7757 - val_loss: 9024.0654\n",
      "Epoch 1454/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9970.3129 - val_loss: 9023.6035\n",
      "Epoch 1455/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9969.8501 - val_loss: 9023.1387\n",
      "Epoch 1456/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9969.3861 - val_loss: 9022.6758\n",
      "Epoch 1457/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9968.9242 - val_loss: 9022.2139\n",
      "Epoch 1458/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9968.4604 - val_loss: 9021.7490\n",
      "Epoch 1459/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9967.9972 - val_loss: 9021.2852\n",
      "Epoch 1460/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9967.5351 - val_loss: 9020.8242\n",
      "Epoch 1461/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9967.0704 - val_loss: 9020.3604\n",
      "Epoch 1462/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9966.6076 - val_loss: 9019.8965\n",
      "Epoch 1463/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9966.1453 - val_loss: 9019.4336\n",
      "Epoch 1464/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9965.6807 - val_loss: 9018.9717\n",
      "Epoch 1465/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9965.2178 - val_loss: 9018.5078\n",
      "Epoch 1466/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9964.7558 - val_loss: 9018.0439\n",
      "Epoch 1467/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9964.2916 - val_loss: 9017.5820\n",
      "Epoch 1468/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9963.8294 - val_loss: 9017.1182\n",
      "Epoch 1469/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9963.3661 - val_loss: 9016.6543\n",
      "Epoch 1470/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 9962.9024 - val_loss: 9016.1924\n",
      "Epoch 1471/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9962.4403 - val_loss: 9015.7295\n",
      "Epoch 1472/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9961.9759 - val_loss: 9015.2646\n",
      "Epoch 1473/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9961.5130 - val_loss: 9014.8008\n",
      "Epoch 1474/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9961.0509 - val_loss: 9014.3398\n",
      "Epoch 1475/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9960.5867 - val_loss: 9013.8779\n",
      "Epoch 1476/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9960.1233 - val_loss: 9013.4131\n",
      "Epoch 1477/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9959.6612 - val_loss: 9012.9492\n",
      "Epoch 1478/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9959.1973 - val_loss: 9012.4873\n",
      "Epoch 1479/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9958.7347 - val_loss: 9012.0244\n",
      "Epoch 1480/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9958.2714 - val_loss: 9011.5596\n",
      "Epoch 1481/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9957.8074 - val_loss: 9011.0967\n",
      "Epoch 1482/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9957.3455 - val_loss: 9010.6357\n",
      "Epoch 1483/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9956.8824 - val_loss: 9010.1699\n",
      "Epoch 1484/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9956.4181 - val_loss: 9009.7080\n",
      "Epoch 1485/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9955.9559 - val_loss: 9009.2451\n",
      "Epoch 1486/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9955.4918 - val_loss: 9008.7822\n",
      "Epoch 1487/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 9955.0295 - val_loss: 9008.3174\n",
      "Epoch 1488/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9954.5663 - val_loss: 9007.8555\n",
      "Epoch 1489/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9954.1025 - val_loss: 9007.3936\n",
      "Epoch 1490/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9953.6399 - val_loss: 9006.9297\n",
      "Epoch 1491/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9953.1777 - val_loss: 9006.4658\n",
      "Epoch 1492/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9952.7132 - val_loss: 9006.0029\n",
      "Epoch 1493/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 9952.2510 - val_loss: 9005.5410\n",
      "Epoch 1494/10000\n",
      "750/750 [==============================] - 0s 107us/step - loss: 9951.7876 - val_loss: 9005.0762\n",
      "Epoch 1495/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9951.3240 - val_loss: 9004.6133\n",
      "Epoch 1496/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9950.8612 - val_loss: 9004.1514\n",
      "Epoch 1497/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9950.3978 - val_loss: 9003.6865\n",
      "Epoch 1498/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9949.9344 - val_loss: 9003.2227\n",
      "Epoch 1499/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9949.4723 - val_loss: 9002.7607\n",
      "Epoch 1500/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9949.0079 - val_loss: 9002.2979\n",
      "Epoch 1501/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9948.5452 - val_loss: 9001.8340\n",
      "Epoch 1502/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9948.0829 - val_loss: 9001.3701\n",
      "Epoch 1503/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9947.6185 - val_loss: 9000.9092\n",
      "Epoch 1504/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9947.1553 - val_loss: 9000.4453\n",
      "Epoch 1505/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9946.6933 - val_loss: 8999.9814\n",
      "Epoch 1506/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9946.2288 - val_loss: 8999.5195\n",
      "Epoch 1507/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9945.7663 - val_loss: 8999.0557\n",
      "Epoch 1508/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9945.3034 - val_loss: 8998.5918\n",
      "Epoch 1509/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9944.8393 - val_loss: 8998.1299\n",
      "Epoch 1510/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9944.3778 - val_loss: 8997.6670\n",
      "Epoch 1511/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9943.9133 - val_loss: 8997.2021\n",
      "Epoch 1512/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9943.4504 - val_loss: 8996.7383\n",
      "Epoch 1513/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9942.9883 - val_loss: 8996.2773\n",
      "Epoch 1514/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9942.5241 - val_loss: 8995.8145\n",
      "Epoch 1515/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 9942.0607 - val_loss: 8995.3496\n",
      "Epoch 1516/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9941.5986 - val_loss: 8994.8867\n",
      "Epoch 1517/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9941.1347 - val_loss: 8994.4248\n",
      "Epoch 1518/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9940.6716 - val_loss: 8993.9619\n",
      "Epoch 1519/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9940.2091 - val_loss: 8993.4971\n",
      "Epoch 1520/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9939.7447 - val_loss: 8993.0342\n",
      "Epoch 1521/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9939.2828 - val_loss: 8992.5713\n",
      "Epoch 1522/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9938.8194 - val_loss: 8992.1074\n",
      "Epoch 1523/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9938.3552 - val_loss: 8991.6455\n",
      "Epoch 1524/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9937.8935 - val_loss: 8991.1826\n",
      "Epoch 1525/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9937.4295 - val_loss: 8990.7188\n",
      "Epoch 1526/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9936.9661 - val_loss: 8990.2549\n",
      "Epoch 1527/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9936.5040 - val_loss: 8989.7930\n",
      "Epoch 1528/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9936.0398 - val_loss: 8989.3311\n",
      "Epoch 1529/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9935.5769 - val_loss: 8988.8672\n",
      "Epoch 1530/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9935.1147 - val_loss: 8988.4033\n",
      "Epoch 1531/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9934.6507 - val_loss: 8987.9404\n",
      "Epoch 1532/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9934.1876 - val_loss: 8987.4775\n",
      "Epoch 1533/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9933.7247 - val_loss: 8987.0117\n",
      "Epoch 1534/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9933.2608 - val_loss: 8986.5508\n",
      "Epoch 1535/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9932.7989 - val_loss: 8986.0889\n",
      "Epoch 1536/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9932.3354 - val_loss: 8985.6240\n",
      "Epoch 1537/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9931.8715 - val_loss: 8985.1602\n",
      "Epoch 1538/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9931.4093 - val_loss: 8984.6982\n",
      "Epoch 1539/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9930.9452 - val_loss: 8984.2354\n",
      "Epoch 1540/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9930.4823 - val_loss: 8983.7705\n",
      "Epoch 1541/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9930.0204 - val_loss: 8983.3086\n",
      "Epoch 1542/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9929.5553 - val_loss: 8982.8467\n",
      "Epoch 1543/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9929.0930 - val_loss: 8982.3828\n",
      "Epoch 1544/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9928.6308 - val_loss: 8981.9189\n",
      "Epoch 1545/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9928.1664 - val_loss: 8981.4561\n",
      "Epoch 1546/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9927.7036 - val_loss: 8980.9932\n",
      "Epoch 1547/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9927.2408 - val_loss: 8980.5293\n",
      "Epoch 1548/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9926.7769 - val_loss: 8980.0674\n",
      "Epoch 1549/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9926.3147 - val_loss: 8979.6045\n",
      "Epoch 1550/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9925.8508 - val_loss: 8979.1396\n",
      "Epoch 1551/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9925.3874 - val_loss: 8978.6758\n",
      "Epoch 1552/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9924.9258 - val_loss: 8978.2148\n",
      "Epoch 1553/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9924.4614 - val_loss: 8977.7520\n",
      "Epoch 1554/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9923.9985 - val_loss: 8977.2881\n",
      "Epoch 1555/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9923.5361 - val_loss: 8976.8242\n",
      "Epoch 1556/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9923.0721 - val_loss: 8976.3623\n",
      "Epoch 1557/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 9922.6089 - val_loss: 8975.8994\n",
      "Epoch 1558/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9922.1463 - val_loss: 8975.4346\n",
      "Epoch 1559/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9921.6821 - val_loss: 8974.9717\n",
      "Epoch 1560/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9921.2199 - val_loss: 8974.5088\n",
      "Epoch 1561/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9920.7564 - val_loss: 8974.0449\n",
      "Epoch 1562/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9920.2928 - val_loss: 8973.5830\n",
      "Epoch 1563/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9919.8310 - val_loss: 8973.1201\n",
      "Epoch 1564/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9919.3668 - val_loss: 8972.6562\n",
      "Epoch 1565/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9918.9035 - val_loss: 8972.1924\n",
      "Epoch 1566/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 9918.4411 - val_loss: 8971.7295\n",
      "Epoch 1567/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9917.9773 - val_loss: 8971.2686\n",
      "Epoch 1568/10000\n",
      "750/750 [==============================] - 0s 73us/step - loss: 9917.5145 - val_loss: 8970.8027\n",
      "Epoch 1569/10000\n",
      "750/750 [==============================] - 0s 74us/step - loss: 9917.0519 - val_loss: 8970.3408\n",
      "Epoch 1570/10000\n",
      "750/750 [==============================] - 0s 79us/step - loss: 9916.5881 - val_loss: 8969.8779\n",
      "Epoch 1571/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9916.1253 - val_loss: 8969.4150\n",
      "Epoch 1572/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9915.6623 - val_loss: 8968.9492\n",
      "Epoch 1573/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9915.1982 - val_loss: 8968.4883\n",
      "Epoch 1574/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 92us/step - loss: 9914.7365 - val_loss: 8968.0264\n",
      "Epoch 1575/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 9914.2727 - val_loss: 8967.5605\n",
      "Epoch 1576/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9913.8087 - val_loss: 8967.0977\n",
      "Epoch 1577/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9913.3467 - val_loss: 8966.6357\n",
      "Epoch 1578/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9912.8825 - val_loss: 8966.1729\n",
      "Epoch 1579/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9912.4198 - val_loss: 8965.7080\n",
      "Epoch 1580/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 9906.26 - 0s 143us/step - loss: 9911.9574 - val_loss: 8965.2451\n",
      "Epoch 1581/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 9911.4930 - val_loss: 8964.7842\n",
      "Epoch 1582/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9911.0303 - val_loss: 8964.3203\n",
      "Epoch 1583/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9910.5684 - val_loss: 8963.8564\n",
      "Epoch 1584/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 9910.1037 - val_loss: 8963.3936\n",
      "Epoch 1585/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 9909.6410 - val_loss: 8962.9307\n",
      "Epoch 1586/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9909.1782 - val_loss: 8962.4668\n",
      "Epoch 1587/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9908.7144 - val_loss: 8962.0049\n",
      "Epoch 1588/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 9908.2521 - val_loss: 8961.5420\n",
      "Epoch 1589/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9907.7881 - val_loss: 8961.0762\n",
      "Epoch 1590/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9907.3251 - val_loss: 8960.6133\n",
      "Epoch 1591/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9906.8626 - val_loss: 8960.1514\n",
      "Epoch 1592/10000\n",
      "750/750 [==============================] - 0s 99us/step - loss: 9906.3982 - val_loss: 8959.6895\n",
      "Epoch 1593/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 9905.9357 - val_loss: 8959.2246\n",
      "Epoch 1594/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 9905.4736 - val_loss: 8958.7617\n",
      "Epoch 1595/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 9905.0089 - val_loss: 8958.2998\n",
      "Epoch 1596/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9904.5460 - val_loss: 8957.8359\n",
      "Epoch 1597/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9904.0838 - val_loss: 8957.3721\n",
      "Epoch 1598/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9903.6198 - val_loss: 8956.9092\n",
      "Epoch 1599/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9903.1572 - val_loss: 8956.4463\n",
      "Epoch 1600/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9902.6940 - val_loss: 8955.9824\n",
      "Epoch 1601/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9902.2303 - val_loss: 8955.5205\n",
      "Epoch 1602/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9901.7685 - val_loss: 8955.0576\n",
      "Epoch 1603/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9901.3040 - val_loss: 8954.5918\n",
      "Epoch 1604/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9900.8408 - val_loss: 8954.1299\n",
      "Epoch 1605/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9900.3789 - val_loss: 8953.6670\n",
      "Epoch 1606/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9899.9145 - val_loss: 8953.2061\n",
      "Epoch 1607/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9899.4517 - val_loss: 8952.7402\n",
      "Epoch 1608/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9898.9893 - val_loss: 8952.2783\n",
      "Epoch 1609/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9898.5256 - val_loss: 8951.8154\n",
      "Epoch 1610/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9898.0624 - val_loss: 8951.3516\n",
      "Epoch 1611/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9897.5998 - val_loss: 8950.8867\n",
      "Epoch 1612/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9897.1355 - val_loss: 8950.4248\n",
      "Epoch 1613/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9896.6740 - val_loss: 8949.9639\n",
      "Epoch 1614/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9896.2098 - val_loss: 8949.4980\n",
      "Epoch 1615/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9895.7462 - val_loss: 8949.0352\n",
      "Epoch 1616/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9895.2842 - val_loss: 8948.5732\n",
      "Epoch 1617/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9894.8200 - val_loss: 8948.1104\n",
      "Epoch 1618/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9894.3574 - val_loss: 8947.6455\n",
      "Epoch 1619/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9893.8944 - val_loss: 8947.1826\n",
      "Epoch 1620/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9893.4302 - val_loss: 8946.7217\n",
      "Epoch 1621/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9892.9676 - val_loss: 8946.2578\n",
      "Epoch 1622/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9892.5057 - val_loss: 8945.7939\n",
      "Epoch 1623/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9892.0410 - val_loss: 8945.3311\n",
      "Epoch 1624/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9891.5784 - val_loss: 8944.8682\n",
      "Epoch 1625/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9891.1157 - val_loss: 8944.4043\n",
      "Epoch 1626/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9890.6518 - val_loss: 8943.9424\n",
      "Epoch 1627/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9890.1896 - val_loss: 8943.4795\n",
      "Epoch 1628/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9889.7258 - val_loss: 8943.0137\n",
      "Epoch 1629/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9889.2625 - val_loss: 8942.5508\n",
      "Epoch 1630/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9888.8000 - val_loss: 8942.0889\n",
      "Epoch 1631/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9888.3359 - val_loss: 8941.6270\n",
      "Epoch 1632/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9887.8731 - val_loss: 8941.1631\n",
      "Epoch 1633/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9887.4111 - val_loss: 8940.6992\n",
      "Epoch 1634/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9886.9465 - val_loss: 8940.2373\n",
      "Epoch 1635/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9886.4836 - val_loss: 8939.7734\n",
      "Epoch 1636/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9886.0213 - val_loss: 8939.3096\n",
      "Epoch 1637/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9885.5572 - val_loss: 8938.8467\n",
      "Epoch 1638/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9885.0948 - val_loss: 8938.3838\n",
      "Epoch 1639/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9884.6315 - val_loss: 8937.9199\n",
      "Epoch 1640/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9884.1673 - val_loss: 8937.4561\n",
      "Epoch 1641/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9883.7059 - val_loss: 8936.9951\n",
      "Epoch 1642/10000\n",
      "750/750 [==============================] - 0s 74us/step - loss: 9883.2417 - val_loss: 8936.5293\n",
      "Epoch 1643/10000\n",
      "750/750 [==============================] - 0s 72us/step - loss: 9882.7779 - val_loss: 8936.0674\n",
      "Epoch 1644/10000\n",
      "750/750 [==============================] - 0s 74us/step - loss: 9882.3161 - val_loss: 8935.6045\n",
      "Epoch 1645/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9881.8514 - val_loss: 8935.1436\n",
      "Epoch 1646/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9881.3891 - val_loss: 8934.6777\n",
      "Epoch 1647/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9880.9269 - val_loss: 8934.2158\n",
      "Epoch 1648/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9880.4629 - val_loss: 8933.7529\n",
      "Epoch 1649/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9879.9996 - val_loss: 8933.2891\n",
      "Epoch 1650/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9879.5373 - val_loss: 8932.8242\n",
      "Epoch 1651/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9879.0731 - val_loss: 8932.3623\n",
      "Epoch 1652/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9878.6110 - val_loss: 8931.9014\n",
      "Epoch 1653/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9878.1473 - val_loss: 8931.4355\n",
      "Epoch 1654/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9877.6834 - val_loss: 8930.9727\n",
      "Epoch 1655/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9877.2215 - val_loss: 8930.5107\n",
      "Epoch 1656/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9876.7576 - val_loss: 8930.0479\n",
      "Epoch 1657/10000\n",
      "750/750 [==============================] - 0s 74us/step - loss: 9876.2945 - val_loss: 8929.5830\n",
      "Epoch 1658/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9875.8320 - val_loss: 8929.1201\n",
      "Epoch 1659/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9875.3676 - val_loss: 8928.6592\n",
      "Epoch 1660/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9874.9051 - val_loss: 8928.1953\n",
      "Epoch 1661/10000\n",
      "750/750 [==============================] - 0s 74us/step - loss: 9874.4429 - val_loss: 8927.7314\n",
      "Epoch 1662/10000\n",
      "750/750 [==============================] - 0s 73us/step - loss: 9873.9783 - val_loss: 8927.2686\n",
      "Epoch 1663/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9873.5157 - val_loss: 8926.8057\n",
      "Epoch 1664/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9873.0532 - val_loss: 8926.3418\n",
      "Epoch 1665/10000\n",
      "750/750 [==============================] - 0s 74us/step - loss: 9872.5888 - val_loss: 8925.8779\n",
      "Epoch 1666/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9872.1268 - val_loss: 8925.4170\n",
      "Epoch 1667/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9871.6633 - val_loss: 8924.9512\n",
      "Epoch 1668/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9871.1997 - val_loss: 8924.4883\n",
      "Epoch 1669/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9870.7370 - val_loss: 8924.0264\n",
      "Epoch 1670/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9870.2732 - val_loss: 8923.5645\n",
      "Epoch 1671/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9869.8103 - val_loss: 8923.0996\n",
      "Epoch 1672/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9869.3485 - val_loss: 8922.6357\n",
      "Epoch 1673/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9868.8837 - val_loss: 8922.1748\n",
      "Epoch 1674/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9868.4209 - val_loss: 8921.7109\n",
      "Epoch 1675/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9867.9586 - val_loss: 8921.2471\n",
      "Epoch 1676/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9867.4945 - val_loss: 8920.7842\n",
      "Epoch 1677/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9867.0319 - val_loss: 8920.3213\n",
      "Epoch 1678/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9866.5689 - val_loss: 8919.8574\n",
      "Epoch 1679/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9866.1048 - val_loss: 8919.3936\n",
      "Epoch 1680/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9865.6431 - val_loss: 8918.9326\n",
      "Epoch 1681/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9865.1792 - val_loss: 8918.4668\n",
      "Epoch 1682/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9864.7158 - val_loss: 8918.0049\n",
      "Epoch 1683/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9864.2539 - val_loss: 8917.5420\n",
      "Epoch 1684/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9863.7890 - val_loss: 8917.0801\n",
      "Epoch 1685/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9863.3263 - val_loss: 8916.6152\n",
      "Epoch 1686/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9862.8641 - val_loss: 8916.1533\n",
      "Epoch 1687/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9862.4002 - val_loss: 8915.6904\n",
      "Epoch 1688/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9861.9367 - val_loss: 8915.2266\n",
      "Epoch 1689/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9861.4745 - val_loss: 8914.7617\n",
      "Epoch 1690/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9861.0103 - val_loss: 8914.2998\n",
      "Epoch 1691/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9860.5485 - val_loss: 8913.8389\n",
      "Epoch 1692/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9860.0846 - val_loss: 8913.3730\n",
      "Epoch 1693/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9859.6206 - val_loss: 8912.9102\n",
      "Epoch 1694/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9859.1590 - val_loss: 8912.4482\n",
      "Epoch 1695/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9858.6948 - val_loss: 8911.9854\n",
      "Epoch 1696/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9858.2316 - val_loss: 8911.5205\n",
      "Epoch 1697/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9857.7693 - val_loss: 8911.0576\n",
      "Epoch 1698/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9857.3051 - val_loss: 8910.5967\n",
      "Epoch 1699/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9856.8423 - val_loss: 8910.1309\n",
      "Epoch 1700/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9856.3801 - val_loss: 8909.6689\n",
      "Epoch 1701/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9855.9157 - val_loss: 8909.2061\n",
      "Epoch 1702/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9855.4535 - val_loss: 8908.7432\n",
      "Epoch 1703/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9854.9904 - val_loss: 8908.2793\n",
      "Epoch 1704/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9854.5264 - val_loss: 8907.8154\n",
      "Epoch 1705/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9854.0642 - val_loss: 8907.3545\n",
      "Epoch 1706/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9853.6007 - val_loss: 8906.8887\n",
      "Epoch 1707/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9853.1371 - val_loss: 8906.4258\n",
      "Epoch 1708/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9852.6748 - val_loss: 8905.9639\n",
      "Epoch 1709/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9852.2107 - val_loss: 8905.5020\n",
      "Epoch 1710/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9851.7478 - val_loss: 8905.0352\n",
      "Epoch 1711/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9851.2857 - val_loss: 8904.5732\n",
      "Epoch 1712/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9850.8212 - val_loss: 8904.1123\n",
      "Epoch 1713/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9850.3583 - val_loss: 8903.6484\n",
      "Epoch 1714/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9849.8961 - val_loss: 8903.1846\n",
      "Epoch 1715/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9849.4317 - val_loss: 8902.7217\n",
      "Epoch 1716/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9848.9695 - val_loss: 8902.2588\n",
      "Epoch 1717/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9848.5064 - val_loss: 8901.7949\n",
      "Epoch 1718/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 88us/step - loss: 9848.0424 - val_loss: 8901.3311\n",
      "Epoch 1719/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9847.5802 - val_loss: 8900.8701\n",
      "Epoch 1720/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9847.1165 - val_loss: 8900.4043\n",
      "Epoch 1721/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9846.6528 - val_loss: 8899.9424\n",
      "Epoch 1722/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9846.1912 - val_loss: 8899.4795\n",
      "Epoch 1723/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9845.7264 - val_loss: 8899.0166\n",
      "Epoch 1724/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9845.2635 - val_loss: 8898.5527\n",
      "Epoch 1725/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9844.8015 - val_loss: 8898.0908\n",
      "Epoch 1726/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9844.3373 - val_loss: 8897.6279\n",
      "Epoch 1727/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9843.8741 - val_loss: 8897.1641\n",
      "Epoch 1728/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9843.4119 - val_loss: 8896.6992\n",
      "Epoch 1729/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9842.9478 - val_loss: 8896.2373\n",
      "Epoch 1730/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9842.4855 - val_loss: 8895.7744\n",
      "Epoch 1731/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9842.0218 - val_loss: 8895.3105\n",
      "Epoch 1732/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9841.5581 - val_loss: 8894.8477\n",
      "Epoch 1733/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9841.0965 - val_loss: 8894.3857\n",
      "Epoch 1734/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9840.6324 - val_loss: 8893.9229\n",
      "Epoch 1735/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9840.1693 - val_loss: 8893.4580\n",
      "Epoch 1736/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9839.7069 - val_loss: 8892.9951\n",
      "Epoch 1737/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9839.2425 - val_loss: 8892.5342\n",
      "Epoch 1738/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9838.7799 - val_loss: 8892.0684\n",
      "Epoch 1739/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9838.3172 - val_loss: 8891.6064\n",
      "Epoch 1740/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9837.8531 - val_loss: 8891.1436\n",
      "Epoch 1741/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9837.3909 - val_loss: 8890.6807\n",
      "Epoch 1742/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9836.9277 - val_loss: 8890.2158\n",
      "Epoch 1743/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9836.4639 - val_loss: 8889.7529\n",
      "Epoch 1744/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9836.0017 - val_loss: 8889.2920\n",
      "Epoch 1745/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9835.5382 - val_loss: 8888.8262\n",
      "Epoch 1746/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9835.0742 - val_loss: 8888.3633\n",
      "Epoch 1747/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9834.6121 - val_loss: 8887.9014\n",
      "Epoch 1748/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9834.1478 - val_loss: 8887.4375\n",
      "Epoch 1749/10000\n",
      "750/750 [==============================] - 0s 74us/step - loss: 9833.6846 - val_loss: 8886.9727\n",
      "Epoch 1750/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9833.2228 - val_loss: 8886.5107\n",
      "Epoch 1751/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9832.7583 - val_loss: 8886.0498\n",
      "Epoch 1752/10000\n",
      "750/750 [==============================] - 0s 74us/step - loss: 9832.2957 - val_loss: 8885.5859\n",
      "Epoch 1753/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9831.8335 - val_loss: 8885.1221\n",
      "Epoch 1754/10000\n",
      "750/750 [==============================] - 0s 74us/step - loss: 9831.3690 - val_loss: 8884.6592\n",
      "Epoch 1755/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9830.9067 - val_loss: 8884.1963\n",
      "Epoch 1756/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9830.4438 - val_loss: 8883.7314\n",
      "Epoch 1757/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9829.9794 - val_loss: 8883.2686\n",
      "Epoch 1758/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9829.5173 - val_loss: 8882.8076\n",
      "Epoch 1759/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9829.0538 - val_loss: 8882.3418\n",
      "Epoch 1760/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9828.5903 - val_loss: 8881.8799\n",
      "Epoch 1761/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9828.1286 - val_loss: 8881.4170\n",
      "Epoch 1762/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9827.6637 - val_loss: 8880.9551\n",
      "Epoch 1763/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9827.2010 - val_loss: 8880.4902\n",
      "Epoch 1764/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9826.7391 - val_loss: 8880.0283\n",
      "Epoch 1765/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9826.2743 - val_loss: 8879.5654\n",
      "Epoch 1766/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9825.8113 - val_loss: 8879.1016\n",
      "Epoch 1767/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9825.3495 - val_loss: 8878.6367\n",
      "Epoch 1768/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9824.8852 - val_loss: 8878.1748\n",
      "Epoch 1769/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9824.4232 - val_loss: 8877.7119\n",
      "Epoch 1770/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9823.9594 - val_loss: 8877.2480\n",
      "Epoch 1771/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9823.4956 - val_loss: 8876.7852\n",
      "Epoch 1772/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9823.0335 - val_loss: 8876.3232\n",
      "Epoch 1773/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9822.5698 - val_loss: 8875.8594\n",
      "Epoch 1774/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9822.1061 - val_loss: 8875.3955\n",
      "Epoch 1775/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9821.6444 - val_loss: 8874.9326\n",
      "Epoch 1776/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9821.1800 - val_loss: 8874.4697\n",
      "Epoch 1777/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9820.7172 - val_loss: 8874.0059\n",
      "Epoch 1778/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9820.2549 - val_loss: 8873.5439\n",
      "Epoch 1779/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9819.7904 - val_loss: 8873.0811\n",
      "Epoch 1780/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9819.3281 - val_loss: 8872.6172\n",
      "Epoch 1781/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9818.8653 - val_loss: 8872.1533\n",
      "Epoch 1782/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9818.4014 - val_loss: 8871.6904\n",
      "Epoch 1783/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9817.9391 - val_loss: 8871.2295\n",
      "Epoch 1784/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9817.4753 - val_loss: 8870.7637\n",
      "Epoch 1785/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9817.0113 - val_loss: 8870.3008\n",
      "Epoch 1786/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9816.5494 - val_loss: 8869.8389\n",
      "Epoch 1787/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9816.0855 - val_loss: 8869.3730\n",
      "Epoch 1788/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9815.6221 - val_loss: 8868.9102\n",
      "Epoch 1789/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9815.1601 - val_loss: 8868.4482\n",
      "Epoch 1790/10000\n",
      "750/750 [==============================] - 0s 79us/step - loss: 9814.6958 - val_loss: 8867.9873\n",
      "Epoch 1791/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9814.2330 - val_loss: 8867.5234\n",
      "Epoch 1792/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9813.7710 - val_loss: 8867.0596\n",
      "Epoch 1793/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9813.3063 - val_loss: 8866.5967\n",
      "Epoch 1794/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9812.8439 - val_loss: 8866.1338\n",
      "Epoch 1795/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9812.3812 - val_loss: 8865.6689\n",
      "Epoch 1796/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9811.9167 - val_loss: 8865.2061\n",
      "Epoch 1797/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9811.4549 - val_loss: 8864.7451\n",
      "Epoch 1798/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9810.9912 - val_loss: 8864.2793\n",
      "Epoch 1799/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9810.5278 - val_loss: 8863.8174\n",
      "Epoch 1800/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9810.0653 - val_loss: 8863.3545\n",
      "Epoch 1801/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9809.6014 - val_loss: 8862.8916\n",
      "Epoch 1802/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9809.1386 - val_loss: 8862.4277\n",
      "Epoch 1803/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9808.6763 - val_loss: 8861.9658\n",
      "Epoch 1804/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9808.2116 - val_loss: 8861.5029\n",
      "Epoch 1805/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9807.7488 - val_loss: 8861.0391\n",
      "Epoch 1806/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9807.2869 - val_loss: 8860.5742\n",
      "Epoch 1807/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9806.8227 - val_loss: 8860.1123\n",
      "Epoch 1808/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9806.3602 - val_loss: 8859.6494\n",
      "Epoch 1809/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9805.8966 - val_loss: 8859.1855\n",
      "Epoch 1810/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9805.4328 - val_loss: 8858.7227\n",
      "Epoch 1811/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9804.9711 - val_loss: 8858.2607\n",
      "Epoch 1812/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9804.5074 - val_loss: 8857.7949\n",
      "Epoch 1813/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9804.0436 - val_loss: 8857.3330\n",
      "Epoch 1814/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9803.5817 - val_loss: 8856.8701\n",
      "Epoch 1815/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9803.1176 - val_loss: 8856.4072\n",
      "Epoch 1816/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9802.6542 - val_loss: 8855.9434\n",
      "Epoch 1817/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9802.1922 - val_loss: 8855.4814\n",
      "Epoch 1818/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9801.7280 - val_loss: 8855.0186\n",
      "Epoch 1819/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9801.2648 - val_loss: 8854.5547\n",
      "Epoch 1820/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9800.8025 - val_loss: 8854.0908\n",
      "Epoch 1821/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9800.3385 - val_loss: 8853.6279\n",
      "Epoch 1822/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9799.8758 - val_loss: 8853.1670\n",
      "Epoch 1823/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9799.4127 - val_loss: 8852.7012\n",
      "Epoch 1824/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9798.9489 - val_loss: 8852.2383\n",
      "Epoch 1825/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9798.4870 - val_loss: 8851.7764\n",
      "Epoch 1826/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9798.0226 - val_loss: 8851.3105\n",
      "Epoch 1827/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9797.5595 - val_loss: 8850.8477\n",
      "Epoch 1828/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9797.0979 - val_loss: 8850.3857\n",
      "Epoch 1829/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9796.6331 - val_loss: 8849.9248\n",
      "Epoch 1830/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9796.1702 - val_loss: 8849.4609\n",
      "Epoch 1831/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9795.7085 - val_loss: 8848.9971\n",
      "Epoch 1832/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9795.2437 - val_loss: 8848.5342\n",
      "Epoch 1833/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9794.7813 - val_loss: 8848.0713\n",
      "Epoch 1834/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9794.3185 - val_loss: 8847.6064\n",
      "Epoch 1835/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9793.8540 - val_loss: 8847.1436\n",
      "Epoch 1836/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9793.3924 - val_loss: 8846.6826\n",
      "Epoch 1837/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9792.9288 - val_loss: 8846.2168\n",
      "Epoch 1838/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9792.4653 - val_loss: 8845.7549\n",
      "Epoch 1839/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9792.0026 - val_loss: 8845.2920\n",
      "Epoch 1840/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9791.5385 - val_loss: 8844.8301\n",
      "Epoch 1841/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9791.0760 - val_loss: 8844.3652\n",
      "Epoch 1842/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9790.6137 - val_loss: 8843.9033\n",
      "Epoch 1843/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9790.1488 - val_loss: 8843.4404\n",
      "Epoch 1844/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9789.6861 - val_loss: 8842.9766\n",
      "Epoch 1845/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9789.2243 - val_loss: 8842.5117\n",
      "Epoch 1846/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9788.7601 - val_loss: 8842.0498\n",
      "Epoch 1847/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9788.2971 - val_loss: 8841.5869\n",
      "Epoch 1848/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9787.8344 - val_loss: 8841.1230\n",
      "Epoch 1849/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9787.3703 - val_loss: 8840.6602\n",
      "Epoch 1850/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9786.9085 - val_loss: 8840.1982\n",
      "Epoch 1851/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9786.4444 - val_loss: 8839.7324\n",
      "Epoch 1852/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9785.9809 - val_loss: 8839.2705\n",
      "Epoch 1853/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9785.5191 - val_loss: 8838.8076\n",
      "Epoch 1854/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9785.0543 - val_loss: 8838.3447\n",
      "Epoch 1855/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9784.5917 - val_loss: 8837.8809\n",
      "Epoch 1856/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9784.1300 - val_loss: 8837.4170\n",
      "Epoch 1857/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9783.6650 - val_loss: 8836.9561\n",
      "Epoch 1858/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9783.2022 - val_loss: 8836.4922\n",
      "Epoch 1859/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9782.7400 - val_loss: 8836.0283\n",
      "Epoch 1860/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9782.2758 - val_loss: 8835.5654\n",
      "Epoch 1861/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9781.8134 - val_loss: 8835.1045\n",
      "Epoch 1862/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 82us/step - loss: 9781.3502 - val_loss: 8834.6387\n",
      "Epoch 1863/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9780.8862 - val_loss: 8834.1758\n",
      "Epoch 1864/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9780.4245 - val_loss: 8833.7139\n",
      "Epoch 1865/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9779.9598 - val_loss: 8833.2480\n",
      "Epoch 1866/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9779.4968 - val_loss: 8832.7852\n",
      "Epoch 1867/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9779.0349 - val_loss: 8832.3232\n",
      "Epoch 1868/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9778.5707 - val_loss: 8831.8623\n",
      "Epoch 1869/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9778.1075 - val_loss: 8831.3984\n",
      "Epoch 1870/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9777.6456 - val_loss: 8830.9346\n",
      "Epoch 1871/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9777.1811 - val_loss: 8830.4717\n",
      "Epoch 1872/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9776.7188 - val_loss: 8830.0088\n",
      "Epoch 1873/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9776.2559 - val_loss: 8829.5439\n",
      "Epoch 1874/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9775.7915 - val_loss: 8829.0811\n",
      "Epoch 1875/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9775.3298 - val_loss: 8828.6201\n",
      "Epoch 1876/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9774.8661 - val_loss: 8828.1543\n",
      "Epoch 1877/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9774.4023 - val_loss: 8827.6924\n",
      "Epoch 1878/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9773.9402 - val_loss: 8827.2295\n",
      "Epoch 1879/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9773.4760 - val_loss: 8826.7666\n",
      "Epoch 1880/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9773.0135 - val_loss: 8826.3027\n",
      "Epoch 1881/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9772.5510 - val_loss: 8825.8408\n",
      "Epoch 1882/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9772.0865 - val_loss: 8825.3779\n",
      "Epoch 1883/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9771.6237 - val_loss: 8824.9141\n",
      "Epoch 1884/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9771.1616 - val_loss: 8824.4492\n",
      "Epoch 1885/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9770.6976 - val_loss: 8823.9873\n",
      "Epoch 1886/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9770.2346 - val_loss: 8823.5244\n",
      "Epoch 1887/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9769.7718 - val_loss: 8823.0605\n",
      "Epoch 1888/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9769.3079 - val_loss: 8822.5967\n",
      "Epoch 1889/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9768.8456 - val_loss: 8822.1357\n",
      "Epoch 1890/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9768.3820 - val_loss: 8821.6699\n",
      "Epoch 1891/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9767.9184 - val_loss: 8821.2080\n",
      "Epoch 1892/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9767.4565 - val_loss: 8820.7451\n",
      "Epoch 1893/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9766.9921 - val_loss: 8820.2822\n",
      "Epoch 1894/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9766.5292 - val_loss: 8819.8184\n",
      "Epoch 1895/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9766.0672 - val_loss: 8819.3545\n",
      "Epoch 1896/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9765.6022 - val_loss: 8818.8936\n",
      "Epoch 1897/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9765.1397 - val_loss: 8818.4297\n",
      "Epoch 1898/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9764.6774 - val_loss: 8817.9658\n",
      "Epoch 1899/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9764.2131 - val_loss: 8817.5029\n",
      "Epoch 1900/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9763.7509 - val_loss: 8817.0420\n",
      "Epoch 1901/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9763.2874 - val_loss: 8816.5762\n",
      "Epoch 1902/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9762.8235 - val_loss: 8816.1133\n",
      "Epoch 1903/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9762.3620 - val_loss: 8815.6514\n",
      "Epoch 1904/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9761.8975 - val_loss: 8815.1855\n",
      "Epoch 1905/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9761.4342 - val_loss: 8814.7227\n",
      "Epoch 1906/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9760.9725 - val_loss: 8814.2607\n",
      "Epoch 1907/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9760.5082 - val_loss: 8813.7998\n",
      "Epoch 1908/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9760.0448 - val_loss: 8813.3340\n",
      "Epoch 1909/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9759.5828 - val_loss: 8812.8721\n",
      "Epoch 1910/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9759.1186 - val_loss: 8812.4092\n",
      "Epoch 1911/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9758.6563 - val_loss: 8811.9463\n",
      "Epoch 1912/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9758.1930 - val_loss: 8811.4814\n",
      "Epoch 1913/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9757.7288 - val_loss: 8811.0186\n",
      "Epoch 1914/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9757.2672 - val_loss: 8810.5557\n",
      "Epoch 1915/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9756.8035 - val_loss: 8810.0918\n",
      "Epoch 1916/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9756.3401 - val_loss: 8809.6299\n",
      "Epoch 1917/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9755.8779 - val_loss: 8809.1670\n",
      "Epoch 1918/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9755.4135 - val_loss: 8808.7051\n",
      "Epoch 1919/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9754.9506 - val_loss: 8808.2402\n",
      "Epoch 1920/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9754.4881 - val_loss: 8807.7764\n",
      "Epoch 1921/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9754.0237 - val_loss: 8807.3154\n",
      "Epoch 1922/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9753.5612 - val_loss: 8806.8516\n",
      "Epoch 1923/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9753.0991 - val_loss: 8806.3867\n",
      "Epoch 1924/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9752.6350 - val_loss: 8805.9248\n",
      "Epoch 1925/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9752.1720 - val_loss: 8805.4619\n",
      "Epoch 1926/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9751.7091 - val_loss: 8804.9980\n",
      "Epoch 1927/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9751.2450 - val_loss: 8804.5342\n",
      "Epoch 1928/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9750.7828 - val_loss: 8804.0732\n",
      "Epoch 1929/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9750.3195 - val_loss: 8803.6074\n",
      "Epoch 1930/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9749.8555 - val_loss: 8803.1455\n",
      "Epoch 1931/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9749.3940 - val_loss: 8802.6826\n",
      "Epoch 1932/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9748.9293 - val_loss: 8802.2197\n",
      "Epoch 1933/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9748.4666 - val_loss: 8801.7549\n",
      "Epoch 1934/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9748.0043 - val_loss: 8801.2920\n",
      "Epoch 1935/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9747.5395 - val_loss: 8800.8311\n",
      "Epoch 1936/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9747.0771 - val_loss: 8800.3672\n",
      "Epoch 1937/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9746.6150 - val_loss: 8799.9033\n",
      "Epoch 1938/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9746.1502 - val_loss: 8799.4404\n",
      "Epoch 1939/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9745.6884 - val_loss: 8798.9795\n",
      "Epoch 1940/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9745.2250 - val_loss: 8798.5137\n",
      "Epoch 1941/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9744.7608 - val_loss: 8798.0508\n",
      "Epoch 1942/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9744.2995 - val_loss: 8797.5889\n",
      "Epoch 1943/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9743.8348 - val_loss: 8797.1230\n",
      "Epoch 1944/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9743.3715 - val_loss: 8796.6602\n",
      "Epoch 1945/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9742.9099 - val_loss: 8796.1982\n",
      "Epoch 1946/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9742.4457 - val_loss: 8795.7354\n",
      "Epoch 1947/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9741.9822 - val_loss: 8795.2715\n",
      "Epoch 1948/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9741.5203 - val_loss: 8794.8096\n",
      "Epoch 1949/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9741.0558 - val_loss: 8794.3467\n",
      "Epoch 1950/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9740.5930 - val_loss: 8793.8838\n",
      "Epoch 1951/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9740.1305 - val_loss: 8793.4189\n",
      "Epoch 1952/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9739.6664 - val_loss: 8792.9561\n",
      "Epoch 1953/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9739.2046 - val_loss: 8792.4932\n",
      "Epoch 1954/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9738.7410 - val_loss: 8792.0293\n",
      "Epoch 1955/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9738.2771 - val_loss: 8791.5674\n",
      "Epoch 1956/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9737.8151 - val_loss: 8791.1045\n",
      "Epoch 1957/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9737.3510 - val_loss: 8790.6416\n",
      "Epoch 1958/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9736.8881 - val_loss: 8790.1758\n",
      "Epoch 1959/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9736.4256 - val_loss: 8789.7139\n",
      "Epoch 1960/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9735.9613 - val_loss: 8789.2529\n",
      "Epoch 1961/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9735.4987 - val_loss: 8788.7881\n",
      "Epoch 1962/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9735.0363 - val_loss: 8788.3242\n",
      "Epoch 1963/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9734.5722 - val_loss: 8787.8623\n",
      "Epoch 1964/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9734.1097 - val_loss: 8787.3984\n",
      "Epoch 1965/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9733.6465 - val_loss: 8786.9346\n",
      "Epoch 1966/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9733.1822 - val_loss: 8786.4717\n",
      "Epoch 1967/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9732.7204 - val_loss: 8786.0107\n",
      "Epoch 1968/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9732.2566 - val_loss: 8785.5449\n",
      "Epoch 1969/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9731.7927 - val_loss: 8785.0830\n",
      "Epoch 1970/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9731.3315 - val_loss: 8784.6201\n",
      "Epoch 1971/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9730.8669 - val_loss: 8784.1572\n",
      "Epoch 1972/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9730.4038 - val_loss: 8783.6924\n",
      "Epoch 1973/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9729.9416 - val_loss: 8783.2295\n",
      "Epoch 1974/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9729.4770 - val_loss: 8782.7686\n",
      "Epoch 1975/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9729.0144 - val_loss: 8782.3047\n",
      "Epoch 1976/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9728.5523 - val_loss: 8781.8408\n",
      "Epoch 1977/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9728.0877 - val_loss: 8781.3779\n",
      "Epoch 1978/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9727.6255 - val_loss: 8780.9160\n",
      "Epoch 1979/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9727.1623 - val_loss: 8780.4512\n",
      "Epoch 1980/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9726.6983 - val_loss: 8779.9883\n",
      "Epoch 1981/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9726.2371 - val_loss: 8779.5264\n",
      "Epoch 1982/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9725.7724 - val_loss: 8779.0605\n",
      "Epoch 1983/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9725.3089 - val_loss: 8778.5977\n",
      "Epoch 1984/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9724.8472 - val_loss: 8778.1357\n",
      "Epoch 1985/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9724.3829 - val_loss: 8777.6729\n",
      "Epoch 1986/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9723.9197 - val_loss: 8777.2090\n",
      "Epoch 1987/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9723.4577 - val_loss: 8776.7471\n",
      "Epoch 1988/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9722.9935 - val_loss: 8776.2842\n",
      "Epoch 1989/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9722.5304 - val_loss: 8775.8213\n",
      "Epoch 1990/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9722.0682 - val_loss: 8775.3564\n",
      "Epoch 1991/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9721.6038 - val_loss: 8774.8936\n",
      "Epoch 1992/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9721.1418 - val_loss: 8774.4307\n",
      "Epoch 1993/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9720.6783 - val_loss: 8773.9668\n",
      "Epoch 1994/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9720.2145 - val_loss: 8773.5049\n",
      "Epoch 1995/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9719.7527 - val_loss: 8773.0420\n",
      "Epoch 1996/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9719.2884 - val_loss: 8772.5771\n",
      "Epoch 1997/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9718.8253 - val_loss: 8772.1133\n",
      "Epoch 1998/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9718.3628 - val_loss: 8771.6514\n",
      "Epoch 1999/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9717.8987 - val_loss: 8771.1904\n",
      "Epoch 2000/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9717.4361 - val_loss: 8770.7246\n",
      "Epoch 2001/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9716.9737 - val_loss: 8770.2617\n",
      "Epoch 2002/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9716.5093 - val_loss: 8769.7998\n",
      "Epoch 2003/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9716.0466 - val_loss: 8769.3359\n",
      "Epoch 2004/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9715.5839 - val_loss: 8768.8721\n",
      "Epoch 2005/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9715.1194 - val_loss: 8768.4092\n",
      "Epoch 2006/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 81us/step - loss: 9714.6577 - val_loss: 8767.9482\n",
      "Epoch 2007/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9714.1939 - val_loss: 8767.4824\n",
      "Epoch 2008/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9713.7301 - val_loss: 8767.0205\n",
      "Epoch 2009/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9713.2690 - val_loss: 8766.5576\n",
      "Epoch 2010/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9712.8042 - val_loss: 8766.0947\n",
      "Epoch 2011/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9712.3414 - val_loss: 8765.6299\n",
      "Epoch 2012/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9711.8795 - val_loss: 8765.1670\n",
      "Epoch 2013/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9711.4144 - val_loss: 8764.7061\n",
      "Epoch 2014/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9710.9519 - val_loss: 8764.2422\n",
      "Epoch 2015/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9710.4898 - val_loss: 8763.7783\n",
      "Epoch 2016/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9710.0250 - val_loss: 8763.3154\n",
      "Epoch 2017/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9709.5630 - val_loss: 8762.8535\n",
      "Epoch 2018/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9709.0999 - val_loss: 8762.3887\n",
      "Epoch 2019/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9708.6360 - val_loss: 8761.9258\n",
      "Epoch 2020/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9708.1738 - val_loss: 8761.4639\n",
      "Epoch 2021/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9707.7099 - val_loss: 8760.9980\n",
      "Epoch 2022/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9707.2466 - val_loss: 8760.5352\n",
      "Epoch 2023/10000\n",
      "750/750 [==============================] - 0s 99us/step - loss: 9706.7843 - val_loss: 8760.0732\n",
      "Epoch 2024/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9706.3203 - val_loss: 8759.6104\n",
      "Epoch 2025/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9705.8572 - val_loss: 8759.1465\n",
      "Epoch 2026/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9705.3954 - val_loss: 8758.6846\n",
      "Epoch 2027/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9704.9309 - val_loss: 8758.2217\n",
      "Epoch 2028/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9704.4679 - val_loss: 8757.7578\n",
      "Epoch 2029/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9704.0055 - val_loss: 8757.2939\n",
      "Epoch 2030/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9703.5413 - val_loss: 8756.8311\n",
      "Epoch 2031/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9703.0793 - val_loss: 8756.3682\n",
      "Epoch 2032/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9702.6154 - val_loss: 8755.9043\n",
      "Epoch 2033/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9702.1518 - val_loss: 8755.4424\n",
      "Epoch 2034/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9701.6901 - val_loss: 8754.9795\n",
      "Epoch 2035/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9701.2257 - val_loss: 8754.5137\n",
      "Epoch 2036/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9700.7623 - val_loss: 8754.0508\n",
      "Epoch 2037/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9700.3003 - val_loss: 8753.5889\n",
      "Epoch 2038/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9699.8354 - val_loss: 8753.1279\n",
      "Epoch 2039/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9699.3731 - val_loss: 8752.6631\n",
      "Epoch 2040/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9698.9110 - val_loss: 8752.1992\n",
      "Epoch 2041/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9698.4467 - val_loss: 8751.7373\n",
      "Epoch 2042/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9697.9841 - val_loss: 8751.2734\n",
      "Epoch 2043/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9697.5213 - val_loss: 8750.8096\n",
      "Epoch 2044/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9697.0570 - val_loss: 8750.3467\n",
      "Epoch 2045/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9696.5954 - val_loss: 8749.8857\n",
      "Epoch 2046/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9696.1313 - val_loss: 8749.4199\n",
      "Epoch 2047/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9695.6676 - val_loss: 8748.9580\n",
      "Epoch 2048/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9695.2060 - val_loss: 8748.4951\n",
      "Epoch 2049/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9694.7417 - val_loss: 8748.0322\n",
      "Epoch 2050/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9694.2789 - val_loss: 8747.5674\n",
      "Epoch 2051/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9693.8166 - val_loss: 8747.1045\n",
      "Epoch 2052/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9693.3518 - val_loss: 8746.6436\n",
      "Epoch 2053/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9692.8894 - val_loss: 8746.1797\n",
      "Epoch 2054/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9692.4275 - val_loss: 8745.7158\n",
      "Epoch 2055/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9691.9625 - val_loss: 8745.2529\n",
      "Epoch 2056/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9691.5002 - val_loss: 8744.7910\n",
      "Epoch 2057/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9691.0373 - val_loss: 8744.3262\n",
      "Epoch 2058/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9690.5734 - val_loss: 8743.8633\n",
      "Epoch 2059/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9690.1114 - val_loss: 8743.4014\n",
      "Epoch 2060/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9689.6474 - val_loss: 8742.9355\n",
      "Epoch 2061/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9689.1839 - val_loss: 8742.4727\n",
      "Epoch 2062/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9688.7215 - val_loss: 8742.0107\n",
      "Epoch 2063/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9688.2577 - val_loss: 8741.5479\n",
      "Epoch 2064/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9687.7945 - val_loss: 8741.0840\n",
      "Epoch 2065/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9687.3328 - val_loss: 8740.6221\n",
      "Epoch 2066/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9686.8684 - val_loss: 8740.1592\n",
      "Epoch 2067/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9686.4051 - val_loss: 8739.6953\n",
      "Epoch 2068/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9685.9430 - val_loss: 8739.2314\n",
      "Epoch 2069/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9685.4786 - val_loss: 8738.7686\n",
      "Epoch 2070/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9685.0163 - val_loss: 8738.3057\n",
      "Epoch 2071/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9684.5530 - val_loss: 8737.8418\n",
      "Epoch 2072/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9684.0889 - val_loss: 8737.3799\n",
      "Epoch 2073/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9683.6277 - val_loss: 8736.9170\n",
      "Epoch 2074/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9683.1630 - val_loss: 8736.4512\n",
      "Epoch 2075/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9682.6997 - val_loss: 8735.9883\n",
      "Epoch 2076/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9682.2379 - val_loss: 8735.5264\n",
      "Epoch 2077/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9681.7731 - val_loss: 8735.0654\n",
      "Epoch 2078/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9681.3106 - val_loss: 8734.5996\n",
      "Epoch 2079/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9680.8484 - val_loss: 8734.1367\n",
      "Epoch 2080/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9680.3840 - val_loss: 8733.6748\n",
      "Epoch 2081/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9679.9215 - val_loss: 8733.2109\n",
      "Epoch 2082/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9679.4589 - val_loss: 8732.7471\n",
      "Epoch 2083/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9678.9945 - val_loss: 8732.2842\n",
      "Epoch 2084/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9678.5326 - val_loss: 8731.8232\n",
      "Epoch 2085/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9678.0689 - val_loss: 8731.3574\n",
      "Epoch 2086/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9677.6048 - val_loss: 8730.8955\n",
      "Epoch 2087/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 9677.1436 - val_loss: 8730.4326\n",
      "Epoch 2088/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9676.6792 - val_loss: 8729.9697\n",
      "Epoch 2089/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9676.2161 - val_loss: 8729.5049\n",
      "Epoch 2090/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9675.7540 - val_loss: 8729.0420\n",
      "Epoch 2091/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9675.2893 - val_loss: 8728.5801\n",
      "Epoch 2092/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9674.8268 - val_loss: 8728.1152\n",
      "Epoch 2093/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9674.3641 - val_loss: 8727.6533\n",
      "Epoch 2094/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9673.9000 - val_loss: 8727.1904\n",
      "Epoch 2095/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9673.4374 - val_loss: 8726.7285\n",
      "Epoch 2096/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9672.9749 - val_loss: 8726.2637\n",
      "Epoch 2097/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9672.5109 - val_loss: 8725.8008\n",
      "Epoch 2098/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9672.0487 - val_loss: 8725.3389\n",
      "Epoch 2099/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9671.5849 - val_loss: 8724.8730\n",
      "Epoch 2100/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9671.1212 - val_loss: 8724.4102\n",
      "Epoch 2101/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9670.6592 - val_loss: 8723.9482\n",
      "Epoch 2102/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9670.1949 - val_loss: 8723.4854\n",
      "Epoch 2103/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 9705.87 - 0s 81us/step - loss: 9669.7319 - val_loss: 8723.0215\n",
      "Epoch 2104/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9669.2701 - val_loss: 8722.5596\n",
      "Epoch 2105/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9668.8056 - val_loss: 8722.0967\n",
      "Epoch 2106/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9668.3425 - val_loss: 8721.6328\n",
      "Epoch 2107/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9667.8803 - val_loss: 8721.1689\n",
      "Epoch 2108/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9667.4162 - val_loss: 8720.7061\n",
      "Epoch 2109/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9666.9535 - val_loss: 8720.2432\n",
      "Epoch 2110/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9666.4905 - val_loss: 8719.7793\n",
      "Epoch 2111/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9666.0260 - val_loss: 8719.3154\n",
      "Epoch 2112/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9665.5647 - val_loss: 8718.8545\n",
      "Epoch 2113/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9665.1004 - val_loss: 8718.3887\n",
      "Epoch 2114/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9664.6370 - val_loss: 8717.9258\n",
      "Epoch 2115/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 9628.16 - 0s 80us/step - loss: 9664.1753 - val_loss: 8717.4639\n",
      "Epoch 2116/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9663.7104 - val_loss: 8717.0029\n",
      "Epoch 2117/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9663.2481 - val_loss: 8716.5381\n",
      "Epoch 2118/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9662.7856 - val_loss: 8716.0742\n",
      "Epoch 2119/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9662.3213 - val_loss: 8715.6123\n",
      "Epoch 2120/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9661.8590 - val_loss: 8715.1484\n",
      "Epoch 2121/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9661.3963 - val_loss: 8714.6846\n",
      "Epoch 2122/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9660.9321 - val_loss: 8714.2217\n",
      "Epoch 2123/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9660.4702 - val_loss: 8713.7607\n",
      "Epoch 2124/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9660.0064 - val_loss: 8713.2949\n",
      "Epoch 2125/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9659.5423 - val_loss: 8712.8330\n",
      "Epoch 2126/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9659.0809 - val_loss: 8712.3701\n",
      "Epoch 2127/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9658.6167 - val_loss: 8711.9072\n",
      "Epoch 2128/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9658.1537 - val_loss: 8711.4424\n",
      "Epoch 2129/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9657.6910 - val_loss: 8710.9795\n",
      "Epoch 2130/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9657.2268 - val_loss: 8710.5166\n",
      "Epoch 2131/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9656.7640 - val_loss: 8710.0527\n",
      "Epoch 2132/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9656.3016 - val_loss: 8709.5908\n",
      "Epoch 2133/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9655.8374 - val_loss: 8709.1279\n",
      "Epoch 2134/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9655.3750 - val_loss: 8708.6660\n",
      "Epoch 2135/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9654.9120 - val_loss: 8708.2012\n",
      "Epoch 2136/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9654.4485 - val_loss: 8707.7373\n",
      "Epoch 2137/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9653.9861 - val_loss: 8707.2764\n",
      "Epoch 2138/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9653.5224 - val_loss: 8706.8105\n",
      "Epoch 2139/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9653.0585 - val_loss: 8706.3477\n",
      "Epoch 2140/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9652.5965 - val_loss: 8705.8857\n",
      "Epoch 2141/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9652.1323 - val_loss: 8705.4229\n",
      "Epoch 2142/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9651.6695 - val_loss: 8704.9590\n",
      "Epoch 2143/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9651.2076 - val_loss: 8704.4951\n",
      "Epoch 2144/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9650.7425 - val_loss: 8704.0342\n",
      "Epoch 2145/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9650.2798 - val_loss: 8703.5703\n",
      "Epoch 2146/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9649.8176 - val_loss: 8703.1064\n",
      "Epoch 2147/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9649.3530 - val_loss: 8702.6436\n",
      "Epoch 2148/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9648.8906 - val_loss: 8702.1807\n",
      "Epoch 2149/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9648.4277 - val_loss: 8701.7168\n",
      "Epoch 2150/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 86us/step - loss: 9647.9635 - val_loss: 8701.2529\n",
      "Epoch 2151/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9647.5022 - val_loss: 8700.7920\n",
      "Epoch 2152/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9647.0381 - val_loss: 8700.3262\n",
      "Epoch 2153/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9646.5747 - val_loss: 8699.8633\n",
      "Epoch 2154/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9646.1127 - val_loss: 8699.4014\n",
      "Epoch 2155/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9645.6479 - val_loss: 8698.9404\n",
      "Epoch 2156/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9645.1857 - val_loss: 8698.4746\n",
      "Epoch 2157/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9644.7234 - val_loss: 8698.0117\n",
      "Epoch 2158/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9644.2586 - val_loss: 8697.5498\n",
      "Epoch 2159/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9643.7965 - val_loss: 8697.0859\n",
      "Epoch 2160/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9643.3337 - val_loss: 8696.6221\n",
      "Epoch 2161/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9642.8694 - val_loss: 8696.1592\n",
      "Epoch 2162/10000\n",
      "750/750 [==============================] - 0s 79us/step - loss: 9642.4076 - val_loss: 8695.6963\n",
      "Epoch 2163/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9641.9436 - val_loss: 8695.2324\n",
      "Epoch 2164/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9641.4799 - val_loss: 8694.7705\n",
      "Epoch 2165/10000\n",
      "750/750 [==============================] - 0s 79us/step - loss: 9641.0176 - val_loss: 8694.3076\n",
      "Epoch 2166/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9640.5540 - val_loss: 8693.8447\n",
      "Epoch 2167/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9640.0908 - val_loss: 8693.3799\n",
      "Epoch 2168/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9639.6286 - val_loss: 8692.9170\n",
      "Epoch 2169/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9639.1643 - val_loss: 8692.4551\n",
      "Epoch 2170/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9638.7015 - val_loss: 8691.9902\n",
      "Epoch 2171/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9638.2391 - val_loss: 8691.5283\n",
      "Epoch 2172/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9637.7750 - val_loss: 8691.0654\n",
      "Epoch 2173/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9637.3118 - val_loss: 8690.6035\n",
      "Epoch 2174/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9636.8496 - val_loss: 8690.1387\n",
      "Epoch 2175/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9636.3855 - val_loss: 8689.6748\n",
      "Epoch 2176/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9635.9236 - val_loss: 8689.2139\n",
      "Epoch 2177/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9635.4598 - val_loss: 8688.7480\n",
      "Epoch 2178/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9634.9960 - val_loss: 8688.2852\n",
      "Epoch 2179/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9634.5338 - val_loss: 8687.8232\n",
      "Epoch 2180/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9634.0698 - val_loss: 8687.3574\n",
      "Epoch 2181/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9633.6065 - val_loss: 8686.8955\n",
      "Epoch 2182/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9633.1450 - val_loss: 8686.4326\n",
      "Epoch 2183/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9632.6800 - val_loss: 8685.9717\n",
      "Epoch 2184/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9632.2170 - val_loss: 8685.5078\n",
      "Epoch 2185/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 9567.61 - 0s 78us/step - loss: 9631.7553 - val_loss: 8685.0439\n",
      "Epoch 2186/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9631.2906 - val_loss: 8684.5811\n",
      "Epoch 2187/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9630.8281 - val_loss: 8684.1182\n",
      "Epoch 2188/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9630.3650 - val_loss: 8683.6533\n",
      "Epoch 2189/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9629.9009 - val_loss: 8683.1904\n",
      "Epoch 2190/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9629.4397 - val_loss: 8682.7295\n",
      "Epoch 2191/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9628.9755 - val_loss: 8682.2637\n",
      "Epoch 2192/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9628.5121 - val_loss: 8681.8008\n",
      "Epoch 2193/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9628.0501 - val_loss: 8681.3389\n",
      "Epoch 2194/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9627.5854 - val_loss: 8680.8779\n",
      "Epoch 2195/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9627.1228 - val_loss: 8680.4131\n",
      "Epoch 2196/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9626.6609 - val_loss: 8679.9492\n",
      "Epoch 2197/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9626.1960 - val_loss: 8679.4873\n",
      "Epoch 2198/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9625.7334 - val_loss: 8679.0234\n",
      "Epoch 2199/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9625.2712 - val_loss: 8678.5596\n",
      "Epoch 2200/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9624.8070 - val_loss: 8678.0967\n",
      "Epoch 2201/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9624.3446 - val_loss: 8677.6338\n",
      "Epoch 2202/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9623.8809 - val_loss: 8677.1699\n",
      "Epoch 2203/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9623.4173 - val_loss: 8676.7080\n",
      "Epoch 2204/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9622.9552 - val_loss: 8676.2451\n",
      "Epoch 2205/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9622.4917 - val_loss: 8675.7822\n",
      "Epoch 2206/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9622.0284 - val_loss: 8675.3174\n",
      "Epoch 2207/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9621.5660 - val_loss: 8674.8545\n",
      "Epoch 2208/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9621.1019 - val_loss: 8674.3916\n",
      "Epoch 2209/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9620.6388 - val_loss: 8673.9277\n",
      "Epoch 2210/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9620.1765 - val_loss: 8673.4658\n",
      "Epoch 2211/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9619.7125 - val_loss: 8673.0029\n",
      "Epoch 2212/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9619.2494 - val_loss: 8672.5410\n",
      "Epoch 2213/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9618.7871 - val_loss: 8672.0742\n",
      "Epoch 2214/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9618.3227 - val_loss: 8671.6123\n",
      "Epoch 2215/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9617.8610 - val_loss: 8671.1514\n",
      "Epoch 2216/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9617.3970 - val_loss: 8670.6855\n",
      "Epoch 2217/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9616.9329 - val_loss: 8670.2227\n",
      "Epoch 2218/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9616.4714 - val_loss: 8669.7607\n",
      "Epoch 2219/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9616.0069 - val_loss: 8669.2949\n",
      "Epoch 2220/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9615.5436 - val_loss: 8668.8330\n",
      "Epoch 2221/10000\n",
      "750/750 [==============================] - 0s 79us/step - loss: 9615.0822 - val_loss: 8668.3701\n",
      "Epoch 2222/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9614.6171 - val_loss: 8667.9092\n",
      "Epoch 2223/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9614.1545 - val_loss: 8667.4453\n",
      "Epoch 2224/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9613.6927 - val_loss: 8666.9814\n",
      "Epoch 2225/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9613.2280 - val_loss: 8666.5186\n",
      "Epoch 2226/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9612.7655 - val_loss: 8666.0557\n",
      "Epoch 2227/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9612.3027 - val_loss: 8665.5908\n",
      "Epoch 2228/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9611.8384 - val_loss: 8665.1279\n",
      "Epoch 2229/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9611.3771 - val_loss: 8664.6670\n",
      "Epoch 2230/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9610.9130 - val_loss: 8664.2012\n",
      "Epoch 2231/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9610.4495 - val_loss: 8663.7383\n",
      "Epoch 2232/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9609.9877 - val_loss: 8663.2764\n",
      "Epoch 2233/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9609.5228 - val_loss: 8662.8145\n",
      "Epoch 2234/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9609.0603 - val_loss: 8662.3496\n",
      "Epoch 2235/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9608.5982 - val_loss: 8661.8867\n",
      "Epoch 2236/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9608.1334 - val_loss: 8661.4248\n",
      "Epoch 2237/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9607.6707 - val_loss: 8660.9609\n",
      "Epoch 2238/10000\n",
      "750/750 [==============================] - 0s 79us/step - loss: 9607.2087 - val_loss: 8660.4971\n",
      "Epoch 2239/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9606.7443 - val_loss: 8660.0342\n",
      "Epoch 2240/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9606.2816 - val_loss: 8659.5713\n",
      "Epoch 2241/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9605.8185 - val_loss: 8659.1074\n",
      "Epoch 2242/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9605.3546 - val_loss: 8658.6455\n",
      "Epoch 2243/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9604.8927 - val_loss: 8658.1826\n",
      "Epoch 2244/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9604.4291 - val_loss: 8657.7168\n",
      "Epoch 2245/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9603.9653 - val_loss: 8657.2549\n",
      "Epoch 2246/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9603.5035 - val_loss: 8656.7920\n",
      "Epoch 2247/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9603.0390 - val_loss: 8656.3301\n",
      "Epoch 2248/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9602.5758 - val_loss: 8655.8652\n",
      "Epoch 2249/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9602.1138 - val_loss: 8655.4033\n",
      "Epoch 2250/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9601.6500 - val_loss: 8654.9404\n",
      "Epoch 2251/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9601.1865 - val_loss: 8654.4766\n",
      "Epoch 2252/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9600.7242 - val_loss: 8654.0117\n",
      "Epoch 2253/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9600.2596 - val_loss: 8653.5498\n",
      "Epoch 2254/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9599.7978 - val_loss: 8653.0889\n",
      "Epoch 2255/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9599.3344 - val_loss: 8652.6230\n",
      "Epoch 2256/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9598.8703 - val_loss: 8652.1602\n",
      "Epoch 2257/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9598.4087 - val_loss: 8651.6982\n",
      "Epoch 2258/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9597.9443 - val_loss: 8651.2324\n",
      "Epoch 2259/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9597.4812 - val_loss: 8650.7705\n",
      "Epoch 2260/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9597.0197 - val_loss: 8650.3076\n",
      "Epoch 2261/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9596.5549 - val_loss: 8649.8467\n",
      "Epoch 2262/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9596.0920 - val_loss: 8649.3828\n",
      "Epoch 2263/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9595.6305 - val_loss: 8648.9189\n",
      "Epoch 2264/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9595.1655 - val_loss: 8648.4561\n",
      "Epoch 2265/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9594.7029 - val_loss: 8647.9932\n",
      "Epoch 2266/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9594.2401 - val_loss: 8647.5283\n",
      "Epoch 2267/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9593.7759 - val_loss: 8647.0654\n",
      "Epoch 2268/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9593.3140 - val_loss: 8646.6045\n",
      "Epoch 2269/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9592.8506 - val_loss: 8646.1387\n",
      "Epoch 2270/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9592.3871 - val_loss: 8645.6758\n",
      "Epoch 2271/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9591.9252 - val_loss: 8645.2139\n",
      "Epoch 2272/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9591.4603 - val_loss: 8644.7520\n",
      "Epoch 2273/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9590.9975 - val_loss: 8644.2881\n",
      "Epoch 2274/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9590.5352 - val_loss: 8643.8232\n",
      "Epoch 2275/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9590.0707 - val_loss: 8643.3623\n",
      "Epoch 2276/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9589.6081 - val_loss: 8642.8984\n",
      "Epoch 2277/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9589.1460 - val_loss: 8642.4346\n",
      "Epoch 2278/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9588.6820 - val_loss: 8641.9717\n",
      "Epoch 2279/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9588.2193 - val_loss: 8641.5088\n",
      "Epoch 2280/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9587.7559 - val_loss: 8641.0449\n",
      "Epoch 2281/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9587.2922 - val_loss: 8640.5830\n",
      "Epoch 2282/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9586.8302 - val_loss: 8640.1201\n",
      "Epoch 2283/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9586.3665 - val_loss: 8639.6543\n",
      "Epoch 2284/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9585.9027 - val_loss: 8639.1924\n",
      "Epoch 2285/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9585.4409 - val_loss: 8638.7295\n",
      "Epoch 2286/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9584.9768 - val_loss: 8638.2666\n",
      "Epoch 2287/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9584.5135 - val_loss: 8637.8027\n",
      "Epoch 2288/10000\n",
      "750/750 [==============================] - 0s 79us/step - loss: 9584.0515 - val_loss: 8637.3408\n",
      "Epoch 2289/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9583.5873 - val_loss: 8636.8779\n",
      "Epoch 2290/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9583.1240 - val_loss: 8636.4141\n",
      "Epoch 2291/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9582.6617 - val_loss: 8635.9492\n",
      "Epoch 2292/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9582.1975 - val_loss: 8635.4873\n",
      "Epoch 2293/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9581.7354 - val_loss: 8635.0254\n",
      "Epoch 2294/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 82us/step - loss: 9581.2718 - val_loss: 8634.5605\n",
      "Epoch 2295/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9580.8077 - val_loss: 8634.0977\n",
      "Epoch 2296/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9580.3461 - val_loss: 8633.6357\n",
      "Epoch 2297/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9579.8818 - val_loss: 8633.1699\n",
      "Epoch 2298/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9579.4187 - val_loss: 8632.7080\n",
      "Epoch 2299/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9578.9569 - val_loss: 8632.2451\n",
      "Epoch 2300/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9578.4926 - val_loss: 8631.7842\n",
      "Epoch 2301/10000\n",
      "750/750 [==============================] - 0s 79us/step - loss: 9578.0295 - val_loss: 8631.3203\n",
      "Epoch 2302/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9577.5677 - val_loss: 8630.8564\n",
      "Epoch 2303/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9577.1030 - val_loss: 8630.3936\n",
      "Epoch 2304/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9576.6407 - val_loss: 8629.9307\n",
      "Epoch 2305/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9576.1775 - val_loss: 8629.4658\n",
      "Epoch 2306/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9575.7134 - val_loss: 8629.0029\n",
      "Epoch 2307/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9575.2514 - val_loss: 8628.5410\n",
      "Epoch 2308/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9574.7879 - val_loss: 8628.0762\n",
      "Epoch 2309/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9574.3243 - val_loss: 8627.6133\n",
      "Epoch 2310/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9573.8622 - val_loss: 8627.1514\n",
      "Epoch 2311/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9573.3978 - val_loss: 8626.6895\n",
      "Epoch 2312/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9572.9350 - val_loss: 8626.2246\n",
      "Epoch 2313/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9572.4724 - val_loss: 8625.7617\n",
      "Epoch 2314/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9572.0083 - val_loss: 8625.2988\n",
      "Epoch 2315/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9571.5457 - val_loss: 8624.8359\n",
      "Epoch 2316/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9571.0833 - val_loss: 8624.3721\n",
      "Epoch 2317/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9570.6188 - val_loss: 8623.9092\n",
      "Epoch 2318/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9570.1565 - val_loss: 8623.4463\n",
      "Epoch 2319/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9569.6933 - val_loss: 8622.9824\n",
      "Epoch 2320/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9569.2297 - val_loss: 8622.5195\n",
      "Epoch 2321/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9568.7676 - val_loss: 8622.0576\n",
      "Epoch 2322/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9568.3036 - val_loss: 8621.5918\n",
      "Epoch 2323/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9567.8404 - val_loss: 8621.1299\n",
      "Epoch 2324/10000\n",
      "750/750 [==============================] - 0s 79us/step - loss: 9567.3783 - val_loss: 8620.6670\n",
      "Epoch 2325/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9566.9135 - val_loss: 8620.2051\n",
      "Epoch 2326/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9566.4508 - val_loss: 8619.7402\n",
      "Epoch 2327/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9565.9888 - val_loss: 8619.2764\n",
      "Epoch 2328/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9565.5245 - val_loss: 8618.8145\n",
      "Epoch 2329/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9565.0616 - val_loss: 8618.3516\n",
      "Epoch 2330/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9564.5991 - val_loss: 8617.8867\n",
      "Epoch 2331/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9564.1343 - val_loss: 8617.4248\n",
      "Epoch 2332/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9563.6729 - val_loss: 8616.9629\n",
      "Epoch 2333/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9563.2093 - val_loss: 8616.4980\n",
      "Epoch 2334/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9562.7456 - val_loss: 8616.0352\n",
      "Epoch 2335/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9562.2837 - val_loss: 8615.5732\n",
      "Epoch 2336/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9561.8193 - val_loss: 8615.1074\n",
      "Epoch 2337/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9561.3561 - val_loss: 8614.6455\n",
      "Epoch 2338/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9560.8942 - val_loss: 8614.1826\n",
      "Epoch 2339/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9560.4295 - val_loss: 8613.7207\n",
      "Epoch 2340/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9559.9671 - val_loss: 8613.2559\n",
      "Epoch 2341/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9559.5046 - val_loss: 8612.7939\n",
      "Epoch 2342/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9559.0405 - val_loss: 8612.3311\n",
      "Epoch 2343/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9558.5782 - val_loss: 8611.8672\n",
      "Epoch 2344/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9558.1151 - val_loss: 8611.4033\n",
      "Epoch 2345/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9557.6508 - val_loss: 8610.9404\n",
      "Epoch 2346/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9557.1889 - val_loss: 8610.4785\n",
      "Epoch 2347/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9556.7255 - val_loss: 8610.0137\n",
      "Epoch 2348/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9556.2616 - val_loss: 8609.5508\n",
      "Epoch 2349/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9555.7993 - val_loss: 8609.0889\n",
      "Epoch 2350/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9555.3352 - val_loss: 8608.6270\n",
      "Epoch 2351/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9554.8724 - val_loss: 8608.1631\n",
      "Epoch 2352/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9554.4101 - val_loss: 8607.6982\n",
      "Epoch 2353/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9553.9457 - val_loss: 8607.2363\n",
      "Epoch 2354/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9553.4830 - val_loss: 8606.7715\n",
      "Epoch 2355/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9553.0202 - val_loss: 8606.3096\n",
      "Epoch 2356/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9552.5563 - val_loss: 8605.8467\n",
      "Epoch 2357/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9552.0937 - val_loss: 8605.3838\n",
      "Epoch 2358/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9551.6309 - val_loss: 8604.9199\n",
      "Epoch 2359/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9551.1667 - val_loss: 8604.4570\n",
      "Epoch 2360/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9550.7051 - val_loss: 8603.9941\n",
      "Epoch 2361/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9550.2409 - val_loss: 8603.5293\n",
      "Epoch 2362/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9549.7778 - val_loss: 8603.0674\n",
      "Epoch 2363/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9549.3159 - val_loss: 8602.6045\n",
      "Epoch 2364/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9548.8510 - val_loss: 8602.1416\n",
      "Epoch 2365/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9548.3885 - val_loss: 8601.6777\n",
      "Epoch 2366/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9547.9261 - val_loss: 8601.2139\n",
      "Epoch 2367/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9547.4614 - val_loss: 8600.7520\n",
      "Epoch 2368/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9546.9986 - val_loss: 8600.2891\n",
      "Epoch 2369/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9546.5368 - val_loss: 8599.8242\n",
      "Epoch 2370/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9546.0720 - val_loss: 8599.3623\n",
      "Epoch 2371/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9545.6104 - val_loss: 8598.9004\n",
      "Epoch 2372/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9545.1468 - val_loss: 8598.4355\n",
      "Epoch 2373/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9544.6829 - val_loss: 8597.9717\n",
      "Epoch 2374/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9544.2212 - val_loss: 8597.5098\n",
      "Epoch 2375/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9543.7568 - val_loss: 8597.0449\n",
      "Epoch 2376/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9543.2933 - val_loss: 8596.5830\n",
      "Epoch 2377/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9542.8315 - val_loss: 8596.1201\n",
      "Epoch 2378/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9542.3676 - val_loss: 8595.6572\n",
      "Epoch 2379/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9541.9045 - val_loss: 8595.1934\n",
      "Epoch 2380/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9541.4418 - val_loss: 8594.7305\n",
      "Epoch 2381/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9540.9780 - val_loss: 8594.2686\n",
      "Epoch 2382/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9540.5157 - val_loss: 8593.8047\n",
      "Epoch 2383/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9540.0523 - val_loss: 8593.3408\n",
      "Epoch 2384/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9539.5881 - val_loss: 8592.8779\n",
      "Epoch 2385/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9539.1265 - val_loss: 8592.4160\n",
      "Epoch 2386/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9538.6627 - val_loss: 8591.9512\n",
      "Epoch 2387/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9538.1992 - val_loss: 8591.4883\n",
      "Epoch 2388/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9537.7369 - val_loss: 8591.0264\n",
      "Epoch 2389/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9537.2729 - val_loss: 8590.5645\n",
      "Epoch 2390/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9536.8095 - val_loss: 8590.0977\n",
      "Epoch 2391/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9536.3473 - val_loss: 8589.6357\n",
      "Epoch 2392/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9535.8831 - val_loss: 8589.1738\n",
      "Epoch 2393/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9535.4204 - val_loss: 8588.7109\n",
      "Epoch 2394/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9534.9581 - val_loss: 8588.2471\n",
      "Epoch 2395/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9534.4935 - val_loss: 8587.7842\n",
      "Epoch 2396/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9534.0310 - val_loss: 8587.3213\n",
      "Epoch 2397/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9533.5685 - val_loss: 8586.8564\n",
      "Epoch 2398/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9533.1039 - val_loss: 8586.3936\n",
      "Epoch 2399/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9532.6419 - val_loss: 8585.9316\n",
      "Epoch 2400/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9532.1783 - val_loss: 8585.4668\n",
      "Epoch 2401/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9531.7144 - val_loss: 8585.0049\n",
      "Epoch 2402/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9531.2534 - val_loss: 8584.5420\n",
      "Epoch 2403/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9530.7885 - val_loss: 8584.0801\n",
      "Epoch 2404/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9530.3255 - val_loss: 8583.6152\n",
      "Epoch 2405/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9529.8638 - val_loss: 8583.1514\n",
      "Epoch 2406/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9529.3989 - val_loss: 8582.6895\n",
      "Epoch 2407/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9528.9361 - val_loss: 8582.2266\n",
      "Epoch 2408/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9528.4742 - val_loss: 8581.7617\n",
      "Epoch 2409/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9528.0093 - val_loss: 8581.2998\n",
      "Epoch 2410/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9527.5472 - val_loss: 8580.8379\n",
      "Epoch 2411/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9527.0843 - val_loss: 8580.3730\n",
      "Epoch 2412/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9526.6205 - val_loss: 8579.9092\n",
      "Epoch 2413/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9526.1585 - val_loss: 8579.4482\n",
      "Epoch 2414/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9525.6942 - val_loss: 8578.9824\n",
      "Epoch 2415/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9525.2310 - val_loss: 8578.5205\n",
      "Epoch 2416/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9524.7689 - val_loss: 8578.0576\n",
      "Epoch 2417/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9524.3044 - val_loss: 8577.5957\n",
      "Epoch 2418/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9523.8416 - val_loss: 8577.1309\n",
      "Epoch 2419/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9523.3795 - val_loss: 8576.6680\n",
      "Epoch 2420/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9522.9154 - val_loss: 8576.2061\n",
      "Epoch 2421/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9522.4522 - val_loss: 8575.7422\n",
      "Epoch 2422/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9521.9899 - val_loss: 8575.2783\n",
      "Epoch 2423/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9521.5258 - val_loss: 8574.8154\n",
      "Epoch 2424/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9521.0630 - val_loss: 8574.3535\n",
      "Epoch 2425/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9520.5999 - val_loss: 8573.8887\n",
      "Epoch 2426/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9520.1360 - val_loss: 8573.4258\n",
      "Epoch 2427/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9519.6744 - val_loss: 8572.9639\n",
      "Epoch 2428/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9519.2102 - val_loss: 8572.5020\n",
      "Epoch 2429/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9518.7475 - val_loss: 8572.0352\n",
      "Epoch 2430/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9518.2845 - val_loss: 8571.5732\n",
      "Epoch 2431/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9517.8205 - val_loss: 8571.1113\n",
      "Epoch 2432/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9517.3578 - val_loss: 8570.6465\n",
      "Epoch 2433/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9516.8951 - val_loss: 8570.1846\n",
      "Epoch 2434/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9516.4311 - val_loss: 8569.7217\n",
      "Epoch 2435/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9515.9687 - val_loss: 8569.2588\n",
      "Epoch 2436/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9515.5058 - val_loss: 8568.7939\n",
      "Epoch 2437/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9515.0414 - val_loss: 8568.3311\n",
      "Epoch 2438/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 84us/step - loss: 9514.5794 - val_loss: 8567.8691\n",
      "Epoch 2439/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9514.1157 - val_loss: 8567.4043\n",
      "Epoch 2440/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9513.6520 - val_loss: 8566.9424\n",
      "Epoch 2441/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9513.1906 - val_loss: 8566.4795\n",
      "Epoch 2442/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9512.7260 - val_loss: 8566.0166\n",
      "Epoch 2443/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9512.2630 - val_loss: 8565.5508\n",
      "Epoch 2444/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9511.8012 - val_loss: 8565.0889\n",
      "Epoch 2445/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9511.3363 - val_loss: 8564.6270\n",
      "Epoch 2446/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9510.8740 - val_loss: 8564.1641\n",
      "Epoch 2447/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9510.4116 - val_loss: 8563.6992\n",
      "Epoch 2448/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9509.9469 - val_loss: 8563.2373\n",
      "Epoch 2449/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9509.4853 - val_loss: 8562.7744\n",
      "Epoch 2450/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9509.0217 - val_loss: 8562.3105\n",
      "Epoch 2451/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9508.5580 - val_loss: 8561.8467\n",
      "Epoch 2452/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9508.0961 - val_loss: 8561.3848\n",
      "Epoch 2453/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9507.6318 - val_loss: 8560.9199\n",
      "Epoch 2454/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9507.1685 - val_loss: 8560.4580\n",
      "Epoch 2455/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9506.7066 - val_loss: 8559.9951\n",
      "Epoch 2456/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9506.2418 - val_loss: 8559.5322\n",
      "Epoch 2457/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9505.7790 - val_loss: 8559.0684\n",
      "Epoch 2458/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9505.3168 - val_loss: 8558.6055\n",
      "Epoch 2459/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9504.8523 - val_loss: 8558.1436\n",
      "Epoch 2460/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9504.3903 - val_loss: 8557.6797\n",
      "Epoch 2461/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9503.9272 - val_loss: 8557.2158\n",
      "Epoch 2462/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9503.4633 - val_loss: 8556.7529\n",
      "Epoch 2463/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 9503.0006 - val_loss: 8556.2910\n",
      "Epoch 2464/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9502.5373 - val_loss: 8555.8262\n",
      "Epoch 2465/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9502.0736 - val_loss: 8555.3633\n",
      "Epoch 2466/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9501.6119 - val_loss: 8554.9014\n",
      "Epoch 2467/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9501.1474 - val_loss: 8554.4355\n",
      "Epoch 2468/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9500.6840 - val_loss: 8553.9727\n",
      "Epoch 2469/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9500.2221 - val_loss: 8553.5107\n",
      "Epoch 2470/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9499.7580 - val_loss: 8553.0488\n",
      "Epoch 2471/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9499.2952 - val_loss: 8552.5840\n",
      "Epoch 2472/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9498.8327 - val_loss: 8552.1211\n",
      "Epoch 2473/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9498.3681 - val_loss: 8551.6592\n",
      "Epoch 2474/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9497.9058 - val_loss: 8551.1953\n",
      "Epoch 2475/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9497.4432 - val_loss: 8550.7324\n",
      "Epoch 2476/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9496.9790 - val_loss: 8550.2686\n",
      "Epoch 2477/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9496.5168 - val_loss: 8549.8066\n",
      "Epoch 2478/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9496.0531 - val_loss: 8549.3418\n",
      "Epoch 2479/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9495.5895 - val_loss: 8548.8789\n",
      "Epoch 2480/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9495.1281 - val_loss: 8548.4170\n",
      "Epoch 2481/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9494.6634 - val_loss: 8547.9551\n",
      "Epoch 2482/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9494.2005 - val_loss: 8547.4883\n",
      "Epoch 2483/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9493.7381 - val_loss: 8547.0264\n",
      "Epoch 2484/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9493.2737 - val_loss: 8546.5645\n",
      "Epoch 2485/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9492.8113 - val_loss: 8546.1016\n",
      "Epoch 2486/10000\n",
      "750/750 [==============================] - 0s 79us/step - loss: 9492.3488 - val_loss: 8545.6367\n",
      "Epoch 2487/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9491.8843 - val_loss: 8545.1748\n",
      "Epoch 2488/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9491.4218 - val_loss: 8544.7119\n",
      "Epoch 2489/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9490.9590 - val_loss: 8544.2480\n",
      "Epoch 2490/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9490.4953 - val_loss: 8543.7842\n",
      "Epoch 2491/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9490.0328 - val_loss: 8543.3223\n",
      "Epoch 2492/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9489.5689 - val_loss: 8542.8574\n",
      "Epoch 2493/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9489.1055 - val_loss: 8542.3955\n",
      "Epoch 2494/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9488.6434 - val_loss: 8541.9326\n",
      "Epoch 2495/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9488.1791 - val_loss: 8541.4697\n",
      "Epoch 2496/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9487.7166 - val_loss: 8541.0059\n",
      "Epoch 2497/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9487.2542 - val_loss: 8540.5430\n",
      "Epoch 2498/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9486.7904 - val_loss: 8540.0811\n",
      "Epoch 2499/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9486.3272 - val_loss: 8539.6172\n",
      "Epoch 2500/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9485.8647 - val_loss: 8539.1533\n",
      "Epoch 2501/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9485.4007 - val_loss: 8538.6904\n",
      "Epoch 2502/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9484.9379 - val_loss: 8538.2285\n",
      "Epoch 2503/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9484.4748 - val_loss: 8537.7637\n",
      "Epoch 2504/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9484.0109 - val_loss: 8537.3008\n",
      "Epoch 2505/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9483.5493 - val_loss: 8536.8389\n",
      "Epoch 2506/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9483.0851 - val_loss: 8536.3730\n",
      "Epoch 2507/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9482.6214 - val_loss: 8535.9102\n",
      "Epoch 2508/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9482.1596 - val_loss: 8535.4482\n",
      "Epoch 2509/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9481.6954 - val_loss: 8534.9863\n",
      "Epoch 2510/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9481.2330 - val_loss: 8534.5215\n",
      "Epoch 2511/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9480.7698 - val_loss: 8534.0586\n",
      "Epoch 2512/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9480.3060 - val_loss: 8533.5957\n",
      "Epoch 2513/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9479.8433 - val_loss: 8533.1328\n",
      "Epoch 2514/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9479.3809 - val_loss: 8532.6689\n",
      "Epoch 2515/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9478.9172 - val_loss: 8532.2061\n",
      "Epoch 2516/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9478.4545 - val_loss: 8531.7441\n",
      "Epoch 2517/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9477.9905 - val_loss: 8531.2793\n",
      "Epoch 2518/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9477.5270 - val_loss: 8530.8164\n",
      "Epoch 2519/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9477.0652 - val_loss: 8530.3545\n",
      "Epoch 2520/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9476.6008 - val_loss: 8529.8916\n",
      "Epoch 2521/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9476.1381 - val_loss: 8529.4258\n",
      "Epoch 2522/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9475.6760 - val_loss: 8528.9639\n",
      "Epoch 2523/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9475.2112 - val_loss: 8528.5020\n",
      "Epoch 2524/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9474.7487 - val_loss: 8528.0391\n",
      "Epoch 2525/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9474.2865 - val_loss: 8527.5742\n",
      "Epoch 2526/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9473.8218 - val_loss: 8527.1123\n",
      "Epoch 2527/10000\n",
      "750/750 [==============================] - 0s 99us/step - loss: 9473.3596 - val_loss: 8526.6494\n",
      "Epoch 2528/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 9472.8966 - val_loss: 8526.1855\n",
      "Epoch 2529/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9472.4330 - val_loss: 8525.7217\n",
      "Epoch 2530/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9471.9708 - val_loss: 8525.2598\n",
      "Epoch 2531/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9471.5068 - val_loss: 8524.7949\n",
      "Epoch 2532/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9471.0431 - val_loss: 8524.3330\n",
      "Epoch 2533/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9470.5809 - val_loss: 8523.8701\n",
      "Epoch 2534/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9470.1167 - val_loss: 8523.4072\n",
      "Epoch 2535/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9469.6540 - val_loss: 8522.9434\n",
      "Epoch 2536/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9469.1914 - val_loss: 8522.4795\n",
      "Epoch 2537/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9468.7271 - val_loss: 8522.0186\n",
      "Epoch 2538/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9468.2644 - val_loss: 8521.5547\n",
      "Epoch 2539/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9467.8024 - val_loss: 8521.0908\n",
      "Epoch 2540/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9467.3376 - val_loss: 8520.6279\n",
      "Epoch 2541/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9466.8749 - val_loss: 8520.1660\n",
      "Epoch 2542/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9466.4120 - val_loss: 8519.7012\n",
      "Epoch 2543/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9465.9479 - val_loss: 8519.2383\n",
      "Epoch 2544/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9465.4869 - val_loss: 8518.7764\n",
      "Epoch 2545/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9465.0224 - val_loss: 8518.3105\n",
      "Epoch 2546/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9464.5589 - val_loss: 8517.8477\n",
      "Epoch 2547/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9464.0971 - val_loss: 8517.3857\n",
      "Epoch 2548/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9463.6323 - val_loss: 8516.9238\n",
      "Epoch 2549/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9463.1698 - val_loss: 8516.4590\n",
      "Epoch 2550/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9462.7073 - val_loss: 8515.9961\n",
      "Epoch 2551/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9462.2428 - val_loss: 8515.5332\n",
      "Epoch 2552/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9461.7803 - val_loss: 8515.0703\n",
      "Epoch 2553/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9461.3181 - val_loss: 8514.6064\n",
      "Epoch 2554/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9460.8539 - val_loss: 8514.1436\n",
      "Epoch 2555/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9460.3919 - val_loss: 8513.6816\n",
      "Epoch 2556/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9459.9280 - val_loss: 8513.2168\n",
      "Epoch 2557/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9459.4641 - val_loss: 8512.7539\n",
      "Epoch 2558/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9459.0025 - val_loss: 8512.2920\n",
      "Epoch 2559/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9458.5383 - val_loss: 8511.8281\n",
      "Epoch 2560/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9458.0753 - val_loss: 8511.3633\n",
      "Epoch 2561/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9457.6128 - val_loss: 8510.9014\n",
      "Epoch 2562/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9457.1488 - val_loss: 8510.4395\n",
      "Epoch 2563/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9456.6862 - val_loss: 8509.9766\n",
      "Epoch 2564/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9456.2240 - val_loss: 8509.5117\n",
      "Epoch 2565/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9455.7594 - val_loss: 8509.0498\n",
      "Epoch 2566/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9455.2963 - val_loss: 8508.5869\n",
      "Epoch 2567/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9454.8340 - val_loss: 8508.1230\n",
      "Epoch 2568/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9454.3702 - val_loss: 8507.6592\n",
      "Epoch 2569/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9453.9076 - val_loss: 8507.1973\n",
      "Epoch 2570/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9453.4441 - val_loss: 8506.7324\n",
      "Epoch 2571/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9452.9804 - val_loss: 8506.2705\n",
      "Epoch 2572/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9452.5180 - val_loss: 8505.8076\n",
      "Epoch 2573/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9452.0541 - val_loss: 8505.3447\n",
      "Epoch 2574/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9451.5915 - val_loss: 8504.8799\n",
      "Epoch 2575/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9451.1291 - val_loss: 8504.4180\n",
      "Epoch 2576/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9450.6651 - val_loss: 8503.9561\n",
      "Epoch 2577/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9450.2017 - val_loss: 8503.4922\n",
      "Epoch 2578/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9449.7397 - val_loss: 8503.0273\n",
      "Epoch 2579/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9449.2756 - val_loss: 8502.5654\n",
      "Epoch 2580/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9448.8125 - val_loss: 8502.1035\n",
      "Epoch 2581/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9448.3496 - val_loss: 8501.6387\n",
      "Epoch 2582/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 77us/step - loss: 9447.8859 - val_loss: 8501.1758\n",
      "Epoch 2583/10000\n",
      "750/750 [==============================] - 0s 74us/step - loss: 9447.4242 - val_loss: 8500.7129\n",
      "Epoch 2584/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9446.9598 - val_loss: 8500.2480\n",
      "Epoch 2585/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9446.4965 - val_loss: 8499.7852\n",
      "Epoch 2586/10000\n",
      "750/750 [==============================] - 0s 77us/step - loss: 9446.0346 - val_loss: 8499.3232\n",
      "Epoch 2587/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9445.5699 - val_loss: 8498.8604\n",
      "Epoch 2588/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9445.1079 - val_loss: 8498.3965\n",
      "Epoch 2589/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9444.6448 - val_loss: 8497.9336\n",
      "Epoch 2590/10000\n",
      "750/750 [==============================] - 0s 76us/step - loss: 9444.1801 - val_loss: 8497.4707\n",
      "Epoch 2591/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9443.7183 - val_loss: 8497.0078\n",
      "Epoch 2592/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9443.2555 - val_loss: 8496.5439\n",
      "Epoch 2593/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9442.7913 - val_loss: 8496.0811\n",
      "Epoch 2594/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9442.3293 - val_loss: 8495.6182\n",
      "Epoch 2595/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9441.8654 - val_loss: 8495.1543\n",
      "Epoch 2596/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9441.4016 - val_loss: 8494.6914\n",
      "Epoch 2597/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9440.9399 - val_loss: 8494.2295\n",
      "Epoch 2598/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9440.4758 - val_loss: 8493.7637\n",
      "Epoch 2599/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9440.0125 - val_loss: 8493.3008\n",
      "Epoch 2600/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9439.5503 - val_loss: 8492.8389\n",
      "Epoch 2601/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9439.0858 - val_loss: 8492.3770\n",
      "Epoch 2602/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9438.6237 - val_loss: 8491.9141\n",
      "Epoch 2603/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9438.1615 - val_loss: 8491.4492\n",
      "Epoch 2604/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9437.6968 - val_loss: 8490.9873\n",
      "Epoch 2605/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9437.2343 - val_loss: 8490.5244\n",
      "Epoch 2606/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9436.7712 - val_loss: 8490.0605\n",
      "Epoch 2607/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9436.3072 - val_loss: 8489.5967\n",
      "Epoch 2608/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9435.8458 - val_loss: 8489.1348\n",
      "Epoch 2609/10000\n",
      "750/750 [==============================] - 0s 79us/step - loss: 9435.3817 - val_loss: 8488.6699\n",
      "Epoch 2610/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9434.9184 - val_loss: 8488.2080\n",
      "Epoch 2611/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9434.4563 - val_loss: 8487.7451\n",
      "Epoch 2612/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9433.9926 - val_loss: 8487.2822\n",
      "Epoch 2613/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9433.5293 - val_loss: 8486.8203\n",
      "Epoch 2614/10000\n",
      "750/750 [==============================] - 0s 78us/step - loss: 9433.0676 - val_loss: 8486.3555\n",
      "Epoch 2615/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9432.6031 - val_loss: 8485.8936\n",
      "Epoch 2616/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9432.1408 - val_loss: 8485.4307\n",
      "Epoch 2617/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9431.6778 - val_loss: 8484.9668\n",
      "Epoch 2618/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9431.2141 - val_loss: 8484.5029\n",
      "Epoch 2619/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9430.7521 - val_loss: 8484.0410\n",
      "Epoch 2620/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9430.2883 - val_loss: 8483.5781\n",
      "Epoch 2621/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9429.8260 - val_loss: 8483.1152\n",
      "Epoch 2622/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9429.3635 - val_loss: 8482.6523\n",
      "Epoch 2623/10000\n",
      "750/750 [==============================] - 0s 159us/step - loss: 9428.8997 - val_loss: 8482.1895\n",
      "Epoch 2624/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9428.4370 - val_loss: 8481.7266\n",
      "Epoch 2625/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9427.9742 - val_loss: 8481.2617\n",
      "Epoch 2626/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9427.5109 - val_loss: 8480.8008\n",
      "Epoch 2627/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 9427.0487 - val_loss: 8480.3379\n",
      "Epoch 2628/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9426.5851 - val_loss: 8479.8730\n",
      "Epoch 2629/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9426.1218 - val_loss: 8479.4131\n",
      "Epoch 2630/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9425.6604 - val_loss: 8478.9482\n",
      "Epoch 2631/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9425.1960 - val_loss: 8478.4873\n",
      "Epoch 2632/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9424.7343 - val_loss: 8478.0244\n",
      "Epoch 2633/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9424.2716 - val_loss: 8477.5605\n",
      "Epoch 2634/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9423.8079 - val_loss: 8477.0967\n",
      "Epoch 2635/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9423.3463 - val_loss: 8476.6357\n",
      "Epoch 2636/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9422.8821 - val_loss: 8476.1729\n",
      "Epoch 2637/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9422.4196 - val_loss: 8475.7109\n",
      "Epoch 2638/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9421.9583 - val_loss: 8475.2461\n",
      "Epoch 2639/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9421.4937 - val_loss: 8474.7842\n",
      "Epoch 2640/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9421.0322 - val_loss: 8474.3223\n",
      "Epoch 2641/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9420.5689 - val_loss: 8473.8574\n",
      "Epoch 2642/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 9420.1059 - val_loss: 8473.3955\n",
      "Epoch 2643/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 9419.6439 - val_loss: 8472.9336\n",
      "Epoch 2644/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 9419.1802 - val_loss: 8472.4707\n",
      "Epoch 2645/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 9418.7175 - val_loss: 8472.0078\n",
      "Epoch 2646/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9418.2558 - val_loss: 8471.5449\n",
      "Epoch 2647/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9417.7923 - val_loss: 8471.0820\n",
      "Epoch 2648/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 9417.3306 - val_loss: 8470.6201\n",
      "Epoch 2649/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 9416.8666 - val_loss: 8470.1582\n",
      "Epoch 2650/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9416.4044 - val_loss: 8469.6953\n",
      "Epoch 2651/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9415.9428 - val_loss: 8469.2305\n",
      "Epoch 2652/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 9415.4789 - val_loss: 8468.7686\n",
      "Epoch 2653/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9415.0169 - val_loss: 8468.3066\n",
      "Epoch 2654/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 9414.5536 - val_loss: 8467.8418\n",
      "Epoch 2655/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9414.0907 - val_loss: 8467.3809\n",
      "Epoch 2656/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 9413.6291 - val_loss: 8466.9180\n",
      "Epoch 2657/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9413.1656 - val_loss: 8466.4561\n",
      "Epoch 2658/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9412.7030 - val_loss: 8465.9932\n",
      "Epoch 2659/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9412.2403 - val_loss: 8465.5293\n",
      "Epoch 2660/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9411.7767 - val_loss: 8465.0664\n",
      "Epoch 2661/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9411.3158 - val_loss: 8464.6045\n",
      "Epoch 2662/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9410.8512 - val_loss: 8464.1436\n",
      "Epoch 2663/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9410.3893 - val_loss: 8463.6797\n",
      "Epoch 2664/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9409.9273 - val_loss: 8463.2158\n",
      "Epoch 2665/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9409.4633 - val_loss: 8462.7529\n",
      "Epoch 2666/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 9409.0014 - val_loss: 8462.2920\n",
      "Epoch 2667/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 9408.5383 - val_loss: 8461.8281\n",
      "Epoch 2668/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 9408.0760 - val_loss: 8461.3652\n",
      "Epoch 2669/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 9407.6136 - val_loss: 8460.9023\n",
      "Epoch 2670/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9407.1499 - val_loss: 8460.4404\n",
      "Epoch 2671/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9406.6881 - val_loss: 8459.9785\n",
      "Epoch 2672/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9406.2247 - val_loss: 8459.5137\n",
      "Epoch 2673/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9405.7621 - val_loss: 8459.0508\n",
      "Epoch 2674/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9405.3002 - val_loss: 8458.5889\n",
      "Epoch 2675/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9404.8365 - val_loss: 8458.1270\n",
      "Epoch 2676/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9404.3737 - val_loss: 8457.6641\n",
      "Epoch 2677/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 9403.9118 - val_loss: 8457.2012\n",
      "Epoch 2678/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 9403.4486 - val_loss: 8456.7383\n",
      "Epoch 2679/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9402.9869 - val_loss: 8456.2764\n",
      "Epoch 2680/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9402.5227 - val_loss: 8455.8135\n",
      "Epoch 2681/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9402.0607 - val_loss: 8455.3496\n",
      "Epoch 2682/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9401.5981 - val_loss: 8454.8867\n",
      "Epoch 2683/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9401.1345 - val_loss: 8454.4248\n",
      "Epoch 2684/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9400.6731 - val_loss: 8453.9629\n",
      "Epoch 2685/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9400.2097 - val_loss: 8453.4980\n",
      "Epoch 2686/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9399.7466 - val_loss: 8453.0352\n",
      "Epoch 2687/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9399.2855 - val_loss: 8452.5732\n",
      "Epoch 2688/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9398.8215 - val_loss: 8452.1113\n",
      "Epoch 2689/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9398.3592 - val_loss: 8451.6494\n",
      "Epoch 2690/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9397.8965 - val_loss: 8451.1855\n",
      "Epoch 2691/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9397.4329 - val_loss: 8450.7217\n",
      "Epoch 2692/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9396.9712 - val_loss: 8450.2607\n",
      "Epoch 2693/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9396.5081 - val_loss: 8449.7988\n",
      "Epoch 2694/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9396.0456 - val_loss: 8449.3359\n",
      "Epoch 2695/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9395.5835 - val_loss: 8448.8721\n",
      "Epoch 2696/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9395.1195 - val_loss: 8448.4092\n",
      "Epoch 2697/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9394.6578 - val_loss: 8447.9473\n",
      "Epoch 2698/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9394.1944 - val_loss: 8447.4824\n",
      "Epoch 2699/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9393.7314 - val_loss: 8447.0215\n",
      "Epoch 2700/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9393.2698 - val_loss: 8446.5586\n",
      "Epoch 2701/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9392.8062 - val_loss: 8446.0967\n",
      "Epoch 2702/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9392.3447 - val_loss: 8445.6338\n",
      "Epoch 2703/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9391.8813 - val_loss: 8445.1699\n",
      "Epoch 2704/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9391.4184 - val_loss: 8444.7080\n",
      "Epoch 2705/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9390.9564 - val_loss: 8444.2451\n",
      "Epoch 2706/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 9390.4925 - val_loss: 8443.7832\n",
      "Epoch 2707/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9390.0309 - val_loss: 8443.3203\n",
      "Epoch 2708/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9389.5680 - val_loss: 8442.8574\n",
      "Epoch 2709/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9389.1048 - val_loss: 8442.3945\n",
      "Epoch 2710/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9388.6431 - val_loss: 8441.9326\n",
      "Epoch 2711/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9388.1790 - val_loss: 8441.4688\n",
      "Epoch 2712/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9387.7166 - val_loss: 8441.0078\n",
      "Epoch 2713/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9387.2553 - val_loss: 8440.5430\n",
      "Epoch 2714/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9386.7908 - val_loss: 8440.0811\n",
      "Epoch 2715/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9386.3291 - val_loss: 8439.6191\n",
      "Epoch 2716/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9385.8660 - val_loss: 8439.1543\n",
      "Epoch 2717/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9385.4027 - val_loss: 8438.6924\n",
      "Epoch 2718/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9384.9408 - val_loss: 8438.2305\n",
      "Epoch 2719/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9384.4781 - val_loss: 8437.7686\n",
      "Epoch 2720/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9384.0155 - val_loss: 8437.3047\n",
      "Epoch 2721/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9383.5528 - val_loss: 8436.8418\n",
      "Epoch 2722/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9383.0891 - val_loss: 8436.3789\n",
      "Epoch 2723/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9382.6276 - val_loss: 8435.9170\n",
      "Epoch 2724/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9382.1642 - val_loss: 8435.4561\n",
      "Epoch 2725/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9381.7019 - val_loss: 8434.9922\n",
      "Epoch 2726/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 100us/step - loss: 9381.2397 - val_loss: 8434.5283\n",
      "Epoch 2727/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9380.7757 - val_loss: 8434.0654\n",
      "Epoch 2728/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9380.3138 - val_loss: 8433.6035\n",
      "Epoch 2729/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9379.8505 - val_loss: 8433.1416\n",
      "Epoch 2730/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9379.3885 - val_loss: 8432.6777\n",
      "Epoch 2731/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 9378.9262 - val_loss: 8432.2148\n",
      "Epoch 2732/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 9378.4624 - val_loss: 8431.7529\n",
      "Epoch 2733/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9378.0002 - val_loss: 8431.2910\n",
      "Epoch 2734/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9377.5376 - val_loss: 8430.8262\n",
      "Epoch 2735/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9377.0746 - val_loss: 8430.3633\n",
      "Epoch 2736/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9376.6126 - val_loss: 8429.9014\n",
      "Epoch 2737/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9376.1488 - val_loss: 8429.4395\n",
      "Epoch 2738/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9375.6867 - val_loss: 8428.9766\n",
      "Epoch 2739/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9375.2245 - val_loss: 8428.5117\n",
      "Epoch 2740/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9374.7609 - val_loss: 8428.0508\n",
      "Epoch 2741/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9374.2993 - val_loss: 8427.5889\n",
      "Epoch 2742/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9373.8355 - val_loss: 8427.1260\n",
      "Epoch 2743/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9373.3728 - val_loss: 8426.6631\n",
      "Epoch 2744/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9372.9110 - val_loss: 8426.1992\n",
      "Epoch 2745/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9372.4476 - val_loss: 8425.7373\n",
      "Epoch 2746/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9371.9856 - val_loss: 8425.2754\n",
      "Epoch 2747/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9371.5222 - val_loss: 8424.8105\n",
      "Epoch 2748/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9371.0590 - val_loss: 8424.3477\n",
      "Epoch 2749/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9370.5972 - val_loss: 8423.8867\n",
      "Epoch 2750/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9370.1335 - val_loss: 8423.4248\n",
      "Epoch 2751/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9369.6714 - val_loss: 8422.9619\n",
      "Epoch 2752/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9369.2090 - val_loss: 8422.4980\n",
      "Epoch 2753/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9368.7454 - val_loss: 8422.0342\n",
      "Epoch 2754/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9368.2838 - val_loss: 8421.5732\n",
      "Epoch 2755/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9367.8202 - val_loss: 8421.1113\n",
      "Epoch 2756/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9367.3581 - val_loss: 8420.6484\n",
      "Epoch 2757/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 9366.8959 - val_loss: 8420.1846\n",
      "Epoch 2758/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 9366.4320 - val_loss: 8419.7217\n",
      "Epoch 2759/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 9365.9708 - val_loss: 8419.2598\n",
      "Epoch 2760/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 9365.5067 - val_loss: 8418.7979\n",
      "Epoch 2761/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 9365.0448 - val_loss: 8418.3340\n",
      "Epoch 2762/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 9364.5823 - val_loss: 8417.8711\n",
      "Epoch 2763/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 9364.1186 - val_loss: 8417.4092\n",
      "Epoch 2764/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 9363.6562 - val_loss: 8416.9473\n",
      "Epoch 2765/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 9363.1939 - val_loss: 8416.4824\n",
      "Epoch 2766/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 9362.7302 - val_loss: 8416.0205\n",
      "Epoch 2767/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9362.2689 - val_loss: 8415.5576\n",
      "Epoch 2768/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9361.8050 - val_loss: 8415.0957\n",
      "Epoch 2769/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9361.3426 - val_loss: 8414.6328\n",
      "Epoch 2770/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9360.8806 - val_loss: 8414.1699\n",
      "Epoch 2771/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9360.4173 - val_loss: 8413.7070\n",
      "Epoch 2772/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9359.9556 - val_loss: 8413.2451\n",
      "Epoch 2773/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 9359.4915 - val_loss: 8412.7812\n",
      "Epoch 2774/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9359.0291 - val_loss: 8412.3184\n",
      "Epoch 2775/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9358.5669 - val_loss: 8411.8555\n",
      "Epoch 2776/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9358.1038 - val_loss: 8411.3936\n",
      "Epoch 2777/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9357.6419 - val_loss: 8410.9316\n",
      "Epoch 2778/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9357.1784 - val_loss: 8410.4668\n",
      "Epoch 2779/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9356.7153 - val_loss: 8410.0049\n",
      "Epoch 2780/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9356.2534 - val_loss: 8409.5420\n",
      "Epoch 2781/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9355.7898 - val_loss: 8409.0811\n",
      "Epoch 2782/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9355.3280 - val_loss: 8408.6172\n",
      "Epoch 2783/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9354.8653 - val_loss: 8408.1543\n",
      "Epoch 2784/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 9354.4016 - val_loss: 8407.6914\n",
      "Epoch 2785/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 9353.9401 - val_loss: 8407.2295\n",
      "Epoch 2786/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9353.4759 - val_loss: 8406.7686\n",
      "Epoch 2787/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9353.0144 - val_loss: 8406.3047\n",
      "Epoch 2788/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9352.5522 - val_loss: 8405.8408\n",
      "Epoch 2789/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9352.0882 - val_loss: 8405.3779\n",
      "Epoch 2790/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9351.6271 - val_loss: 8404.9160\n",
      "Epoch 2791/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9351.1631 - val_loss: 8404.4531\n",
      "Epoch 2792/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9350.7010 - val_loss: 8403.9883\n",
      "Epoch 2793/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9350.2388 - val_loss: 8403.5273\n",
      "Epoch 2794/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9349.7749 - val_loss: 8403.0654\n",
      "Epoch 2795/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9349.3124 - val_loss: 8402.6035\n",
      "Epoch 2796/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9348.8502 - val_loss: 8402.1387\n",
      "Epoch 2797/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9348.3872 - val_loss: 8401.6758\n",
      "Epoch 2798/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9347.9252 - val_loss: 8401.2139\n",
      "Epoch 2799/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9347.4613 - val_loss: 8400.7520\n",
      "Epoch 2800/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9346.9987 - val_loss: 8400.2891\n",
      "Epoch 2801/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9346.5366 - val_loss: 8399.8242\n",
      "Epoch 2802/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9346.0735 - val_loss: 8399.3633\n",
      "Epoch 2803/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9345.6118 - val_loss: 8398.9014\n",
      "Epoch 2804/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9345.1478 - val_loss: 8398.4385\n",
      "Epoch 2805/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9344.6857 - val_loss: 8397.9766\n",
      "Epoch 2806/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9344.2239 - val_loss: 8397.5117\n",
      "Epoch 2807/10000\n",
      "750/750 [==============================] - 0s 103us/step - loss: 9343.7600 - val_loss: 8397.0498\n",
      "Epoch 2808/10000\n",
      "750/750 [==============================] - 0s 107us/step - loss: 9343.2982 - val_loss: 8396.5879\n",
      "Epoch 2809/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9342.8347 - val_loss: 8396.1230\n",
      "Epoch 2810/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9342.3715 - val_loss: 8395.6631\n",
      "Epoch 2811/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 9341.9096 - val_loss: 8395.1982\n",
      "Epoch 2812/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9341.4456 - val_loss: 8394.7373\n",
      "Epoch 2813/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9340.9843 - val_loss: 8394.2744\n",
      "Epoch 2814/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9340.5215 - val_loss: 8393.8105\n",
      "Epoch 2815/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9340.0580 - val_loss: 8393.3467\n",
      "Epoch 2816/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 9339.5965 - val_loss: 8392.8857\n",
      "Epoch 2817/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 9339.1325 - val_loss: 8392.4229\n",
      "Epoch 2818/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9338.6702 - val_loss: 8391.9609\n",
      "Epoch 2819/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9338.2085 - val_loss: 8391.4971\n",
      "Epoch 2820/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9337.7445 - val_loss: 8391.0342\n",
      "Epoch 2821/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9337.2826 - val_loss: 8390.5723\n",
      "Epoch 2822/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9336.8194 - val_loss: 8390.1104\n",
      "Epoch 2823/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9336.3569 - val_loss: 8389.6465\n",
      "Epoch 2824/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9335.8949 - val_loss: 8389.1836\n",
      "Epoch 2825/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9335.4312 - val_loss: 8388.7217\n",
      "Epoch 2826/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9334.9687 - val_loss: 8388.2578\n",
      "Epoch 2827/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9334.5059 - val_loss: 8387.7949\n",
      "Epoch 2828/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9334.0435 - val_loss: 8387.3330\n",
      "Epoch 2829/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9333.5814 - val_loss: 8386.8701\n",
      "Epoch 2830/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9333.1175 - val_loss: 8386.4082\n",
      "Epoch 2831/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9332.6550 - val_loss: 8385.9453\n",
      "Epoch 2832/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9332.1930 - val_loss: 8385.4814\n",
      "Epoch 2833/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9331.7291 - val_loss: 8385.0195\n",
      "Epoch 2834/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9331.2680 - val_loss: 8384.5576\n",
      "Epoch 2835/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9330.8040 - val_loss: 8384.0938\n",
      "Epoch 2836/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9330.3416 - val_loss: 8383.6309\n",
      "Epoch 2837/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9329.8794 - val_loss: 8383.1689\n",
      "Epoch 2838/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 9329.4156 - val_loss: 8382.7061\n",
      "Epoch 2839/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9328.9541 - val_loss: 8382.2441\n",
      "Epoch 2840/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9328.4909 - val_loss: 8381.7793\n",
      "Epoch 2841/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9328.0278 - val_loss: 8381.3174\n",
      "Epoch 2842/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9327.5665 - val_loss: 8380.8555\n",
      "Epoch 2843/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9327.1029 - val_loss: 8380.3936\n",
      "Epoch 2844/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9326.6405 - val_loss: 8379.9297\n",
      "Epoch 2845/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9326.1777 - val_loss: 8379.4668\n",
      "Epoch 2846/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9325.7141 - val_loss: 8379.0039\n",
      "Epoch 2847/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 9325.2526 - val_loss: 8378.5420\n",
      "Epoch 2848/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 9324.7891 - val_loss: 8378.0801\n",
      "Epoch 2849/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9324.3269 - val_loss: 8377.6172\n",
      "Epoch 2850/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9323.8647 - val_loss: 8377.1533\n",
      "Epoch 2851/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9323.4014 - val_loss: 8376.6904\n",
      "Epoch 2852/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 9322.9388 - val_loss: 8376.2285\n",
      "Epoch 2853/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9322.4754 - val_loss: 8375.7666\n",
      "Epoch 2854/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9322.0136 - val_loss: 8375.3027\n",
      "Epoch 2855/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9321.5511 - val_loss: 8374.8398\n",
      "Epoch 2856/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9321.0875 - val_loss: 8374.3779\n",
      "Epoch 2857/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9320.6257 - val_loss: 8373.9150\n",
      "Epoch 2858/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9320.1622 - val_loss: 8373.4512\n",
      "Epoch 2859/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9319.6997 - val_loss: 8372.9883\n",
      "Epoch 2860/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9319.2377 - val_loss: 8372.5264\n",
      "Epoch 2861/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9318.7738 - val_loss: 8372.0645\n",
      "Epoch 2862/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9318.3113 - val_loss: 8371.6025\n",
      "Epoch 2863/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9317.8493 - val_loss: 8371.1367\n",
      "Epoch 2864/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9317.3860 - val_loss: 8370.6758\n",
      "Epoch 2865/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9316.9244 - val_loss: 8370.2129\n",
      "Epoch 2866/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9316.4602 - val_loss: 8369.7510\n",
      "Epoch 2867/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9315.9979 - val_loss: 8369.2881\n",
      "Epoch 2868/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9315.5356 - val_loss: 8368.8242\n",
      "Epoch 2869/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9315.0719 - val_loss: 8368.3623\n",
      "Epoch 2870/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 88us/step - loss: 9314.6103 - val_loss: 8367.9004\n",
      "Epoch 2871/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9314.1474 - val_loss: 8367.4355\n",
      "Epoch 2872/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9313.6841 - val_loss: 8366.9727\n",
      "Epoch 2873/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9313.2224 - val_loss: 8366.5107\n",
      "Epoch 2874/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9312.7591 - val_loss: 8366.0498\n",
      "Epoch 2875/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9312.2967 - val_loss: 8365.5869\n",
      "Epoch 2876/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9311.8340 - val_loss: 8365.1230\n",
      "Epoch 2877/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9311.3708 - val_loss: 8364.6592\n",
      "Epoch 2878/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9310.9088 - val_loss: 8364.1982\n",
      "Epoch 2879/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9310.4448 - val_loss: 8363.7354\n",
      "Epoch 2880/10000\n",
      "750/750 [==============================] - 0s 99us/step - loss: 9309.9829 - val_loss: 8363.2734\n",
      "Epoch 2881/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 9309.5210 - val_loss: 8362.8096\n",
      "Epoch 2882/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9309.0570 - val_loss: 8362.3467\n",
      "Epoch 2883/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9308.5951 - val_loss: 8361.8848\n",
      "Epoch 2884/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9308.1318 - val_loss: 8361.4199\n",
      "Epoch 2885/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9307.6688 - val_loss: 8360.9590\n",
      "Epoch 2886/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9307.2073 - val_loss: 8360.4961\n",
      "Epoch 2887/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 9306.7436 - val_loss: 8360.0342\n",
      "Epoch 2888/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9306.2813 - val_loss: 8359.5713\n",
      "Epoch 2889/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9305.8186 - val_loss: 8359.1074\n",
      "Epoch 2890/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9305.3556 - val_loss: 8358.6455\n",
      "Epoch 2891/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9304.8939 - val_loss: 8358.1826\n",
      "Epoch 2892/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9304.4299 - val_loss: 8357.7207\n",
      "Epoch 2893/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9303.9675 - val_loss: 8357.2578\n",
      "Epoch 2894/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9303.5055 - val_loss: 8356.7939\n",
      "Epoch 2895/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9303.0421 - val_loss: 8356.3320\n",
      "Epoch 2896/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9302.5805 - val_loss: 8355.8691\n",
      "Epoch 2897/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9302.1165 - val_loss: 8355.4072\n",
      "Epoch 2898/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9301.6541 - val_loss: 8354.9434\n",
      "Epoch 2899/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9301.1919 - val_loss: 8354.4805\n",
      "Epoch 2900/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9300.7289 - val_loss: 8354.0186\n",
      "Epoch 2901/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9300.2666 - val_loss: 8353.5557\n",
      "Epoch 2902/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9299.8034 - val_loss: 8353.0918\n",
      "Epoch 2903/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9299.3405 - val_loss: 8352.6299\n",
      "Epoch 2904/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9298.8783 - val_loss: 8352.1670\n",
      "Epoch 2905/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9298.4147 - val_loss: 8351.7061\n",
      "Epoch 2906/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9297.9529 - val_loss: 8351.2422\n",
      "Epoch 2907/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 9297.4903 - val_loss: 8350.7793\n",
      "Epoch 2908/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9297.0269 - val_loss: 8350.3164\n",
      "Epoch 2909/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9296.5658 - val_loss: 8349.8545\n",
      "Epoch 2910/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9296.1011 - val_loss: 8349.3926\n",
      "Epoch 2911/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9295.6393 - val_loss: 8348.9297\n",
      "Epoch 2912/10000\n",
      "750/750 [==============================] - 0s 83us/step - loss: 9295.1773 - val_loss: 8348.4658\n",
      "Epoch 2913/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9294.7132 - val_loss: 8348.0029\n",
      "Epoch 2914/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9294.2514 - val_loss: 8347.5410\n",
      "Epoch 2915/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9293.7884 - val_loss: 8347.0762\n",
      "Epoch 2916/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9293.3254 - val_loss: 8346.6143\n",
      "Epoch 2917/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 9292.8636 - val_loss: 8346.1523\n",
      "Epoch 2918/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9292.3999 - val_loss: 8345.6904\n",
      "Epoch 2919/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9291.9375 - val_loss: 8345.2285\n",
      "Epoch 2920/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9291.4749 - val_loss: 8344.7637\n",
      "Epoch 2921/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 9291.0116 - val_loss: 8344.3008\n",
      "Epoch 2922/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9290.5501 - val_loss: 8343.8389\n",
      "Epoch 2923/10000\n",
      "750/750 [==============================] - 0s 179us/step - loss: 9290.0862 - val_loss: 8343.3770\n",
      "Epoch 2924/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 9289.6236 - val_loss: 8342.9141\n",
      "Epoch 2925/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 9289.1618 - val_loss: 8342.4492\n",
      "Epoch 2926/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9288.6985 - val_loss: 8341.9883\n",
      "Epoch 2927/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9288.2368 - val_loss: 8341.5264\n",
      "Epoch 2928/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 9287.7727 - val_loss: 8341.0635\n",
      "Epoch 2929/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9287.3104 - val_loss: 8340.5996\n",
      "Epoch 2930/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 9286.8481 - val_loss: 8340.1367\n",
      "Epoch 2931/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 9286.3843 - val_loss: 8339.6748\n",
      "Epoch 2932/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 9285.9231 - val_loss: 8339.2129\n",
      "Epoch 2933/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 9285.4597 - val_loss: 8338.7480\n",
      "Epoch 2934/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 9284.9965 - val_loss: 8338.2852\n",
      "Epoch 2935/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 9284.5354 - val_loss: 8337.8232\n",
      "Epoch 2936/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 9284.0709 - val_loss: 8337.3613\n",
      "Epoch 2937/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 9283.6089 - val_loss: 8336.8994\n",
      "Epoch 2938/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 9283.1465 - val_loss: 8336.4355\n",
      "Epoch 2939/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 9282.6830 - val_loss: 8335.9717\n",
      "Epoch 2940/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 9282.2212 - val_loss: 8335.5107\n",
      "Epoch 2941/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 9281.7579 - val_loss: 8335.0479\n",
      "Epoch 2942/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 9281.2950 - val_loss: 8334.5859\n",
      "Epoch 2943/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 9280.8335 - val_loss: 8334.1221\n",
      "Epoch 2944/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9280.3695 - val_loss: 8333.6592\n",
      "Epoch 2945/10000\n",
      "750/750 [==============================] - 0s 99us/step - loss: 9279.9073 - val_loss: 8333.1973\n",
      "Epoch 2946/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9279.4442 - val_loss: 8332.7354\n",
      "Epoch 2947/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9278.9822 - val_loss: 8332.2705\n",
      "Epoch 2948/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9278.5198 - val_loss: 8331.8086\n",
      "Epoch 2949/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9278.0561 - val_loss: 8331.3467\n",
      "Epoch 2950/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9277.5937 - val_loss: 8330.8828\n",
      "Epoch 2951/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9277.1309 - val_loss: 8330.4199\n",
      "Epoch 2952/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9276.6683 - val_loss: 8329.9570\n",
      "Epoch 2953/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 9276.2064 - val_loss: 8329.4951\n",
      "Epoch 2954/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 9282.15 - 0s 110us/step - loss: 9275.7425 - val_loss: 8329.0332\n",
      "Epoch 2955/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9275.2805 - val_loss: 8328.5703\n",
      "Epoch 2956/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9274.8179 - val_loss: 8328.1064\n",
      "Epoch 2957/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9274.3540 - val_loss: 8327.6436\n",
      "Epoch 2958/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9273.8927 - val_loss: 8327.1816\n",
      "Epoch 2959/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9273.4290 - val_loss: 8326.7188\n",
      "Epoch 2960/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9272.9668 - val_loss: 8326.2578\n",
      "Epoch 2961/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9272.5053 - val_loss: 8325.7930\n",
      "Epoch 2962/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9272.0406 - val_loss: 8325.3311\n",
      "Epoch 2963/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 9271.5794 - val_loss: 8324.8691\n",
      "Epoch 2964/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 9271.1159 - val_loss: 8324.4043\n",
      "Epoch 2965/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9270.6527 - val_loss: 8323.9424\n",
      "Epoch 2966/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9270.1907 - val_loss: 8323.4805\n",
      "Epoch 2967/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9269.7272 - val_loss: 8323.0186\n",
      "Epoch 2968/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9269.2655 - val_loss: 8322.5547\n",
      "Epoch 2969/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9268.8027 - val_loss: 8322.0918\n",
      "Epoch 2970/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9268.3393 - val_loss: 8321.6289\n",
      "Epoch 2971/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9267.8774 - val_loss: 8321.1670\n",
      "Epoch 2972/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9267.4133 - val_loss: 8320.7061\n",
      "Epoch 2973/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9266.9515 - val_loss: 8320.2422\n",
      "Epoch 2974/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9266.4897 - val_loss: 8319.7783\n",
      "Epoch 2975/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9266.0258 - val_loss: 8319.3154\n",
      "Epoch 2976/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9265.5638 - val_loss: 8318.8535\n",
      "Epoch 2977/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9265.1004 - val_loss: 8318.3916\n",
      "Epoch 2978/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9264.6382 - val_loss: 8317.9268\n",
      "Epoch 2979/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9264.1755 - val_loss: 8317.4648\n",
      "Epoch 2980/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 9263.7124 - val_loss: 8317.0029\n",
      "Epoch 2981/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9263.2502 - val_loss: 8316.5400\n",
      "Epoch 2982/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9262.7873 - val_loss: 8316.0762\n",
      "Epoch 2983/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9262.3239 - val_loss: 8315.6133\n",
      "Epoch 2984/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9261.8625 - val_loss: 8315.1514\n",
      "Epoch 2985/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9261.3988 - val_loss: 8314.6895\n",
      "Epoch 2986/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9260.9367 - val_loss: 8314.2266\n",
      "Epoch 2987/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9260.4744 - val_loss: 8313.7617\n",
      "Epoch 2988/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9260.0102 - val_loss: 8313.3008\n",
      "Epoch 2989/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9259.5493 - val_loss: 8312.8379\n",
      "Epoch 2990/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9259.0853 - val_loss: 8312.3760\n",
      "Epoch 2991/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9258.6226 - val_loss: 8311.9131\n",
      "Epoch 2992/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9258.1606 - val_loss: 8311.4492\n",
      "Epoch 2993/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9257.6970 - val_loss: 8310.9873\n",
      "Epoch 2994/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9257.2350 - val_loss: 8310.5254\n",
      "Epoch 2995/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9256.7721 - val_loss: 8310.0605\n",
      "Epoch 2996/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9256.3091 - val_loss: 8309.5977\n",
      "Epoch 2997/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9255.8472 - val_loss: 8309.1357\n",
      "Epoch 2998/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9255.3840 - val_loss: 8308.6738\n",
      "Epoch 2999/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9254.9217 - val_loss: 8308.2119\n",
      "Epoch 3000/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9254.4590 - val_loss: 8307.7480\n",
      "Epoch 3001/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9253.9956 - val_loss: 8307.2842\n",
      "Epoch 3002/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9253.5337 - val_loss: 8306.8232\n",
      "Epoch 3003/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9253.0697 - val_loss: 8306.3604\n",
      "Epoch 3004/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9252.6080 - val_loss: 8305.8984\n",
      "Epoch 3005/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9252.1460 - val_loss: 8305.4346\n",
      "Epoch 3006/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9251.6820 - val_loss: 8304.9717\n",
      "Epoch 3007/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9251.2201 - val_loss: 8304.5098\n",
      "Epoch 3008/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9250.7566 - val_loss: 8304.0449\n",
      "Epoch 3009/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9250.2938 - val_loss: 8303.5840\n",
      "Epoch 3010/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9249.8323 - val_loss: 8303.1211\n",
      "Epoch 3011/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9249.3686 - val_loss: 8302.6592\n",
      "Epoch 3012/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9248.9061 - val_loss: 8302.1973\n",
      "Epoch 3013/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9248.4436 - val_loss: 8301.7324\n",
      "Epoch 3014/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 96us/step - loss: 9247.9800 - val_loss: 8301.2695\n",
      "Epoch 3015/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9247.5189 - val_loss: 8300.8076\n",
      "Epoch 3016/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9247.0550 - val_loss: 8300.3457\n",
      "Epoch 3017/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9246.5926 - val_loss: 8299.8828\n",
      "Epoch 3018/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9246.1304 - val_loss: 8299.4189\n",
      "Epoch 3019/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9245.6667 - val_loss: 8298.9570\n",
      "Epoch 3020/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9245.2055 - val_loss: 8298.4941\n",
      "Epoch 3021/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9244.7415 - val_loss: 8298.0312\n",
      "Epoch 3022/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9244.2790 - val_loss: 8297.5684\n",
      "Epoch 3023/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9243.8167 - val_loss: 8297.1055\n",
      "Epoch 3024/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9243.3533 - val_loss: 8296.6436\n",
      "Epoch 3025/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9242.8914 - val_loss: 8296.1807\n",
      "Epoch 3026/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9242.4285 - val_loss: 8295.7168\n",
      "Epoch 3027/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9241.9653 - val_loss: 8295.2549\n",
      "Epoch 3028/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9241.5033 - val_loss: 8294.7920\n",
      "Epoch 3029/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9241.0402 - val_loss: 8294.3311\n",
      "Epoch 3030/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9240.5778 - val_loss: 8293.8672\n",
      "Epoch 3031/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9240.1153 - val_loss: 8293.4043\n",
      "Epoch 3032/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 9239.6518 - val_loss: 8292.9414\n",
      "Epoch 3033/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9239.1899 - val_loss: 8292.4795\n",
      "Epoch 3034/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9238.7260 - val_loss: 8292.0176\n",
      "Epoch 3035/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9238.2641 - val_loss: 8291.5547\n",
      "Epoch 3036/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9237.8023 - val_loss: 8291.0908\n",
      "Epoch 3037/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9237.3382 - val_loss: 8290.6279\n",
      "Epoch 3038/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9236.8763 - val_loss: 8290.1660\n",
      "Epoch 3039/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9236.4131 - val_loss: 8289.7012\n",
      "Epoch 3040/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9235.9500 - val_loss: 8289.2393\n",
      "Epoch 3041/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9235.4886 - val_loss: 8288.7773\n",
      "Epoch 3042/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9235.0249 - val_loss: 8288.3145\n",
      "Epoch 3043/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9234.5621 - val_loss: 8287.8525\n",
      "Epoch 3044/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9234.0999 - val_loss: 8287.3887\n",
      "Epoch 3045/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9233.6362 - val_loss: 8286.9258\n",
      "Epoch 3046/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9233.1752 - val_loss: 8286.4639\n",
      "Epoch 3047/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9232.7112 - val_loss: 8286.0020\n",
      "Epoch 3048/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9232.2487 - val_loss: 8285.5391\n",
      "Epoch 3049/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9231.7867 - val_loss: 8285.0742\n",
      "Epoch 3050/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9231.3234 - val_loss: 8284.6133\n",
      "Epoch 3051/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9230.8618 - val_loss: 8284.1504\n",
      "Epoch 3052/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9230.3977 - val_loss: 8283.6885\n",
      "Epoch 3053/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9229.9354 - val_loss: 8283.2246\n",
      "Epoch 3054/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9229.4729 - val_loss: 8282.7617\n",
      "Epoch 3055/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9229.0094 - val_loss: 8282.2998\n",
      "Epoch 3056/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9228.5480 - val_loss: 8281.8369\n",
      "Epoch 3057/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9228.0846 - val_loss: 8281.3730\n",
      "Epoch 3058/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9227.6217 - val_loss: 8280.9102\n",
      "Epoch 3059/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9227.1598 - val_loss: 8280.4482\n",
      "Epoch 3060/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9226.6956 - val_loss: 8279.9863\n",
      "Epoch 3061/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9226.2339 - val_loss: 8279.5244\n",
      "Epoch 3062/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9225.7715 - val_loss: 8279.0605\n",
      "Epoch 3063/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9225.3079 - val_loss: 8278.5967\n",
      "Epoch 3064/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9224.8463 - val_loss: 8278.1357\n",
      "Epoch 3065/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9224.3825 - val_loss: 8277.6729\n",
      "Epoch 3066/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9223.9201 - val_loss: 8277.2109\n",
      "Epoch 3067/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9223.4585 - val_loss: 8276.7471\n",
      "Epoch 3068/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9222.9947 - val_loss: 8276.2842\n",
      "Epoch 3069/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9222.5324 - val_loss: 8275.8223\n",
      "Epoch 3070/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9222.0692 - val_loss: 8275.3574\n",
      "Epoch 3071/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9221.6063 - val_loss: 8274.8955\n",
      "Epoch 3072/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9221.1447 - val_loss: 8274.4336\n",
      "Epoch 3073/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9220.6812 - val_loss: 8273.9707\n",
      "Epoch 3074/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9220.2187 - val_loss: 8273.5078\n",
      "Epoch 3075/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9219.7559 - val_loss: 8273.0449\n",
      "Epoch 3076/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9219.2927 - val_loss: 8272.5820\n",
      "Epoch 3077/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9218.8314 - val_loss: 8272.1201\n",
      "Epoch 3078/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9218.3676 - val_loss: 8271.6582\n",
      "Epoch 3079/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9217.9052 - val_loss: 8271.1953\n",
      "Epoch 3080/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9217.4429 - val_loss: 8270.7314\n",
      "Epoch 3081/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9216.9797 - val_loss: 8270.2695\n",
      "Epoch 3082/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9216.5181 - val_loss: 8269.8066\n",
      "Epoch 3083/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9216.0540 - val_loss: 8269.3438\n",
      "Epoch 3084/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9215.5919 - val_loss: 8268.8809\n",
      "Epoch 3085/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9215.1295 - val_loss: 8268.4180\n",
      "Epoch 3086/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9214.6656 - val_loss: 8267.9561\n",
      "Epoch 3087/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9214.2041 - val_loss: 8267.4941\n",
      "Epoch 3088/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9213.7408 - val_loss: 8267.0293\n",
      "Epoch 3089/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9213.2780 - val_loss: 8266.5674\n",
      "Epoch 3090/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9212.8159 - val_loss: 8266.1045\n",
      "Epoch 3091/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9212.3519 - val_loss: 8265.6436\n",
      "Epoch 3092/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9211.8900 - val_loss: 8265.1797\n",
      "Epoch 3093/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9211.4277 - val_loss: 8264.7168\n",
      "Epoch 3094/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9210.9641 - val_loss: 8264.2539\n",
      "Epoch 3095/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9210.5025 - val_loss: 8263.7920\n",
      "Epoch 3096/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9210.0390 - val_loss: 8263.3311\n",
      "Epoch 3097/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9209.5760 - val_loss: 8262.8672\n",
      "Epoch 3098/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9209.1148 - val_loss: 8262.4033\n",
      "Epoch 3099/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9208.6508 - val_loss: 8261.9404\n",
      "Epoch 3100/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9208.1888 - val_loss: 8261.4785\n",
      "Epoch 3101/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9207.7253 - val_loss: 8261.0146\n",
      "Epoch 3102/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9207.2635 - val_loss: 8260.5508\n",
      "Epoch 3103/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9206.8011 - val_loss: 8260.0898\n",
      "Epoch 3104/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9206.3374 - val_loss: 8259.6270\n",
      "Epoch 3105/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9205.8750 - val_loss: 8259.1641\n",
      "Epoch 3106/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9205.4121 - val_loss: 8258.7012\n",
      "Epoch 3107/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9204.9490 - val_loss: 8258.2383\n",
      "Epoch 3108/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9204.4876 - val_loss: 8257.7764\n",
      "Epoch 3109/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9204.0238 - val_loss: 8257.3145\n",
      "Epoch 3110/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9203.5613 - val_loss: 8256.8516\n",
      "Epoch 3111/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9203.0993 - val_loss: 8256.3867\n",
      "Epoch 3112/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9202.6352 - val_loss: 8255.9248\n",
      "Epoch 3113/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9202.1739 - val_loss: 8255.4629\n",
      "Epoch 3114/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9201.7102 - val_loss: 8255.0000\n",
      "Epoch 3115/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9201.2478 - val_loss: 8254.5381\n",
      "Epoch 3116/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9200.7857 - val_loss: 8254.0742\n",
      "Epoch 3117/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9200.3225 - val_loss: 8253.6123\n",
      "Epoch 3118/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9199.8605 - val_loss: 8253.1494\n",
      "Epoch 3119/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9199.3971 - val_loss: 8252.6855\n",
      "Epoch 3120/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9198.9342 - val_loss: 8252.2227\n",
      "Epoch 3121/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9198.4721 - val_loss: 8251.7607\n",
      "Epoch 3122/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 9198.0091 - val_loss: 8251.2988\n",
      "Epoch 3123/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9197.5463 - val_loss: 8250.8359\n",
      "Epoch 3124/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9197.0840 - val_loss: 8250.3730\n",
      "Epoch 3125/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9196.6204 - val_loss: 8249.9092\n",
      "Epoch 3126/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9196.1588 - val_loss: 8249.4482\n",
      "Epoch 3127/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9195.6945 - val_loss: 8248.9854\n",
      "Epoch 3128/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9195.2331 - val_loss: 8248.5234\n",
      "Epoch 3129/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9194.7710 - val_loss: 8248.0586\n",
      "Epoch 3130/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9194.3071 - val_loss: 8247.5967\n",
      "Epoch 3131/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9193.8448 - val_loss: 8247.1348\n",
      "Epoch 3132/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9193.3814 - val_loss: 8246.6699\n",
      "Epoch 3133/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9192.9191 - val_loss: 8246.2090\n",
      "Epoch 3134/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9192.4573 - val_loss: 8245.7461\n",
      "Epoch 3135/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9191.9936 - val_loss: 8245.2842\n",
      "Epoch 3136/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9191.5308 - val_loss: 8244.8203\n",
      "Epoch 3137/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9191.0685 - val_loss: 8244.3574\n",
      "Epoch 3138/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9190.6052 - val_loss: 8243.8945\n",
      "Epoch 3139/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9190.1440 - val_loss: 8243.4326\n",
      "Epoch 3140/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9189.6800 - val_loss: 8242.9707\n",
      "Epoch 3141/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9189.2175 - val_loss: 8242.5078\n",
      "Epoch 3142/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9188.7554 - val_loss: 8242.0449\n",
      "Epoch 3143/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9188.2916 - val_loss: 8241.5811\n",
      "Epoch 3144/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9187.8300 - val_loss: 8241.1191\n",
      "Epoch 3145/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9187.3665 - val_loss: 8240.6562\n",
      "Epoch 3146/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9186.9040 - val_loss: 8240.1934\n",
      "Epoch 3147/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9186.4417 - val_loss: 8239.7305\n",
      "Epoch 3148/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9185.9787 - val_loss: 8239.2686\n",
      "Epoch 3149/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9185.5163 - val_loss: 8238.8066\n",
      "Epoch 3150/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9185.0534 - val_loss: 8238.3418\n",
      "Epoch 3151/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9184.5906 - val_loss: 8237.8799\n",
      "Epoch 3152/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9184.1282 - val_loss: 8237.4170\n",
      "Epoch 3153/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9183.6644 - val_loss: 8236.9561\n",
      "Epoch 3154/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9183.2028 - val_loss: 8236.4922\n",
      "Epoch 3155/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9182.7402 - val_loss: 8236.0293\n",
      "Epoch 3156/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9182.2768 - val_loss: 8235.5664\n",
      "Epoch 3157/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9181.8149 - val_loss: 8235.1045\n",
      "Epoch 3158/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 91us/step - loss: 9181.3510 - val_loss: 8234.6396\n",
      "Epoch 3159/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9180.8888 - val_loss: 8234.1797\n",
      "Epoch 3160/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9180.4273 - val_loss: 8233.7148\n",
      "Epoch 3161/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9179.9633 - val_loss: 8233.2529\n",
      "Epoch 3162/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9179.5011 - val_loss: 8232.7910\n",
      "Epoch 3163/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9179.0379 - val_loss: 8232.3262\n",
      "Epoch 3164/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9178.5748 - val_loss: 8231.8633\n",
      "Epoch 3165/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9178.1137 - val_loss: 8231.4023\n",
      "Epoch 3166/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9177.6499 - val_loss: 8230.9395\n",
      "Epoch 3167/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9177.1874 - val_loss: 8230.4766\n",
      "Epoch 3168/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 9176.7246 - val_loss: 8230.0137\n",
      "Epoch 3169/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9176.2620 - val_loss: 8229.5508\n",
      "Epoch 3170/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9175.7994 - val_loss: 8229.0889\n",
      "Epoch 3171/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9175.3363 - val_loss: 8228.6270\n",
      "Epoch 3172/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9174.8738 - val_loss: 8228.1641\n",
      "Epoch 3173/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9174.4116 - val_loss: 8227.6992\n",
      "Epoch 3174/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9173.9477 - val_loss: 8227.2373\n",
      "Epoch 3175/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9173.4863 - val_loss: 8226.7754\n",
      "Epoch 3176/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9173.0228 - val_loss: 8226.3125\n",
      "Epoch 3177/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9172.5604 - val_loss: 8225.8496\n",
      "Epoch 3178/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9172.0980 - val_loss: 8225.3867\n",
      "Epoch 3179/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9171.6343 - val_loss: 8224.9248\n",
      "Epoch 3180/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9171.1729 - val_loss: 8224.4619\n",
      "Epoch 3181/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9170.7096 - val_loss: 8223.9980\n",
      "Epoch 3182/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9170.2464 - val_loss: 8223.5352\n",
      "Epoch 3183/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9169.7846 - val_loss: 8223.0732\n",
      "Epoch 3184/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9169.3207 - val_loss: 8222.6113\n",
      "Epoch 3185/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9168.8591 - val_loss: 8222.1484\n",
      "Epoch 3186/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9168.3963 - val_loss: 8221.6855\n",
      "Epoch 3187/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9167.9330 - val_loss: 8221.2217\n",
      "Epoch 3188/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9167.4711 - val_loss: 8220.7607\n",
      "Epoch 3189/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9167.0072 - val_loss: 8220.2969\n",
      "Epoch 3190/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9166.5453 - val_loss: 8219.8359\n",
      "Epoch 3191/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9166.0834 - val_loss: 8219.3721\n",
      "Epoch 3192/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9165.6195 - val_loss: 8218.9092\n",
      "Epoch 3193/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9165.1575 - val_loss: 8218.4473\n",
      "Epoch 3194/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9164.6940 - val_loss: 8217.9834\n",
      "Epoch 3195/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9164.2311 - val_loss: 8217.5215\n",
      "Epoch 3196/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9163.7698 - val_loss: 8217.0586\n",
      "Epoch 3197/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9163.3062 - val_loss: 8216.5957\n",
      "Epoch 3198/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9162.8436 - val_loss: 8216.1328\n",
      "Epoch 3199/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9162.3809 - val_loss: 8215.6699\n",
      "Epoch 3200/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9161.9185 - val_loss: 8215.2080\n",
      "Epoch 3201/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9161.4558 - val_loss: 8214.7451\n",
      "Epoch 3202/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9160.9925 - val_loss: 8214.2832\n",
      "Epoch 3203/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9160.5301 - val_loss: 8213.8203\n",
      "Epoch 3204/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9160.0678 - val_loss: 8213.3564\n",
      "Epoch 3205/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9159.6041 - val_loss: 8212.8945\n",
      "Epoch 3206/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9159.1428 - val_loss: 8212.4316\n",
      "Epoch 3207/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9158.6789 - val_loss: 8211.9688\n",
      "Epoch 3208/10000\n",
      "750/750 [==============================] - 0s 103us/step - loss: 9158.2167 - val_loss: 8211.5059\n",
      "Epoch 3209/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9157.7542 - val_loss: 8211.0430\n",
      "Epoch 3210/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9157.2906 - val_loss: 8210.5811\n",
      "Epoch 3211/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9156.8292 - val_loss: 8210.1182\n",
      "Epoch 3212/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9156.3659 - val_loss: 8209.6543\n",
      "Epoch 3213/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9155.9028 - val_loss: 8209.1924\n",
      "Epoch 3214/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9155.4407 - val_loss: 8208.7295\n",
      "Epoch 3215/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9154.9768 - val_loss: 8208.2686\n",
      "Epoch 3216/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9154.5148 - val_loss: 8207.8047\n",
      "Epoch 3217/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9154.0528 - val_loss: 8207.3418\n",
      "Epoch 3218/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9153.5893 - val_loss: 8206.8789\n",
      "Epoch 3219/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9153.1273 - val_loss: 8206.4170\n",
      "Epoch 3220/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9152.6639 - val_loss: 8205.9551\n",
      "Epoch 3221/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9152.2012 - val_loss: 8205.4902\n",
      "Epoch 3222/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9151.7391 - val_loss: 8205.0273\n",
      "Epoch 3223/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9151.2758 - val_loss: 8204.5654\n",
      "Epoch 3224/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 9150.8139 - val_loss: 8204.1035\n",
      "Epoch 3225/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9150.3502 - val_loss: 8203.6387\n",
      "Epoch 3226/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9149.8880 - val_loss: 8203.1758\n",
      "Epoch 3227/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9149.4262 - val_loss: 8202.7148\n",
      "Epoch 3228/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9148.9625 - val_loss: 8202.2520\n",
      "Epoch 3229/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9148.4999 - val_loss: 8201.7891\n",
      "Epoch 3230/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9148.0371 - val_loss: 8201.3262\n",
      "Epoch 3231/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9147.5741 - val_loss: 8200.8633\n",
      "Epoch 3232/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9147.1125 - val_loss: 8200.4014\n",
      "Epoch 3233/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9146.6487 - val_loss: 8199.9395\n",
      "Epoch 3234/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9146.1862 - val_loss: 8199.4766\n",
      "Epoch 3235/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9145.7241 - val_loss: 8199.0117\n",
      "Epoch 3236/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9145.2603 - val_loss: 8198.5508\n",
      "Epoch 3237/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9144.7989 - val_loss: 8198.0879\n",
      "Epoch 3238/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9144.3352 - val_loss: 8197.6250\n",
      "Epoch 3239/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9143.8721 - val_loss: 8197.1631\n",
      "Epoch 3240/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9143.4107 - val_loss: 8196.6992\n",
      "Epoch 3241/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9142.9468 - val_loss: 8196.2373\n",
      "Epoch 3242/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9142.4854 - val_loss: 8195.7744\n",
      "Epoch 3243/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9142.0222 - val_loss: 8195.3105\n",
      "Epoch 3244/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9141.5591 - val_loss: 8194.8477\n",
      "Epoch 3245/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9141.0971 - val_loss: 8194.3857\n",
      "Epoch 3246/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9140.6334 - val_loss: 8193.9238\n",
      "Epoch 3247/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9140.1713 - val_loss: 8193.4609\n",
      "Epoch 3248/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9139.7089 - val_loss: 8192.9980\n",
      "Epoch 3249/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9139.2456 - val_loss: 8192.5342\n",
      "Epoch 3250/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9138.7837 - val_loss: 8192.0732\n",
      "Epoch 3251/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 9138.3195 - val_loss: 8191.6108\n",
      "Epoch 3252/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9137.8580 - val_loss: 8191.1470\n",
      "Epoch 3253/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9137.3958 - val_loss: 8190.6846\n",
      "Epoch 3254/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9136.9322 - val_loss: 8190.2212\n",
      "Epoch 3255/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9136.4705 - val_loss: 8189.7598\n",
      "Epoch 3256/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9136.0064 - val_loss: 8189.2954\n",
      "Epoch 3257/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9135.5440 - val_loss: 8188.8345\n",
      "Epoch 3258/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9135.0823 - val_loss: 8188.3711\n",
      "Epoch 3259/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9134.6186 - val_loss: 8187.9082\n",
      "Epoch 3260/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9134.1562 - val_loss: 8187.4448\n",
      "Epoch 3261/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 9133.6934 - val_loss: 8186.9829\n",
      "Epoch 3262/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9133.2302 - val_loss: 8186.5195\n",
      "Epoch 3263/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9132.7687 - val_loss: 8186.0571\n",
      "Epoch 3264/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 9132.3049 - val_loss: 8185.5957\n",
      "Epoch 3265/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 9131.8426 - val_loss: 8185.1323\n",
      "Epoch 3266/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 9131.3803 - val_loss: 8184.6685\n",
      "Epoch 3267/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 9130.9165 - val_loss: 8184.2065\n",
      "Epoch 3268/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 9130.4546 - val_loss: 8183.7441\n",
      "Epoch 3269/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 9129.9915 - val_loss: 8183.2808\n",
      "Epoch 3270/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 9129.5293 - val_loss: 8182.8184\n",
      "Epoch 3271/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 9129.0667 - val_loss: 8182.3555\n",
      "Epoch 3272/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 9128.6030 - val_loss: 8181.8940\n",
      "Epoch 3273/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 9128.1410 - val_loss: 8181.4307\n",
      "Epoch 3274/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 9127.6785 - val_loss: 8180.9668\n",
      "Epoch 3275/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 9127.2155 - val_loss: 8180.5044\n",
      "Epoch 3276/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9126.7532 - val_loss: 8180.0420\n",
      "Epoch 3277/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9126.2893 - val_loss: 8179.5806\n",
      "Epoch 3278/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9125.8277 - val_loss: 8179.1167\n",
      "Epoch 3279/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9125.3651 - val_loss: 8178.6528\n",
      "Epoch 3280/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9124.9017 - val_loss: 8178.1914\n",
      "Epoch 3281/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9124.4397 - val_loss: 8177.7290\n",
      "Epoch 3282/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9123.9758 - val_loss: 8177.2651\n",
      "Epoch 3283/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9123.5141 - val_loss: 8176.8027\n",
      "Epoch 3284/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9123.0522 - val_loss: 8176.3398\n",
      "Epoch 3285/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9122.5882 - val_loss: 8175.8779\n",
      "Epoch 3286/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9122.1260 - val_loss: 8175.4165\n",
      "Epoch 3287/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9121.6627 - val_loss: 8174.9526\n",
      "Epoch 3288/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9121.2000 - val_loss: 8174.4888\n",
      "Epoch 3289/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9120.7387 - val_loss: 8174.0273\n",
      "Epoch 3290/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9120.2744 - val_loss: 8173.5649\n",
      "Epoch 3291/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9119.8119 - val_loss: 8173.1016\n",
      "Epoch 3292/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9119.3496 - val_loss: 8172.6387\n",
      "Epoch 3293/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9118.8866 - val_loss: 8172.1753\n",
      "Epoch 3294/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9118.4249 - val_loss: 8171.7139\n",
      "Epoch 3295/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9117.9613 - val_loss: 8171.2524\n",
      "Epoch 3296/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9117.4988 - val_loss: 8170.7876\n",
      "Epoch 3297/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9117.0366 - val_loss: 8170.3247\n",
      "Epoch 3298/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9116.5728 - val_loss: 8169.8623\n",
      "Epoch 3299/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9116.1112 - val_loss: 8169.4004\n",
      "Epoch 3300/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9115.6478 - val_loss: 8168.9375\n",
      "Epoch 3301/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9115.1857 - val_loss: 8168.4746\n",
      "Epoch 3302/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 91us/step - loss: 9114.7230 - val_loss: 8168.0112\n",
      "Epoch 3303/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9114.2593 - val_loss: 8167.5498\n",
      "Epoch 3304/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9113.7975 - val_loss: 8167.0879\n",
      "Epoch 3305/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9113.3345 - val_loss: 8166.6235\n",
      "Epoch 3306/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9112.8717 - val_loss: 8166.1602\n",
      "Epoch 3307/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9112.4095 - val_loss: 8165.6982\n",
      "Epoch 3308/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9111.9456 - val_loss: 8165.2363\n",
      "Epoch 3309/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9111.4841 - val_loss: 8164.7734\n",
      "Epoch 3310/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9111.0215 - val_loss: 8164.3110\n",
      "Epoch 3311/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9110.5583 - val_loss: 8163.8472\n",
      "Epoch 3312/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9110.0959 - val_loss: 8163.3857\n",
      "Epoch 3313/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9109.6323 - val_loss: 8162.9219\n",
      "Epoch 3314/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9109.1701 - val_loss: 8162.4595\n",
      "Epoch 3315/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9108.7085 - val_loss: 8161.9971\n",
      "Epoch 3316/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9108.2447 - val_loss: 8161.5337\n",
      "Epoch 3317/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9107.7822 - val_loss: 8161.0723\n",
      "Epoch 3318/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9107.3191 - val_loss: 8160.6079\n",
      "Epoch 3319/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9106.8563 - val_loss: 8160.1455\n",
      "Epoch 3320/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9106.3942 - val_loss: 8159.6836\n",
      "Epoch 3321/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9105.9311 - val_loss: 8159.2207\n",
      "Epoch 3322/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9105.4685 - val_loss: 8158.7573\n",
      "Epoch 3323/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9105.0059 - val_loss: 8158.2954\n",
      "Epoch 3324/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9104.5429 - val_loss: 8157.8320\n",
      "Epoch 3325/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9104.0806 - val_loss: 8157.3696\n",
      "Epoch 3326/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9103.6172 - val_loss: 8156.9082\n",
      "Epoch 3327/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9103.1550 - val_loss: 8156.4448\n",
      "Epoch 3328/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 9102.6928 - val_loss: 8155.9810\n",
      "Epoch 3329/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9102.2290 - val_loss: 8155.5190\n",
      "Epoch 3330/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9101.7676 - val_loss: 8155.0566\n",
      "Epoch 3331/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9101.3038 - val_loss: 8154.5933\n",
      "Epoch 3332/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 9100.8419 - val_loss: 8154.1309\n",
      "Epoch 3333/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9100.3794 - val_loss: 8153.6680\n",
      "Epoch 3334/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9099.9155 - val_loss: 8153.2065\n",
      "Epoch 3335/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9099.4538 - val_loss: 8152.7432\n",
      "Epoch 3336/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9098.9910 - val_loss: 8152.2793\n",
      "Epoch 3337/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9098.5277 - val_loss: 8151.8169\n",
      "Epoch 3338/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 9098.0657 - val_loss: 8151.3545\n",
      "Epoch 3339/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9097.6019 - val_loss: 8150.8931\n",
      "Epoch 3340/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9097.1402 - val_loss: 8150.4292\n",
      "Epoch 3341/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9096.6776 - val_loss: 8149.9668\n",
      "Epoch 3342/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9096.2137 - val_loss: 8149.5029\n",
      "Epoch 3343/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9095.7522 - val_loss: 8149.0420\n",
      "Epoch 3344/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9095.2883 - val_loss: 8148.5796\n",
      "Epoch 3345/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9094.8263 - val_loss: 8148.1152\n",
      "Epoch 3346/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9094.3648 - val_loss: 8147.6523\n",
      "Epoch 3347/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9093.9009 - val_loss: 8147.1904\n",
      "Epoch 3348/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9093.4387 - val_loss: 8146.7290\n",
      "Epoch 3349/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9092.9753 - val_loss: 8146.2637\n",
      "Epoch 3350/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9092.5128 - val_loss: 8145.8027\n",
      "Epoch 3351/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9092.0504 - val_loss: 8145.3389\n",
      "Epoch 3352/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 9091.5872 - val_loss: 8144.8774\n",
      "Epoch 3353/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 9091.1249 - val_loss: 8144.4141\n",
      "Epoch 3354/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9090.6621 - val_loss: 8143.9512\n",
      "Epoch 3355/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9090.1991 - val_loss: 8143.4888\n",
      "Epoch 3356/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9089.7374 - val_loss: 8143.0264\n",
      "Epoch 3357/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9089.2731 - val_loss: 8142.5649\n",
      "Epoch 3358/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9088.8114 - val_loss: 8142.1016\n",
      "Epoch 3359/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9088.3492 - val_loss: 8141.6372\n",
      "Epoch 3360/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9087.8854 - val_loss: 8141.1753\n",
      "Epoch 3361/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9087.4241 - val_loss: 8140.7129\n",
      "Epoch 3362/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9086.9603 - val_loss: 8140.2500\n",
      "Epoch 3363/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9086.4976 - val_loss: 8139.7876\n",
      "Epoch 3364/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9086.0356 - val_loss: 8139.3237\n",
      "Epoch 3365/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9085.5719 - val_loss: 8138.8623\n",
      "Epoch 3366/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9085.1102 - val_loss: 8138.3984\n",
      "Epoch 3367/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 9084.6471 - val_loss: 8137.9360\n",
      "Epoch 3368/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9084.1844 - val_loss: 8137.4727\n",
      "Epoch 3369/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9083.7219 - val_loss: 8137.0107\n",
      "Epoch 3370/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9083.2584 - val_loss: 8136.5488\n",
      "Epoch 3371/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9082.7954 - val_loss: 8136.0859\n",
      "Epoch 3372/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9082.3339 - val_loss: 8135.6235\n",
      "Epoch 3373/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9081.8704 - val_loss: 8135.1597\n",
      "Epoch 3374/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9081.4084 - val_loss: 8134.6973\n",
      "Epoch 3375/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9080.9446 - val_loss: 8134.2344\n",
      "Epoch 3376/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9080.4829 - val_loss: 8133.7720\n",
      "Epoch 3377/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9080.0202 - val_loss: 8133.3086\n",
      "Epoch 3378/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9079.5571 - val_loss: 8132.8462\n",
      "Epoch 3379/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9079.0950 - val_loss: 8132.3848\n",
      "Epoch 3380/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9078.6315 - val_loss: 8131.9204\n",
      "Epoch 3381/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9078.1685 - val_loss: 8131.4595\n",
      "Epoch 3382/10000\n",
      "750/750 [==============================] - 0s 81us/step - loss: 9077.7074 - val_loss: 8130.9961\n",
      "Epoch 3383/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9077.2437 - val_loss: 8130.5332\n",
      "Epoch 3384/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9076.7810 - val_loss: 8130.0698\n",
      "Epoch 3385/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9076.3183 - val_loss: 8129.6079\n",
      "Epoch 3386/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9075.8552 - val_loss: 8129.1445\n",
      "Epoch 3387/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9075.3931 - val_loss: 8128.6821\n",
      "Epoch 3388/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9074.9300 - val_loss: 8128.2197\n",
      "Epoch 3389/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9074.4674 - val_loss: 8127.7573\n",
      "Epoch 3390/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9074.0053 - val_loss: 8127.2935\n",
      "Epoch 3391/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9073.5416 - val_loss: 8126.8315\n",
      "Epoch 3392/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9073.0791 - val_loss: 8126.3691\n",
      "Epoch 3393/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9072.6165 - val_loss: 8125.9058\n",
      "Epoch 3394/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9072.1537 - val_loss: 8125.4434\n",
      "Epoch 3395/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9071.6917 - val_loss: 8124.9805\n",
      "Epoch 3396/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9071.2281 - val_loss: 8124.5190\n",
      "Epoch 3397/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9070.7664 - val_loss: 8124.0557\n",
      "Epoch 3398/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9070.3035 - val_loss: 8123.5918\n",
      "Epoch 3399/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9069.8405 - val_loss: 8123.1294\n",
      "Epoch 3400/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9069.3782 - val_loss: 8122.6670\n",
      "Epoch 3401/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9068.9144 - val_loss: 8122.2056\n",
      "Epoch 3402/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9068.4522 - val_loss: 8121.7417\n",
      "Epoch 3403/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9067.9899 - val_loss: 8121.2788\n",
      "Epoch 3404/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9067.5268 - val_loss: 8120.8164\n",
      "Epoch 3405/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9067.0648 - val_loss: 8120.3540\n",
      "Epoch 3406/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9066.6008 - val_loss: 8119.8901\n",
      "Epoch 3407/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9066.1386 - val_loss: 8119.4277\n",
      "Epoch 3408/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9065.6772 - val_loss: 8118.9648\n",
      "Epoch 3409/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9065.2126 - val_loss: 8118.5029\n",
      "Epoch 3410/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9064.7511 - val_loss: 8118.0415\n",
      "Epoch 3411/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9064.2880 - val_loss: 8117.5762\n",
      "Epoch 3412/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9063.8251 - val_loss: 8117.1138\n",
      "Epoch 3413/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9063.3630 - val_loss: 8116.6523\n",
      "Epoch 3414/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9062.8999 - val_loss: 8116.1899\n",
      "Epoch 3415/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9062.4371 - val_loss: 8115.7266\n",
      "Epoch 3416/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9061.9747 - val_loss: 8115.2637\n",
      "Epoch 3417/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9061.5114 - val_loss: 8114.8003\n",
      "Epoch 3418/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9061.0500 - val_loss: 8114.3389\n",
      "Epoch 3419/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9060.5861 - val_loss: 8113.8774\n",
      "Epoch 3420/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9060.1238 - val_loss: 8113.4141\n",
      "Epoch 3421/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9059.6615 - val_loss: 8112.9497\n",
      "Epoch 3422/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9059.1979 - val_loss: 8112.4873\n",
      "Epoch 3423/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9058.7355 - val_loss: 8112.0254\n",
      "Epoch 3424/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9058.2726 - val_loss: 8111.5625\n",
      "Epoch 3425/10000\n",
      "750/750 [==============================] - 0s 82us/step - loss: 9057.8106 - val_loss: 8111.0996\n",
      "Epoch 3426/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9057.3480 - val_loss: 8110.6362\n",
      "Epoch 3427/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9056.8843 - val_loss: 8110.1748\n",
      "Epoch 3428/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9056.4220 - val_loss: 8109.7114\n",
      "Epoch 3429/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9055.9593 - val_loss: 8109.2485\n",
      "Epoch 3430/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9055.4969 - val_loss: 8108.7852\n",
      "Epoch 3431/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9055.0344 - val_loss: 8108.3232\n",
      "Epoch 3432/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9054.5706 - val_loss: 8107.8613\n",
      "Epoch 3433/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9054.1087 - val_loss: 8107.3984\n",
      "Epoch 3434/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9053.6461 - val_loss: 8106.9346\n",
      "Epoch 3435/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9053.1829 - val_loss: 8106.4722\n",
      "Epoch 3436/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9052.7210 - val_loss: 8106.0098\n",
      "Epoch 3437/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9052.2571 - val_loss: 8105.5483\n",
      "Epoch 3438/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 9051.7949 - val_loss: 8105.0845\n",
      "Epoch 3439/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9051.3327 - val_loss: 8104.6211\n",
      "Epoch 3440/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9050.8696 - val_loss: 8104.1587\n",
      "Epoch 3441/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9050.4072 - val_loss: 8103.6973\n",
      "Epoch 3442/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9049.9440 - val_loss: 8103.2344\n",
      "Epoch 3443/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9049.4819 - val_loss: 8102.7705\n",
      "Epoch 3444/10000\n",
      "750/750 [==============================] - 0s 84us/step - loss: 9049.0191 - val_loss: 8102.3071\n",
      "Epoch 3445/10000\n",
      "750/750 [==============================] - 0s 85us/step - loss: 9048.5560 - val_loss: 8101.8457\n",
      "Epoch 3446/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 88us/step - loss: 9048.0935 - val_loss: 8101.3823\n",
      "Epoch 3447/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9047.6309 - val_loss: 8100.9204\n",
      "Epoch 3448/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 9047.1678 - val_loss: 8100.4570\n",
      "Epoch 3449/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9046.7059 - val_loss: 8099.9956\n",
      "Epoch 3450/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9046.2425 - val_loss: 8099.5332\n",
      "Epoch 3451/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 9045.7798 - val_loss: 8099.0684\n",
      "Epoch 3452/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9045.3178 - val_loss: 8098.6060\n",
      "Epoch 3453/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9044.8540 - val_loss: 8098.1445\n",
      "Epoch 3454/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9044.3925 - val_loss: 8097.6816\n",
      "Epoch 3455/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9043.9289 - val_loss: 8097.2183\n",
      "Epoch 3456/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9043.4667 - val_loss: 8096.7559\n",
      "Epoch 3457/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9043.0042 - val_loss: 8096.2930\n",
      "Epoch 3458/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9042.5406 - val_loss: 8095.8315\n",
      "Epoch 3459/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9042.0788 - val_loss: 8095.3691\n",
      "Epoch 3460/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9041.6157 - val_loss: 8094.9043\n",
      "Epoch 3461/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 9041.1522 - val_loss: 8094.4419\n",
      "Epoch 3462/10000\n",
      "750/750 [==============================] - 0s 80us/step - loss: 9040.6906 - val_loss: 8093.9795\n",
      "Epoch 3463/10000\n",
      "750/750 [==============================] - 0s 87us/step - loss: 9040.2268 - val_loss: 8093.5181\n",
      "Epoch 3464/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9039.7648 - val_loss: 8093.0542\n",
      "Epoch 3465/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 9039.3025 - val_loss: 8092.5918\n",
      "Epoch 3466/10000\n",
      "750/750 [==============================] - 0s 88us/step - loss: 9038.8394 - val_loss: 8092.1289\n",
      "Epoch 3467/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9038.3771 - val_loss: 8091.6665\n",
      "Epoch 3468/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9037.9133 - val_loss: 8091.2031\n",
      "Epoch 3469/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 9037.4511 - val_loss: 8090.7402\n",
      "Epoch 3470/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 9036.9896 - val_loss: 8090.2773\n",
      "Epoch 3471/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9036.5257 - val_loss: 8089.8154\n",
      "Epoch 3472/10000\n",
      "750/750 [==============================] - 0s 162us/step - loss: 9036.0636 - val_loss: 8089.3540\n",
      "Epoch 3473/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 9035.6002 - val_loss: 8088.8887\n",
      "Epoch 3474/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9035.1375 - val_loss: 8088.4263\n",
      "Epoch 3475/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9034.6761 - val_loss: 8087.9639\n",
      "Epoch 3476/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9034.2123 - val_loss: 8087.5024\n",
      "Epoch 3477/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 9033.7497 - val_loss: 8087.0391\n",
      "Epoch 3478/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 9033.2871 - val_loss: 8086.5762\n",
      "Epoch 3479/10000\n",
      "750/750 [==============================] - 0s 86us/step - loss: 9032.8238 - val_loss: 8086.1128\n",
      "Epoch 3480/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9032.3619 - val_loss: 8085.6504\n",
      "Epoch 3481/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 9031.8986 - val_loss: 8085.1899\n",
      "Epoch 3482/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 9031.4362 - val_loss: 8084.7266\n",
      "Epoch 3483/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 9030.9741 - val_loss: 8084.2622\n",
      "Epoch 3484/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 9030.5103 - val_loss: 8083.8003\n",
      "Epoch 3485/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9030.0487 - val_loss: 8083.3379\n",
      "Epoch 3486/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9029.5851 - val_loss: 8082.8750\n",
      "Epoch 3487/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9029.1231 - val_loss: 8082.4126\n",
      "Epoch 3488/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 9028.6605 - val_loss: 8081.9487\n",
      "Epoch 3489/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 9028.1969 - val_loss: 8081.4863\n",
      "Epoch 3490/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 9027.7348 - val_loss: 8081.0249\n",
      "Epoch 3491/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 9027.2722 - val_loss: 8080.5610\n",
      "Epoch 3492/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 9026.8091 - val_loss: 8080.0972\n",
      "Epoch 3493/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9026.3470 - val_loss: 8079.6357\n",
      "Epoch 3494/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 9025.8830 - val_loss: 8079.1738\n",
      "Epoch 3495/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 9025.4209 - val_loss: 8078.7109\n",
      "Epoch 3496/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 9024.9589 - val_loss: 8078.2471\n",
      "Epoch 3497/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 9024.4954 - val_loss: 8077.7847\n",
      "Epoch 3498/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 9024.0334 - val_loss: 8077.3223\n",
      "Epoch 3499/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 9023.5696 - val_loss: 8076.8599\n",
      "Epoch 3500/10000\n",
      "750/750 [==============================] - 0s 180us/step - loss: 9023.1074 - val_loss: 8076.3984\n",
      "Epoch 3501/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 9022.6461 - val_loss: 8075.9346\n",
      "Epoch 3502/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 9022.1818 - val_loss: 8075.4712\n",
      "Epoch 3503/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 9021.7197 - val_loss: 8075.0098\n",
      "Epoch 3504/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 9021.2565 - val_loss: 8074.5454\n",
      "Epoch 3505/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 9020.7937 - val_loss: 8074.0835\n",
      "Epoch 3506/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 9020.3319 - val_loss: 8073.6206\n",
      "Epoch 3507/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 9019.8683 - val_loss: 8073.1582\n",
      "Epoch 3508/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 9019.4062 - val_loss: 8072.6948\n",
      "Epoch 3509/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 9018.9433 - val_loss: 8072.2329\n",
      "Epoch 3510/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 9018.4801 - val_loss: 8071.7695\n",
      "Epoch 3511/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 9018.0181 - val_loss: 8071.3071\n",
      "Epoch 3512/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 9017.5542 - val_loss: 8070.8457\n",
      "Epoch 3513/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 9017.0924 - val_loss: 8070.3823\n",
      "Epoch 3514/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 9016.6303 - val_loss: 8069.9185\n",
      "Epoch 3515/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 9016.1667 - val_loss: 8069.4565\n",
      "Epoch 3516/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 9015.7042 - val_loss: 8068.9941\n",
      "Epoch 3517/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 9015.2414 - val_loss: 8068.5308\n",
      "Epoch 3518/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 9014.7789 - val_loss: 8068.0684\n",
      "Epoch 3519/10000\n",
      "750/750 [==============================] - 0s 170us/step - loss: 9014.3161 - val_loss: 8067.6055\n",
      "Epoch 3520/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 9013.8531 - val_loss: 8067.1431\n",
      "Epoch 3521/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 9013.3907 - val_loss: 8066.6792\n",
      "Epoch 3522/10000\n",
      "750/750 [==============================] - 0s 174us/step - loss: 9012.9281 - val_loss: 8066.2168\n",
      "Epoch 3523/10000\n",
      "750/750 [==============================] - 0s 167us/step - loss: 9012.4656 - val_loss: 8065.7544\n",
      "Epoch 3524/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 9012.0032 - val_loss: 8065.2920\n",
      "Epoch 3525/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 9011.5393 - val_loss: 8064.8306\n",
      "Epoch 3526/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 9011.0771 - val_loss: 8064.3667\n",
      "Epoch 3527/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 9010.6151 - val_loss: 8063.9028\n",
      "Epoch 3528/10000\n",
      "750/750 [==============================] - 0s 168us/step - loss: 9010.1516 - val_loss: 8063.4414\n",
      "Epoch 3529/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 9009.6897 - val_loss: 8062.9790\n",
      "Epoch 3530/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 9009.2258 - val_loss: 8062.5151\n",
      "Epoch 3531/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 9008.7637 - val_loss: 8062.0527\n",
      "Epoch 3532/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 9008.3021 - val_loss: 8061.5898\n",
      "Epoch 3533/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 9007.8383 - val_loss: 8061.1279\n",
      "Epoch 3534/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 9007.3762 - val_loss: 8060.6665\n",
      "Epoch 3535/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 9006.9127 - val_loss: 8060.2012\n",
      "Epoch 3536/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 9006.4499 - val_loss: 8059.7388\n",
      "Epoch 3537/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 9005.9879 - val_loss: 8059.2773\n",
      "Epoch 3538/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 9005.5241 - val_loss: 8058.8149\n",
      "Epoch 3539/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 9005.0624 - val_loss: 8058.3516\n",
      "Epoch 3540/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 9004.5996 - val_loss: 8057.8887\n",
      "Epoch 3541/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 9004.1363 - val_loss: 8057.4253\n",
      "Epoch 3542/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 9003.6743 - val_loss: 8056.9639\n",
      "Epoch 3543/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 9003.2110 - val_loss: 8056.5015\n",
      "Epoch 3544/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 9002.7486 - val_loss: 8056.0391\n",
      "Epoch 3545/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 9002.2866 - val_loss: 8055.5747\n",
      "Epoch 3546/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 9001.8228 - val_loss: 8055.1128\n",
      "Epoch 3547/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 9001.3606 - val_loss: 8054.6504\n",
      "Epoch 3548/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 9000.8973 - val_loss: 8054.1875\n",
      "Epoch 3549/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 9000.4355 - val_loss: 8053.7246\n",
      "Epoch 3550/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8999.9731 - val_loss: 8053.2607\n",
      "Epoch 3551/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8999.5092 - val_loss: 8052.7998\n",
      "Epoch 3552/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8999.0475 - val_loss: 8052.3374\n",
      "Epoch 3553/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8998.5844 - val_loss: 8051.8735\n",
      "Epoch 3554/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8998.1219 - val_loss: 8051.4102\n",
      "Epoch 3555/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8997.6594 - val_loss: 8050.9482\n",
      "Epoch 3556/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8997.1956 - val_loss: 8050.4863\n",
      "Epoch 3557/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8996.7337 - val_loss: 8050.0234\n",
      "Epoch 3558/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 8996.2714 - val_loss: 8049.5610\n",
      "Epoch 3559/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8995.8076 - val_loss: 8049.0972\n",
      "Epoch 3560/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8995.3459 - val_loss: 8048.6348\n",
      "Epoch 3561/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8994.8820 - val_loss: 8048.1719\n",
      "Epoch 3562/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8994.4199 - val_loss: 8047.7095\n",
      "Epoch 3563/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8993.9579 - val_loss: 8047.2471\n",
      "Epoch 3564/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8993.4940 - val_loss: 8046.7837\n",
      "Epoch 3565/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8993.0321 - val_loss: 8046.3223\n",
      "Epoch 3566/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8992.5691 - val_loss: 8045.8579\n",
      "Epoch 3567/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8992.1065 - val_loss: 8045.3955\n",
      "Epoch 3568/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8991.6446 - val_loss: 8044.9336\n",
      "Epoch 3569/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 8991.1810 - val_loss: 8044.4707\n",
      "Epoch 3570/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8990.7184 - val_loss: 8044.0073\n",
      "Epoch 3571/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8990.2558 - val_loss: 8043.5454\n",
      "Epoch 3572/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8989.7927 - val_loss: 8043.0820\n",
      "Epoch 3573/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8989.3304 - val_loss: 8042.6206\n",
      "Epoch 3574/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 8988.8670 - val_loss: 8042.1582\n",
      "Epoch 3575/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8988.4047 - val_loss: 8041.6948\n",
      "Epoch 3576/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8987.9428 - val_loss: 8041.2310\n",
      "Epoch 3577/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8987.4791 - val_loss: 8040.7690\n",
      "Epoch 3578/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8987.0176 - val_loss: 8040.3066\n",
      "Epoch 3579/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8986.5538 - val_loss: 8039.8433\n",
      "Epoch 3580/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8986.0910 - val_loss: 8039.3809\n",
      "Epoch 3581/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8985.6293 - val_loss: 8038.9180\n",
      "Epoch 3582/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8985.1656 - val_loss: 8038.4565\n",
      "Epoch 3583/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8984.7030 - val_loss: 8037.9932\n",
      "Epoch 3584/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8984.2407 - val_loss: 8037.5293\n",
      "Epoch 3585/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 8983.7779 - val_loss: 8037.0669\n",
      "Epoch 3586/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8983.3157 - val_loss: 8036.6045\n",
      "Epoch 3587/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8982.8520 - val_loss: 8036.1431\n",
      "Epoch 3588/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8982.3899 - val_loss: 8035.6792\n",
      "Epoch 3589/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8981.9277 - val_loss: 8035.2163\n",
      "Epoch 3590/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 121us/step - loss: 8981.4637 - val_loss: 8034.7539\n",
      "Epoch 3591/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8981.0020 - val_loss: 8034.2915\n",
      "Epoch 3592/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8980.5383 - val_loss: 8033.8291\n",
      "Epoch 3593/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8980.0763 - val_loss: 8033.3652\n",
      "Epoch 3594/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 8985.67 - 0s 124us/step - loss: 8979.6140 - val_loss: 8032.9023\n",
      "Epoch 3595/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8979.1508 - val_loss: 8032.4404\n",
      "Epoch 3596/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8978.6885 - val_loss: 8031.9775\n",
      "Epoch 3597/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8978.2252 - val_loss: 8031.5137\n",
      "Epoch 3598/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8977.7627 - val_loss: 8031.0518\n",
      "Epoch 3599/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8977.3002 - val_loss: 8030.5898\n",
      "Epoch 3600/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8976.8373 - val_loss: 8030.1274\n",
      "Epoch 3601/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8976.3748 - val_loss: 8029.6641\n",
      "Epoch 3602/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8975.9121 - val_loss: 8029.2012\n",
      "Epoch 3603/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8975.4490 - val_loss: 8028.7378\n",
      "Epoch 3604/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8974.9875 - val_loss: 8028.2764\n",
      "Epoch 3605/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8974.5230 - val_loss: 8027.8135\n",
      "Epoch 3606/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8974.0611 - val_loss: 8027.3516\n",
      "Epoch 3607/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8973.5990 - val_loss: 8026.8872\n",
      "Epoch 3608/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8973.1353 - val_loss: 8026.4253\n",
      "Epoch 3609/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8972.6730 - val_loss: 8025.9629\n",
      "Epoch 3610/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8972.2102 - val_loss: 8025.5000\n",
      "Epoch 3611/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8971.7479 - val_loss: 8025.0376\n",
      "Epoch 3612/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8971.2856 - val_loss: 8024.5737\n",
      "Epoch 3613/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8970.8225 - val_loss: 8024.1113\n",
      "Epoch 3614/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 8970.3593 - val_loss: 8023.6489\n",
      "Epoch 3615/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 8969.8968 - val_loss: 8023.1860\n",
      "Epoch 3616/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 8969.4336 - val_loss: 8022.7227\n",
      "Epoch 3617/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 8968.9719 - val_loss: 8022.2607\n",
      "Epoch 3618/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8968.5081 - val_loss: 8021.7988\n",
      "Epoch 3619/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 8968.0460 - val_loss: 8021.3359\n",
      "Epoch 3620/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 8967.5839 - val_loss: 8020.8726\n",
      "Epoch 3621/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 8967.1208 - val_loss: 8020.4097\n",
      "Epoch 3622/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 8966.6577 - val_loss: 8019.9473\n",
      "Epoch 3623/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8966.1946 - val_loss: 8019.4844\n",
      "Epoch 3624/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8965.7326 - val_loss: 8019.0220\n",
      "Epoch 3625/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8965.2699 - val_loss: 8018.5586\n",
      "Epoch 3626/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8964.8066 - val_loss: 8018.0962\n",
      "Epoch 3627/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8964.3448 - val_loss: 8017.6348\n",
      "Epoch 3628/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8963.8815 - val_loss: 8017.1704\n",
      "Epoch 3629/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8963.4188 - val_loss: 8016.7085\n",
      "Epoch 3630/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 8962.9567 - val_loss: 8016.2456\n",
      "Epoch 3631/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 8962.4927 - val_loss: 8015.7832\n",
      "Epoch 3632/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8962.0310 - val_loss: 8015.3198\n",
      "Epoch 3633/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8961.5683 - val_loss: 8014.8579\n",
      "Epoch 3634/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 8961.1050 - val_loss: 8014.3945\n",
      "Epoch 3635/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 8960.6429 - val_loss: 8013.9321\n",
      "Epoch 3636/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 8960.1800 - val_loss: 8013.4692\n",
      "Epoch 3637/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 8959.7172 - val_loss: 8013.0073\n",
      "Epoch 3638/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 8959.2553 - val_loss: 8012.5444\n",
      "Epoch 3639/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 8958.7915 - val_loss: 8012.0815\n",
      "Epoch 3640/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 8958.3293 - val_loss: 8011.6191\n",
      "Epoch 3641/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 8957.8663 - val_loss: 8011.1558\n",
      "Epoch 3642/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 8957.4042 - val_loss: 8010.6929\n",
      "Epoch 3643/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 8956.9418 - val_loss: 8010.2305\n",
      "Epoch 3644/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 8956.4781 - val_loss: 8009.7690\n",
      "Epoch 3645/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8956.0154 - val_loss: 8009.3052\n",
      "Epoch 3646/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8955.5532 - val_loss: 8008.8418\n",
      "Epoch 3647/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8955.0906 - val_loss: 8008.3789\n",
      "Epoch 3648/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8954.6281 - val_loss: 8007.9170\n",
      "Epoch 3649/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8954.1644 - val_loss: 8007.4556\n",
      "Epoch 3650/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8953.7020 - val_loss: 8006.9917\n",
      "Epoch 3651/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 8953.2398 - val_loss: 8006.5288\n",
      "Epoch 3652/10000\n",
      "750/750 [==============================] - 0s 181us/step - loss: 8952.7768 - val_loss: 8006.0664\n",
      "Epoch 3653/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8952.3147 - val_loss: 8005.6040\n",
      "Epoch 3654/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8951.8508 - val_loss: 8005.1401\n",
      "Epoch 3655/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8951.3886 - val_loss: 8004.6777\n",
      "Epoch 3656/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8950.9264 - val_loss: 8004.2148\n",
      "Epoch 3657/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8950.4631 - val_loss: 8003.7529\n",
      "Epoch 3658/10000\n",
      "750/750 [==============================] - 0s 107us/step - loss: 8950.0010 - val_loss: 8003.2915\n",
      "Epoch 3659/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8949.5378 - val_loss: 8002.8262\n",
      "Epoch 3660/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8949.0750 - val_loss: 8002.3643\n",
      "Epoch 3661/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8948.6128 - val_loss: 8001.9023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3662/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8948.1498 - val_loss: 8001.4399\n",
      "Epoch 3663/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 8947.6872 - val_loss: 8000.9766\n",
      "Epoch 3664/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 8947.2246 - val_loss: 8000.5137\n",
      "Epoch 3665/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 8946.7615 - val_loss: 8000.0503\n",
      "Epoch 3666/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 8946.2989 - val_loss: 7999.5889\n",
      "Epoch 3667/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 8945.8356 - val_loss: 7999.1274\n",
      "Epoch 3668/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 8945.3736 - val_loss: 7998.6641\n",
      "Epoch 3669/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 8944.9116 - val_loss: 7998.1997\n",
      "Epoch 3670/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8944.4480 - val_loss: 7997.7373\n",
      "Epoch 3671/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 8943.9860 - val_loss: 7997.2754\n",
      "Epoch 3672/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8943.5226 - val_loss: 7996.8125\n",
      "Epoch 3673/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8943.0605 - val_loss: 7996.3496\n",
      "Epoch 3674/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8942.5980 - val_loss: 7995.8862\n",
      "Epoch 3675/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8942.1344 - val_loss: 7995.4238\n",
      "Epoch 3676/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8941.6717 - val_loss: 7994.9609\n",
      "Epoch 3677/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8941.2092 - val_loss: 7994.4985\n",
      "Epoch 3678/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8940.7467 - val_loss: 7994.0352\n",
      "Epoch 3679/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8940.2845 - val_loss: 7993.5732\n",
      "Epoch 3680/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8939.8206 - val_loss: 7993.1113\n",
      "Epoch 3681/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8939.3581 - val_loss: 7992.6484\n",
      "Epoch 3682/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8938.8963 - val_loss: 7992.1851\n",
      "Epoch 3683/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8938.4325 - val_loss: 7991.7222\n",
      "Epoch 3684/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8937.9710 - val_loss: 7991.2598\n",
      "Epoch 3685/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8937.5071 - val_loss: 7990.7969\n",
      "Epoch 3686/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8937.0449 - val_loss: 7990.3345\n",
      "Epoch 3687/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8936.5827 - val_loss: 7989.8711\n",
      "Epoch 3688/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8936.1195 - val_loss: 7989.4087\n",
      "Epoch 3689/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8935.6573 - val_loss: 7988.9473\n",
      "Epoch 3690/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8935.1940 - val_loss: 7988.4844\n",
      "Epoch 3691/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8934.7313 - val_loss: 7988.0210\n",
      "Epoch 3692/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8934.2690 - val_loss: 7987.5586\n",
      "Epoch 3693/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8933.8059 - val_loss: 7987.0957\n",
      "Epoch 3694/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8933.3434 - val_loss: 7986.6323\n",
      "Epoch 3695/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8932.8809 - val_loss: 7986.1704\n",
      "Epoch 3696/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8932.4178 - val_loss: 7985.7070\n",
      "Epoch 3697/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8931.9555 - val_loss: 7985.2446\n",
      "Epoch 3698/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8931.4920 - val_loss: 7984.7817\n",
      "Epoch 3699/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8931.0296 - val_loss: 7984.3184\n",
      "Epoch 3700/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8930.5673 - val_loss: 7983.8560\n",
      "Epoch 3701/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8930.1041 - val_loss: 7983.3940\n",
      "Epoch 3702/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8929.6417 - val_loss: 7982.9316\n",
      "Epoch 3703/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8929.1786 - val_loss: 7982.4683\n",
      "Epoch 3704/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8928.7166 - val_loss: 7982.0059\n",
      "Epoch 3705/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8928.2542 - val_loss: 7981.5430\n",
      "Epoch 3706/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8927.7906 - val_loss: 7981.0815\n",
      "Epoch 3707/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8927.3279 - val_loss: 7980.6177\n",
      "Epoch 3708/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8926.8656 - val_loss: 7980.1543\n",
      "Epoch 3709/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8926.4022 - val_loss: 7979.6914\n",
      "Epoch 3710/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8925.9406 - val_loss: 7979.2295\n",
      "Epoch 3711/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8925.4769 - val_loss: 7978.7681\n",
      "Epoch 3712/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8925.0144 - val_loss: 7978.3042\n",
      "Epoch 3713/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8924.5526 - val_loss: 7977.8418\n",
      "Epoch 3714/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8924.0892 - val_loss: 7977.3789\n",
      "Epoch 3715/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8923.6271 - val_loss: 7976.9165\n",
      "Epoch 3716/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8923.1634 - val_loss: 7976.4531\n",
      "Epoch 3717/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8922.7012 - val_loss: 7975.9902\n",
      "Epoch 3718/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 8922.2388 - val_loss: 7975.5273\n",
      "Epoch 3719/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8921.7757 - val_loss: 7975.0654\n",
      "Epoch 3720/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 8921.3132 - val_loss: 7974.6040\n",
      "Epoch 3721/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8920.8502 - val_loss: 7974.1387\n",
      "Epoch 3722/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8920.3876 - val_loss: 7973.6763\n",
      "Epoch 3723/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 8919.9259 - val_loss: 7973.2148\n",
      "Epoch 3724/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 8919.4614 - val_loss: 7972.7524\n",
      "Epoch 3725/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8918.9996 - val_loss: 7972.2891\n",
      "Epoch 3726/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8918.5370 - val_loss: 7971.8262\n",
      "Epoch 3727/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8918.0739 - val_loss: 7971.3628\n",
      "Epoch 3728/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8917.6115 - val_loss: 7970.9004\n",
      "Epoch 3729/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8917.1484 - val_loss: 7970.4385\n",
      "Epoch 3730/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8916.6860 - val_loss: 7969.9766\n",
      "Epoch 3731/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8916.2241 - val_loss: 7969.5122\n",
      "Epoch 3732/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8915.7603 - val_loss: 7969.0503\n",
      "Epoch 3733/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8915.2979 - val_loss: 7968.5879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3734/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 8914.8349 - val_loss: 7968.1235\n",
      "Epoch 3735/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8914.3724 - val_loss: 7967.6626\n",
      "Epoch 3736/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8913.9105 - val_loss: 7967.1987\n",
      "Epoch 3737/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8913.4468 - val_loss: 7966.7373\n",
      "Epoch 3738/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8912.9840 - val_loss: 7966.2734\n",
      "Epoch 3739/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8912.5217 - val_loss: 7965.8110\n",
      "Epoch 3740/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8912.0589 - val_loss: 7965.3477\n",
      "Epoch 3741/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8911.5967 - val_loss: 7964.8857\n",
      "Epoch 3742/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8911.1331 - val_loss: 7964.4238\n",
      "Epoch 3743/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8910.6705 - val_loss: 7963.9609\n",
      "Epoch 3744/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8910.2086 - val_loss: 7963.4976\n",
      "Epoch 3745/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8909.7456 - val_loss: 7963.0347\n",
      "Epoch 3746/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8909.2833 - val_loss: 7962.5723\n",
      "Epoch 3747/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8908.8196 - val_loss: 7962.1094\n",
      "Epoch 3748/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8908.3573 - val_loss: 7961.6470\n",
      "Epoch 3749/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8907.8957 - val_loss: 7961.1836\n",
      "Epoch 3750/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8907.4318 - val_loss: 7960.7212\n",
      "Epoch 3751/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8906.9698 - val_loss: 7960.2583\n",
      "Epoch 3752/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8906.5064 - val_loss: 7959.7954\n",
      "Epoch 3753/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8906.0438 - val_loss: 7959.3335\n",
      "Epoch 3754/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8905.5815 - val_loss: 7958.8711\n",
      "Epoch 3755/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8905.1185 - val_loss: 7958.4082\n",
      "Epoch 3756/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8904.6560 - val_loss: 7957.9448\n",
      "Epoch 3757/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8904.1933 - val_loss: 7957.4829\n",
      "Epoch 3758/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8903.7301 - val_loss: 7957.0195\n",
      "Epoch 3759/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8903.2680 - val_loss: 7956.5571\n",
      "Epoch 3760/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8902.8044 - val_loss: 7956.0957\n",
      "Epoch 3761/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8902.3425 - val_loss: 7955.6323\n",
      "Epoch 3762/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8901.8803 - val_loss: 7955.1685\n",
      "Epoch 3763/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8901.4166 - val_loss: 7954.7065\n",
      "Epoch 3764/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8900.9540 - val_loss: 7954.2441\n",
      "Epoch 3765/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8900.4912 - val_loss: 7953.7808\n",
      "Epoch 3766/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8900.0290 - val_loss: 7953.3184\n",
      "Epoch 3767/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8899.5667 - val_loss: 7952.8555\n",
      "Epoch 3768/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8899.1031 - val_loss: 7952.3931\n",
      "Epoch 3769/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8898.6406 - val_loss: 7951.9302\n",
      "Epoch 3770/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8898.1779 - val_loss: 7951.4668\n",
      "Epoch 3771/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8897.7148 - val_loss: 7951.0044\n",
      "Epoch 3772/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8897.2532 - val_loss: 7950.5420\n",
      "Epoch 3773/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8896.7894 - val_loss: 7950.0806\n",
      "Epoch 3774/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8896.3272 - val_loss: 7949.6167\n",
      "Epoch 3775/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8895.8650 - val_loss: 7949.1528\n",
      "Epoch 3776/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8895.4017 - val_loss: 7948.6914\n",
      "Epoch 3777/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8894.9396 - val_loss: 7948.2290\n",
      "Epoch 3778/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8894.4757 - val_loss: 7947.7651\n",
      "Epoch 3779/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8894.0137 - val_loss: 7947.3027\n",
      "Epoch 3780/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8893.5512 - val_loss: 7946.8398\n",
      "Epoch 3781/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8893.0876 - val_loss: 7946.3779\n",
      "Epoch 3782/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8892.6260 - val_loss: 7945.9165\n",
      "Epoch 3783/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8892.1627 - val_loss: 7945.4512\n",
      "Epoch 3784/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8891.7000 - val_loss: 7944.9888\n",
      "Epoch 3785/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8891.2379 - val_loss: 7944.5273\n",
      "Epoch 3786/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8890.7741 - val_loss: 7944.0649\n",
      "Epoch 3787/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8890.3120 - val_loss: 7943.6016\n",
      "Epoch 3788/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8889.8496 - val_loss: 7943.1387\n",
      "Epoch 3789/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8889.3862 - val_loss: 7942.6753\n",
      "Epoch 3790/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8888.9240 - val_loss: 7942.2129\n",
      "Epoch 3791/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8888.4609 - val_loss: 7941.7510\n",
      "Epoch 3792/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8887.9986 - val_loss: 7941.2876\n",
      "Epoch 3793/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8887.5359 - val_loss: 7940.8247\n",
      "Epoch 3794/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 8887.0728 - val_loss: 7940.3628\n",
      "Epoch 3795/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8886.6105 - val_loss: 7939.9004\n",
      "Epoch 3796/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8886.1471 - val_loss: 7939.4360\n",
      "Epoch 3797/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8885.6848 - val_loss: 7938.9736\n",
      "Epoch 3798/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8885.2229 - val_loss: 7938.5112\n",
      "Epoch 3799/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8884.7593 - val_loss: 7938.0498\n",
      "Epoch 3800/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8884.2971 - val_loss: 7937.5864\n",
      "Epoch 3801/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8883.8344 - val_loss: 7937.1235\n",
      "Epoch 3802/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8883.3716 - val_loss: 7936.6597\n",
      "Epoch 3803/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 8882.9094 - val_loss: 7936.1982\n",
      "Epoch 3804/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8882.4456 - val_loss: 7935.7363\n",
      "Epoch 3805/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8881.9832 - val_loss: 7935.2734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3806/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8881.5212 - val_loss: 7934.8101\n",
      "Epoch 3807/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8881.0574 - val_loss: 7934.3472\n",
      "Epoch 3808/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8880.5959 - val_loss: 7933.8848\n",
      "Epoch 3809/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8880.1320 - val_loss: 7933.4219\n",
      "Epoch 3810/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8879.6693 - val_loss: 7932.9595\n",
      "Epoch 3811/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8879.2077 - val_loss: 7932.4961\n",
      "Epoch 3812/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8878.7437 - val_loss: 7932.0337\n",
      "Epoch 3813/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8878.2822 - val_loss: 7931.5723\n",
      "Epoch 3814/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8877.8190 - val_loss: 7931.1079\n",
      "Epoch 3815/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8877.3563 - val_loss: 7930.6455\n",
      "Epoch 3816/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8876.8937 - val_loss: 7930.1836\n",
      "Epoch 3817/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8876.4308 - val_loss: 7929.7207\n",
      "Epoch 3818/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8875.9683 - val_loss: 7929.2573\n",
      "Epoch 3819/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8875.5059 - val_loss: 7928.7954\n",
      "Epoch 3820/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8875.0428 - val_loss: 7928.3320\n",
      "Epoch 3821/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 8874.5804 - val_loss: 7927.8696\n",
      "Epoch 3822/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 8874.1166 - val_loss: 7927.4067\n",
      "Epoch 3823/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8873.6544 - val_loss: 7926.9448\n",
      "Epoch 3824/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8873.1928 - val_loss: 7926.4810\n",
      "Epoch 3825/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8872.7291 - val_loss: 7926.0190\n",
      "Epoch 3826/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8872.2674 - val_loss: 7925.5566\n",
      "Epoch 3827/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8871.8036 - val_loss: 7925.0918\n",
      "Epoch 3828/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8871.3411 - val_loss: 7924.6309\n",
      "Epoch 3829/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8870.8791 - val_loss: 7924.1680\n",
      "Epoch 3830/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8870.4155 - val_loss: 7923.7056\n",
      "Epoch 3831/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8869.9528 - val_loss: 7923.2417\n",
      "Epoch 3832/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8869.4905 - val_loss: 7922.7793\n",
      "Epoch 3833/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8869.0277 - val_loss: 7922.3169\n",
      "Epoch 3834/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8868.5657 - val_loss: 7921.8545\n",
      "Epoch 3835/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8868.1016 - val_loss: 7921.3931\n",
      "Epoch 3836/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8867.6397 - val_loss: 7920.9292\n",
      "Epoch 3837/10000\n",
      "750/750 [==============================] - 0s 162us/step - loss: 8867.1774 - val_loss: 7920.4663\n",
      "Epoch 3838/10000\n",
      "750/750 [==============================] - 0s 169us/step - loss: 8866.7138 - val_loss: 7920.0029\n",
      "Epoch 3839/10000\n",
      "750/750 [==============================] - 0s 181us/step - loss: 8866.2519 - val_loss: 7919.5415\n",
      "Epoch 3840/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 8865.7884 - val_loss: 7919.0791\n",
      "Epoch 3841/10000\n",
      "750/750 [==============================] - 0s 172us/step - loss: 8865.3263 - val_loss: 7918.6152\n",
      "Epoch 3842/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 8864.8638 - val_loss: 7918.1523\n",
      "Epoch 3843/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8864.4000 - val_loss: 7917.6899\n",
      "Epoch 3844/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8863.9377 - val_loss: 7917.2290\n",
      "Epoch 3845/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8863.4752 - val_loss: 7916.7637\n",
      "Epoch 3846/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8863.0125 - val_loss: 7916.3018\n",
      "Epoch 3847/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8862.5502 - val_loss: 7915.8389\n",
      "Epoch 3848/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8862.0865 - val_loss: 7915.3774\n",
      "Epoch 3849/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8861.6248 - val_loss: 7914.9141\n",
      "Epoch 3850/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8861.1621 - val_loss: 7914.4512\n",
      "Epoch 3851/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8860.6989 - val_loss: 7913.9878\n",
      "Epoch 3852/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8860.2372 - val_loss: 7913.5264\n",
      "Epoch 3853/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8859.7729 - val_loss: 7913.0635\n",
      "Epoch 3854/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8859.3111 - val_loss: 7912.6016\n",
      "Epoch 3855/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8858.8491 - val_loss: 7912.1362\n",
      "Epoch 3856/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8858.3854 - val_loss: 7911.6748\n",
      "Epoch 3857/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8857.9230 - val_loss: 7911.2129\n",
      "Epoch 3858/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8857.4600 - val_loss: 7910.7500\n",
      "Epoch 3859/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8856.9975 - val_loss: 7910.2876\n",
      "Epoch 3860/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8856.5355 - val_loss: 7909.8237\n",
      "Epoch 3861/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8856.0711 - val_loss: 7909.3613\n",
      "Epoch 3862/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8855.6090 - val_loss: 7908.8989\n",
      "Epoch 3863/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8855.1465 - val_loss: 7908.4360\n",
      "Epoch 3864/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8854.6834 - val_loss: 7907.9722\n",
      "Epoch 3865/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8854.2216 - val_loss: 7907.5107\n",
      "Epoch 3866/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8853.7581 - val_loss: 7907.0488\n",
      "Epoch 3867/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8853.2956 - val_loss: 7906.5859\n",
      "Epoch 3868/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8852.8337 - val_loss: 7906.1226\n",
      "Epoch 3869/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8852.3705 - val_loss: 7905.6597\n",
      "Epoch 3870/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8851.9083 - val_loss: 7905.1973\n",
      "Epoch 3871/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8851.4445 - val_loss: 7904.7344\n",
      "Epoch 3872/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8850.9824 - val_loss: 7904.2710\n",
      "Epoch 3873/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8850.5198 - val_loss: 7903.8086\n",
      "Epoch 3874/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8850.0561 - val_loss: 7903.3462\n",
      "Epoch 3875/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8849.5946 - val_loss: 7902.8848\n",
      "Epoch 3876/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8849.1313 - val_loss: 7902.4204\n",
      "Epoch 3877/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8848.6688 - val_loss: 7901.9580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3878/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8848.2065 - val_loss: 7901.4956\n",
      "Epoch 3879/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8847.7427 - val_loss: 7901.0332\n",
      "Epoch 3880/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8847.2808 - val_loss: 7900.5698\n",
      "Epoch 3881/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8846.8184 - val_loss: 7900.1079\n",
      "Epoch 3882/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8846.3552 - val_loss: 7899.6445\n",
      "Epoch 3883/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8845.8927 - val_loss: 7899.1821\n",
      "Epoch 3884/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8845.4300 - val_loss: 7898.7183\n",
      "Epoch 3885/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8844.9670 - val_loss: 7898.2573\n",
      "Epoch 3886/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8844.5054 - val_loss: 7897.7935\n",
      "Epoch 3887/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8844.0416 - val_loss: 7897.3315\n",
      "Epoch 3888/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8843.5791 - val_loss: 7896.8691\n",
      "Epoch 3889/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8843.1162 - val_loss: 7896.4043\n",
      "Epoch 3890/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8842.6534 - val_loss: 7895.9429\n",
      "Epoch 3891/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8842.1917 - val_loss: 7895.4805\n",
      "Epoch 3892/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8841.7281 - val_loss: 7895.0190\n",
      "Epoch 3893/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8841.2654 - val_loss: 7894.5552\n",
      "Epoch 3894/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8840.8029 - val_loss: 7894.0918\n",
      "Epoch 3895/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8840.3398 - val_loss: 7893.6289\n",
      "Epoch 3896/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8839.8775 - val_loss: 7893.1670\n",
      "Epoch 3897/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8839.4143 - val_loss: 7892.7056\n",
      "Epoch 3898/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8838.9519 - val_loss: 7892.2417\n",
      "Epoch 3899/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 8838.4898 - val_loss: 7891.7778\n",
      "Epoch 3900/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 8838.0267 - val_loss: 7891.3164\n",
      "Epoch 3901/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8837.5647 - val_loss: 7890.8540\n",
      "Epoch 3902/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8837.1008 - val_loss: 7890.3901\n",
      "Epoch 3903/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8836.6386 - val_loss: 7889.9277\n",
      "Epoch 3904/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8836.1763 - val_loss: 7889.4648\n",
      "Epoch 3905/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8835.7125 - val_loss: 7889.0029\n",
      "Epoch 3906/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8835.2511 - val_loss: 7888.5415\n",
      "Epoch 3907/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8834.7877 - val_loss: 7888.0762\n",
      "Epoch 3908/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8834.3248 - val_loss: 7887.6138\n",
      "Epoch 3909/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8833.8625 - val_loss: 7887.1523\n",
      "Epoch 3910/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8833.3995 - val_loss: 7886.6899\n",
      "Epoch 3911/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8832.9371 - val_loss: 7886.2266\n",
      "Epoch 3912/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8832.4745 - val_loss: 7885.7637\n",
      "Epoch 3913/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8832.0110 - val_loss: 7885.3003\n",
      "Epoch 3914/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8831.5489 - val_loss: 7884.8389\n",
      "Epoch 3915/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8831.0858 - val_loss: 7884.3774\n",
      "Epoch 3916/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8830.6233 - val_loss: 7883.9126\n",
      "Epoch 3917/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8830.1616 - val_loss: 7883.4497\n",
      "Epoch 3918/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8829.6980 - val_loss: 7882.9873\n",
      "Epoch 3919/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8829.2354 - val_loss: 7882.5254\n",
      "Epoch 3920/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8828.7722 - val_loss: 7882.0625\n",
      "Epoch 3921/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8828.3103 - val_loss: 7881.5996\n",
      "Epoch 3922/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8827.8477 - val_loss: 7881.1362\n",
      "Epoch 3923/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8827.3843 - val_loss: 7880.6738\n",
      "Epoch 3924/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8826.9217 - val_loss: 7880.2109\n",
      "Epoch 3925/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8826.4591 - val_loss: 7879.7485\n",
      "Epoch 3926/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8825.9966 - val_loss: 7879.2847\n",
      "Epoch 3927/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8825.5344 - val_loss: 7878.8232\n",
      "Epoch 3928/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8825.0704 - val_loss: 7878.3613\n",
      "Epoch 3929/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8824.6081 - val_loss: 7877.8984\n",
      "Epoch 3930/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8824.1464 - val_loss: 7877.4346\n",
      "Epoch 3931/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8823.6823 - val_loss: 7876.9722\n",
      "Epoch 3932/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8823.2208 - val_loss: 7876.5098\n",
      "Epoch 3933/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8822.7571 - val_loss: 7876.0469\n",
      "Epoch 3934/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8822.2949 - val_loss: 7875.5845\n",
      "Epoch 3935/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8821.8325 - val_loss: 7875.1211\n",
      "Epoch 3936/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8821.3695 - val_loss: 7874.6587\n",
      "Epoch 3937/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8820.9069 - val_loss: 7874.1973\n",
      "Epoch 3938/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8820.4440 - val_loss: 7873.7344\n",
      "Epoch 3939/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8819.9813 - val_loss: 7873.2705\n",
      "Epoch 3940/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8819.5188 - val_loss: 7872.8071\n",
      "Epoch 3941/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8819.0552 - val_loss: 7872.3457\n",
      "Epoch 3942/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8818.5934 - val_loss: 7871.8823\n",
      "Epoch 3943/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8818.1308 - val_loss: 7871.4204\n",
      "Epoch 3944/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8817.6677 - val_loss: 7870.9570\n",
      "Epoch 3945/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8817.2052 - val_loss: 7870.4946\n",
      "Epoch 3946/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8816.7416 - val_loss: 7870.0317\n",
      "Epoch 3947/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8816.2798 - val_loss: 7869.5684\n",
      "Epoch 3948/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8815.8172 - val_loss: 7869.1055\n",
      "Epoch 3949/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8815.3541 - val_loss: 7868.6440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3950/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8814.8916 - val_loss: 7868.1816\n",
      "Epoch 3951/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8814.4284 - val_loss: 7867.7183\n",
      "Epoch 3952/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8813.9655 - val_loss: 7867.2554\n",
      "Epoch 3953/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8813.5042 - val_loss: 7866.7930\n",
      "Epoch 3954/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8813.0406 - val_loss: 7866.3306\n",
      "Epoch 3955/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8812.5778 - val_loss: 7865.8667\n",
      "Epoch 3956/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8812.1154 - val_loss: 7865.4043\n",
      "Epoch 3957/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8811.6522 - val_loss: 7864.9414\n",
      "Epoch 3958/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8811.1906 - val_loss: 7864.4795\n",
      "Epoch 3959/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8810.7269 - val_loss: 7864.0166\n",
      "Epoch 3960/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8810.2641 - val_loss: 7863.5542\n",
      "Epoch 3961/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8809.8022 - val_loss: 7863.0918\n",
      "Epoch 3962/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8809.3392 - val_loss: 7862.6289\n",
      "Epoch 3963/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8808.8771 - val_loss: 7862.1665\n",
      "Epoch 3964/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8808.4134 - val_loss: 7861.7031\n",
      "Epoch 3965/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8807.9510 - val_loss: 7861.2402\n",
      "Epoch 3966/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8807.4886 - val_loss: 7860.7773\n",
      "Epoch 3967/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8807.0251 - val_loss: 7860.3154\n",
      "Epoch 3968/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8806.5632 - val_loss: 7859.8540\n",
      "Epoch 3969/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8806.1002 - val_loss: 7859.3887\n",
      "Epoch 3970/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8805.6374 - val_loss: 7858.9263\n",
      "Epoch 3971/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 8805.1757 - val_loss: 7858.4639\n",
      "Epoch 3972/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8804.7112 - val_loss: 7858.0024\n",
      "Epoch 3973/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8804.2496 - val_loss: 7857.5391\n",
      "Epoch 3974/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8803.7869 - val_loss: 7857.0762\n",
      "Epoch 3975/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8803.3239 - val_loss: 7856.6128\n",
      "Epoch 3976/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 8802.8616 - val_loss: 7856.1504\n",
      "Epoch 3977/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8802.3979 - val_loss: 7855.6875\n",
      "Epoch 3978/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8801.9359 - val_loss: 7855.2266\n",
      "Epoch 3979/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8801.4739 - val_loss: 7854.7622\n",
      "Epoch 3980/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8801.0103 - val_loss: 7854.2998\n",
      "Epoch 3981/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8800.5479 - val_loss: 7853.8379\n",
      "Epoch 3982/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8800.0847 - val_loss: 7853.3735\n",
      "Epoch 3983/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8799.6220 - val_loss: 7852.9111\n",
      "Epoch 3984/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8799.1605 - val_loss: 7852.4487\n",
      "Epoch 3985/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8798.6968 - val_loss: 7851.9863\n",
      "Epoch 3986/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 8798.2341 - val_loss: 7851.5234\n",
      "Epoch 3987/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8797.7715 - val_loss: 7851.0610\n",
      "Epoch 3988/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8797.3089 - val_loss: 7850.5972\n",
      "Epoch 3989/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8796.8462 - val_loss: 7850.1357\n",
      "Epoch 3990/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8796.3831 - val_loss: 7849.6738\n",
      "Epoch 3991/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 8795.9208 - val_loss: 7849.2109\n",
      "Epoch 3992/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8795.4586 - val_loss: 7848.7471\n",
      "Epoch 3993/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8794.9953 - val_loss: 7848.2837\n",
      "Epoch 3994/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8794.5332 - val_loss: 7847.8223\n",
      "Epoch 3995/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8794.0696 - val_loss: 7847.3594\n",
      "Epoch 3996/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8793.6074 - val_loss: 7846.8970\n",
      "Epoch 3997/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8793.1453 - val_loss: 7846.4336\n",
      "Epoch 3998/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8792.6812 - val_loss: 7845.9712\n",
      "Epoch 3999/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8792.2190 - val_loss: 7845.5083\n",
      "Epoch 4000/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8791.7561 - val_loss: 7845.0454\n",
      "Epoch 4001/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8791.2937 - val_loss: 7844.5830\n",
      "Epoch 4002/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8790.8313 - val_loss: 7844.1211\n",
      "Epoch 4003/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8790.3677 - val_loss: 7843.6582\n",
      "Epoch 4004/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8789.9057 - val_loss: 7843.1948\n",
      "Epoch 4005/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8789.4431 - val_loss: 7842.7319\n",
      "Epoch 4006/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8788.9800 - val_loss: 7842.2695\n",
      "Epoch 4007/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8788.5177 - val_loss: 7841.8071\n",
      "Epoch 4008/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8788.0542 - val_loss: 7841.3442\n",
      "Epoch 4009/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8787.5919 - val_loss: 7840.8823\n",
      "Epoch 4010/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8787.1303 - val_loss: 7840.4180\n",
      "Epoch 4011/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8786.6665 - val_loss: 7839.9565\n",
      "Epoch 4012/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8786.2041 - val_loss: 7839.4941\n",
      "Epoch 4013/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8785.7412 - val_loss: 7839.0308\n",
      "Epoch 4014/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8785.2789 - val_loss: 7838.5679\n",
      "Epoch 4015/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8784.8159 - val_loss: 7838.1055\n",
      "Epoch 4016/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8784.3531 - val_loss: 7837.6431\n",
      "Epoch 4017/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8783.8904 - val_loss: 7837.1792\n",
      "Epoch 4018/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8783.4277 - val_loss: 7836.7168\n",
      "Epoch 4019/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8782.9649 - val_loss: 7836.2544\n",
      "Epoch 4020/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8782.5031 - val_loss: 7835.7920\n",
      "Epoch 4021/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8782.0392 - val_loss: 7835.3306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4022/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8781.5773 - val_loss: 7834.8652\n",
      "Epoch 4023/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8781.1148 - val_loss: 7834.4028\n",
      "Epoch 4024/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8780.6511 - val_loss: 7833.9414\n",
      "Epoch 4025/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8780.1895 - val_loss: 7833.4790\n",
      "Epoch 4026/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8779.7258 - val_loss: 7833.0151\n",
      "Epoch 4027/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8779.2636 - val_loss: 7832.5527\n",
      "Epoch 4028/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8778.8012 - val_loss: 7832.0898\n",
      "Epoch 4029/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8778.3376 - val_loss: 7831.6279\n",
      "Epoch 4030/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8777.8761 - val_loss: 7831.1650\n",
      "Epoch 4031/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8777.4126 - val_loss: 7830.7012\n",
      "Epoch 4032/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8776.9500 - val_loss: 7830.2388\n",
      "Epoch 4033/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8776.4875 - val_loss: 7829.7773\n",
      "Epoch 4034/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8776.0239 - val_loss: 7829.3149\n",
      "Epoch 4035/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8775.5621 - val_loss: 7828.8516\n",
      "Epoch 4036/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8775.0997 - val_loss: 7828.3887\n",
      "Epoch 4037/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8774.6362 - val_loss: 7827.9253\n",
      "Epoch 4038/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8774.1739 - val_loss: 7827.4629\n",
      "Epoch 4039/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8773.7108 - val_loss: 7827.0000\n",
      "Epoch 4040/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8773.2484 - val_loss: 7826.5376\n",
      "Epoch 4041/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8772.7862 - val_loss: 7826.0747\n",
      "Epoch 4042/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8772.3229 - val_loss: 7825.6123\n",
      "Epoch 4043/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8771.8605 - val_loss: 7825.1504\n",
      "Epoch 4044/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8771.3974 - val_loss: 7824.6875\n",
      "Epoch 4045/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8770.9346 - val_loss: 7824.2246\n",
      "Epoch 4046/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8770.4730 - val_loss: 7823.7607\n",
      "Epoch 4047/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8770.0091 - val_loss: 7823.2988\n",
      "Epoch 4048/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8769.5473 - val_loss: 7822.8364\n",
      "Epoch 4049/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8769.0841 - val_loss: 7822.3735\n",
      "Epoch 4050/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8768.6207 - val_loss: 7821.9097\n",
      "Epoch 4051/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8768.1594 - val_loss: 7821.4482\n",
      "Epoch 4052/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8767.6954 - val_loss: 7820.9849\n",
      "Epoch 4053/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8767.2329 - val_loss: 7820.5234\n",
      "Epoch 4054/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8766.7712 - val_loss: 7820.0601\n",
      "Epoch 4055/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 8766.3072 - val_loss: 7819.5972\n",
      "Epoch 4056/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8765.8457 - val_loss: 7819.1348\n",
      "Epoch 4057/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8765.3819 - val_loss: 7818.6719\n",
      "Epoch 4058/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8764.9200 - val_loss: 7818.2095\n",
      "Epoch 4059/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8764.4575 - val_loss: 7817.7461\n",
      "Epoch 4060/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8763.9937 - val_loss: 7817.2837\n",
      "Epoch 4061/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8763.5319 - val_loss: 7816.8208\n",
      "Epoch 4062/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8763.0689 - val_loss: 7816.3579\n",
      "Epoch 4063/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8762.6060 - val_loss: 7815.8955\n",
      "Epoch 4064/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8762.1438 - val_loss: 7815.4321\n",
      "Epoch 4065/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8761.6804 - val_loss: 7814.9707\n",
      "Epoch 4066/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8761.2178 - val_loss: 7814.5073\n",
      "Epoch 4067/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8760.7558 - val_loss: 7814.0454\n",
      "Epoch 4068/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8760.2926 - val_loss: 7813.5820\n",
      "Epoch 4069/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8759.8302 - val_loss: 7813.1191\n",
      "Epoch 4070/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8759.3667 - val_loss: 7812.6567\n",
      "Epoch 4071/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8758.9044 - val_loss: 7812.1948\n",
      "Epoch 4072/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8758.4428 - val_loss: 7811.7310\n",
      "Epoch 4073/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8757.9791 - val_loss: 7811.2690\n",
      "Epoch 4074/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8757.5166 - val_loss: 7810.8066\n",
      "Epoch 4075/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8757.0537 - val_loss: 7810.3418\n",
      "Epoch 4076/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8756.5908 - val_loss: 7809.8804\n",
      "Epoch 4077/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8756.1291 - val_loss: 7809.4180\n",
      "Epoch 4078/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8755.6656 - val_loss: 7808.9556\n",
      "Epoch 4079/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8755.2029 - val_loss: 7808.4917\n",
      "Epoch 4080/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8754.7403 - val_loss: 7808.0293\n",
      "Epoch 4081/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8754.2774 - val_loss: 7807.5664\n",
      "Epoch 4082/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8753.8154 - val_loss: 7807.1045\n",
      "Epoch 4083/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8753.3513 - val_loss: 7806.6431\n",
      "Epoch 4084/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8752.8891 - val_loss: 7806.1792\n",
      "Epoch 4085/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8752.4272 - val_loss: 7805.7153\n",
      "Epoch 4086/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8751.9637 - val_loss: 7805.2529\n",
      "Epoch 4087/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8751.5018 - val_loss: 7804.7915\n",
      "Epoch 4088/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8751.0383 - val_loss: 7804.3281\n",
      "Epoch 4089/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8750.5762 - val_loss: 7803.8652\n",
      "Epoch 4090/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8750.1137 - val_loss: 7803.4023\n",
      "Epoch 4091/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8749.6506 - val_loss: 7802.9404\n",
      "Epoch 4092/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 8749.1885 - val_loss: 7802.4775\n",
      "Epoch 4093/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 8748.7249 - val_loss: 7802.0137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4094/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 8748.2624 - val_loss: 7801.5518\n",
      "Epoch 4095/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8747.7999 - val_loss: 7801.0889\n",
      "Epoch 4096/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8747.3363 - val_loss: 7800.6274\n",
      "Epoch 4097/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8746.8746 - val_loss: 7800.1641\n",
      "Epoch 4098/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8746.4119 - val_loss: 7799.7002\n",
      "Epoch 4099/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8745.9490 - val_loss: 7799.2378\n",
      "Epoch 4100/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8745.4866 - val_loss: 7798.7764\n",
      "Epoch 4101/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8745.0229 - val_loss: 7798.3135\n",
      "Epoch 4102/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 8744.5608 - val_loss: 7797.8501\n",
      "Epoch 4103/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 8744.0989 - val_loss: 7797.3862\n",
      "Epoch 4104/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8743.6353 - val_loss: 7796.9248\n",
      "Epoch 4105/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8743.1728 - val_loss: 7796.4629\n",
      "Epoch 4106/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8742.7099 - val_loss: 7796.0000\n",
      "Epoch 4107/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8742.2477 - val_loss: 7795.5361\n",
      "Epoch 4108/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8741.7855 - val_loss: 7795.0737\n",
      "Epoch 4109/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8741.3217 - val_loss: 7794.6113\n",
      "Epoch 4110/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8740.8590 - val_loss: 7794.1484\n",
      "Epoch 4111/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8740.3966 - val_loss: 7793.6860\n",
      "Epoch 4112/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8739.9335 - val_loss: 7793.2222\n",
      "Epoch 4113/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8739.4714 - val_loss: 7792.7607\n",
      "Epoch 4114/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8739.0081 - val_loss: 7792.2974\n",
      "Epoch 4115/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8738.5455 - val_loss: 7791.8359\n",
      "Epoch 4116/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8738.0835 - val_loss: 7791.3726\n",
      "Epoch 4117/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8737.6200 - val_loss: 7790.9097\n",
      "Epoch 4118/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8737.1578 - val_loss: 7790.4473\n",
      "Epoch 4119/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8736.6945 - val_loss: 7789.9844\n",
      "Epoch 4120/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8736.2324 - val_loss: 7789.5220\n",
      "Epoch 4121/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8735.7698 - val_loss: 7789.0586\n",
      "Epoch 4122/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8735.3061 - val_loss: 7788.5962\n",
      "Epoch 4123/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8734.8441 - val_loss: 7788.1348\n",
      "Epoch 4124/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8734.3812 - val_loss: 7787.6704\n",
      "Epoch 4125/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8733.9188 - val_loss: 7787.2080\n",
      "Epoch 4126/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8733.4563 - val_loss: 7786.7446\n",
      "Epoch 4127/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8732.9925 - val_loss: 7786.2832\n",
      "Epoch 4128/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8732.5310 - val_loss: 7785.8198\n",
      "Epoch 4129/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 8732.0683 - val_loss: 7785.3579\n",
      "Epoch 4130/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 8731.6050 - val_loss: 7784.8945\n",
      "Epoch 4131/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 8731.1428 - val_loss: 7784.4316\n",
      "Epoch 4132/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8730.6790 - val_loss: 7783.9683\n",
      "Epoch 4133/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8730.2170 - val_loss: 7783.5073\n",
      "Epoch 4134/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8729.7553 - val_loss: 7783.0435\n",
      "Epoch 4135/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8729.2908 - val_loss: 7782.5815\n",
      "Epoch 4136/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8728.8291 - val_loss: 7782.1191\n",
      "Epoch 4137/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8728.3659 - val_loss: 7781.6558\n",
      "Epoch 4138/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8727.9036 - val_loss: 7781.1929\n",
      "Epoch 4139/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8727.4413 - val_loss: 7780.7305\n",
      "Epoch 4140/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8726.9779 - val_loss: 7780.2690\n",
      "Epoch 4141/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8726.5153 - val_loss: 7779.8042\n",
      "Epoch 4142/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8726.0527 - val_loss: 7779.3418\n",
      "Epoch 4143/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8725.5899 - val_loss: 7778.8789\n",
      "Epoch 4144/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8725.1274 - val_loss: 7778.4170\n",
      "Epoch 4145/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8724.6644 - val_loss: 7777.9556\n",
      "Epoch 4146/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8724.2020 - val_loss: 7777.4917\n",
      "Epoch 4147/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8723.7397 - val_loss: 7777.0288\n",
      "Epoch 4148/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8723.2760 - val_loss: 7776.5664\n",
      "Epoch 4149/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8722.8146 - val_loss: 7776.1040\n",
      "Epoch 4150/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 8722.3507 - val_loss: 7775.6401\n",
      "Epoch 4151/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8721.8886 - val_loss: 7775.1777\n",
      "Epoch 4152/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8721.4264 - val_loss: 7774.7148\n",
      "Epoch 4153/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8720.9627 - val_loss: 7774.2529\n",
      "Epoch 4154/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8720.5002 - val_loss: 7773.7900\n",
      "Epoch 4155/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8720.0377 - val_loss: 7773.3262\n",
      "Epoch 4156/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8719.5750 - val_loss: 7772.8638\n",
      "Epoch 4157/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8719.1125 - val_loss: 7772.4023\n",
      "Epoch 4158/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8718.6496 - val_loss: 7771.9399\n",
      "Epoch 4159/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8718.1868 - val_loss: 7771.4766\n",
      "Epoch 4160/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8717.7247 - val_loss: 7771.0127\n",
      "Epoch 4161/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8717.2614 - val_loss: 7770.5503\n",
      "Epoch 4162/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8716.7990 - val_loss: 7770.0889\n",
      "Epoch 4163/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 8716.3353 - val_loss: 7769.6260\n",
      "Epoch 4164/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 8715.8734 - val_loss: 7769.1641\n",
      "Epoch 4165/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8715.4114 - val_loss: 7768.6987\n",
      "Epoch 4166/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8714.9478 - val_loss: 7768.2373\n",
      "Epoch 4167/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8714.4854 - val_loss: 7767.7754\n",
      "Epoch 4168/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8714.0224 - val_loss: 7767.3110\n",
      "Epoch 4169/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8713.5593 - val_loss: 7766.8496\n",
      "Epoch 4170/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8713.0971 - val_loss: 7766.3862\n",
      "Epoch 4171/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8712.6344 - val_loss: 7765.9238\n",
      "Epoch 4172/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8712.1716 - val_loss: 7765.4609\n",
      "Epoch 4173/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8711.7090 - val_loss: 7764.9985\n",
      "Epoch 4174/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8711.2461 - val_loss: 7764.5352\n",
      "Epoch 4175/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8710.7842 - val_loss: 7764.0732\n",
      "Epoch 4176/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8710.3201 - val_loss: 7763.6113\n",
      "Epoch 4177/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8709.8580 - val_loss: 7763.1484\n",
      "Epoch 4178/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8709.3962 - val_loss: 7762.6846\n",
      "Epoch 4179/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8708.9322 - val_loss: 7762.2222\n",
      "Epoch 4180/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8708.4700 - val_loss: 7761.7598\n",
      "Epoch 4181/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8708.0071 - val_loss: 7761.2969\n",
      "Epoch 4182/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8707.5447 - val_loss: 7760.8345\n",
      "Epoch 4183/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8707.0823 - val_loss: 7760.3711\n",
      "Epoch 4184/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8706.6195 - val_loss: 7759.9087\n",
      "Epoch 4185/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8706.1563 - val_loss: 7759.4458\n",
      "Epoch 4186/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8705.6938 - val_loss: 7758.9829\n",
      "Epoch 4187/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8705.2307 - val_loss: 7758.5205\n",
      "Epoch 4188/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8704.7687 - val_loss: 7758.0571\n",
      "Epoch 4189/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8704.3054 - val_loss: 7757.5957\n",
      "Epoch 4190/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8703.8428 - val_loss: 7757.1323\n",
      "Epoch 4191/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8703.3808 - val_loss: 7756.6704\n",
      "Epoch 4192/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8702.9177 - val_loss: 7756.2070\n",
      "Epoch 4193/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8702.4549 - val_loss: 7755.7441\n",
      "Epoch 4194/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8701.9917 - val_loss: 7755.2808\n",
      "Epoch 4195/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8701.5298 - val_loss: 7754.8184\n",
      "Epoch 4196/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 8701.0670 - val_loss: 7754.3555\n",
      "Epoch 4197/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8700.6041 - val_loss: 7753.8940\n",
      "Epoch 4198/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8700.1416 - val_loss: 7753.4316\n",
      "Epoch 4199/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8699.6785 - val_loss: 7752.9668\n",
      "Epoch 4200/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8699.2157 - val_loss: 7752.5054\n",
      "Epoch 4201/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8698.7543 - val_loss: 7752.0420\n",
      "Epoch 4202/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8698.2900 - val_loss: 7751.5806\n",
      "Epoch 4203/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8697.8278 - val_loss: 7751.1177\n",
      "Epoch 4204/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8697.3652 - val_loss: 7750.6543\n",
      "Epoch 4205/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8696.9020 - val_loss: 7750.1914\n",
      "Epoch 4206/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 8696.4400 - val_loss: 7749.7295\n",
      "Epoch 4207/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 8695.9768 - val_loss: 7749.2666\n",
      "Epoch 4208/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8695.5141 - val_loss: 7748.8042\n",
      "Epoch 4209/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 8695.0523 - val_loss: 7748.3403\n",
      "Epoch 4210/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8694.5891 - val_loss: 7747.8789\n",
      "Epoch 4211/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8694.1265 - val_loss: 7747.4165\n",
      "Epoch 4212/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8693.6632 - val_loss: 7746.9531\n",
      "Epoch 4213/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8693.2012 - val_loss: 7746.4902\n",
      "Epoch 4214/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8692.7386 - val_loss: 7746.0273\n",
      "Epoch 4215/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8692.2749 - val_loss: 7745.5654\n",
      "Epoch 4216/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8691.8133 - val_loss: 7745.1025\n",
      "Epoch 4217/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8691.3500 - val_loss: 7744.6387\n",
      "Epoch 4218/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8690.8876 - val_loss: 7744.1753\n",
      "Epoch 4219/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8690.4250 - val_loss: 7743.7139\n",
      "Epoch 4220/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8689.9612 - val_loss: 7743.2524\n",
      "Epoch 4221/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8689.4989 - val_loss: 7742.7891\n",
      "Epoch 4222/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8689.0367 - val_loss: 7742.3262\n",
      "Epoch 4223/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8688.5739 - val_loss: 7741.8628\n",
      "Epoch 4224/10000\n",
      "750/750 [==============================] - 0s 155us/step - loss: 8688.1115 - val_loss: 7741.4004\n",
      "Epoch 4225/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8687.6478 - val_loss: 7740.9375\n",
      "Epoch 4226/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8687.1855 - val_loss: 7740.4751\n",
      "Epoch 4227/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8686.7240 - val_loss: 7740.0122\n",
      "Epoch 4228/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8686.2601 - val_loss: 7739.5498\n",
      "Epoch 4229/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8685.7978 - val_loss: 7739.0879\n",
      "Epoch 4230/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8685.3349 - val_loss: 7738.6235\n",
      "Epoch 4231/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8684.8720 - val_loss: 7738.1611\n",
      "Epoch 4232/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8684.4099 - val_loss: 7737.6987\n",
      "Epoch 4233/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8683.9466 - val_loss: 7737.2363\n",
      "Epoch 4234/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8683.4842 - val_loss: 7736.7734\n",
      "Epoch 4235/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8683.0216 - val_loss: 7736.3110\n",
      "Epoch 4236/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8682.5583 - val_loss: 7735.8472\n",
      "Epoch 4237/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 125us/step - loss: 8682.0963 - val_loss: 7735.3857\n",
      "Epoch 4238/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8681.6330 - val_loss: 7734.9238\n",
      "Epoch 4239/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8681.1704 - val_loss: 7734.4609\n",
      "Epoch 4240/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8680.7085 - val_loss: 7733.9971\n",
      "Epoch 4241/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8680.2451 - val_loss: 7733.5347\n",
      "Epoch 4242/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8679.7831 - val_loss: 7733.0723\n",
      "Epoch 4243/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8679.3194 - val_loss: 7732.6094\n",
      "Epoch 4244/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8678.8575 - val_loss: 7732.1470\n",
      "Epoch 4245/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8678.3946 - val_loss: 7731.6836\n",
      "Epoch 4246/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8677.9313 - val_loss: 7731.2212\n",
      "Epoch 4247/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8677.4694 - val_loss: 7730.7573\n",
      "Epoch 4248/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8677.0062 - val_loss: 7730.2954\n",
      "Epoch 4249/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8676.5437 - val_loss: 7729.8335\n",
      "Epoch 4250/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8676.0814 - val_loss: 7729.3706\n",
      "Epoch 4251/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8675.6176 - val_loss: 7728.9082\n",
      "Epoch 4252/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8675.1556 - val_loss: 7728.4448\n",
      "Epoch 4253/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8674.6931 - val_loss: 7727.9819\n",
      "Epoch 4254/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8674.2294 - val_loss: 7727.5195\n",
      "Epoch 4255/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8673.7678 - val_loss: 7727.0566\n",
      "Epoch 4256/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8673.3040 - val_loss: 7726.5942\n",
      "Epoch 4257/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8672.8419 - val_loss: 7726.1309\n",
      "Epoch 4258/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8672.3802 - val_loss: 7725.6680\n",
      "Epoch 4259/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8671.9167 - val_loss: 7725.2065\n",
      "Epoch 4260/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8671.4541 - val_loss: 7724.7441\n",
      "Epoch 4261/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8670.9910 - val_loss: 7724.2808\n",
      "Epoch 4262/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8670.5289 - val_loss: 7723.8169\n",
      "Epoch 4263/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8670.0657 - val_loss: 7723.3555\n",
      "Epoch 4264/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8669.6031 - val_loss: 7722.8931\n",
      "Epoch 4265/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8669.1403 - val_loss: 7722.4292\n",
      "Epoch 4266/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8668.6778 - val_loss: 7721.9668\n",
      "Epoch 4267/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8668.2147 - val_loss: 7721.5039\n",
      "Epoch 4268/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8667.7527 - val_loss: 7721.0420\n",
      "Epoch 4269/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8667.2891 - val_loss: 7720.5806\n",
      "Epoch 4270/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 8666.8269 - val_loss: 7720.1167\n",
      "Epoch 4271/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 8666.3646 - val_loss: 7719.6528\n",
      "Epoch 4272/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 8665.9009 - val_loss: 7719.1904\n",
      "Epoch 4273/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 8665.4387 - val_loss: 7718.7290\n",
      "Epoch 4274/10000\n",
      "750/750 [==============================] - 0s 90us/step - loss: 8664.9756 - val_loss: 7718.2651\n",
      "Epoch 4275/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 8664.5137 - val_loss: 7717.8027\n",
      "Epoch 4276/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 8664.0512 - val_loss: 7717.3398\n",
      "Epoch 4277/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 8663.5874 - val_loss: 7716.8779\n",
      "Epoch 4278/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 8663.1255 - val_loss: 7716.4165\n",
      "Epoch 4279/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 8662.6625 - val_loss: 7715.9512\n",
      "Epoch 4280/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8662.2000 - val_loss: 7715.4888\n",
      "Epoch 4281/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8661.7376 - val_loss: 7715.0264\n",
      "Epoch 4282/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8661.2738 - val_loss: 7714.5649\n",
      "Epoch 4283/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8660.8120 - val_loss: 7714.1016\n",
      "Epoch 4284/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8660.3494 - val_loss: 7713.6387\n",
      "Epoch 4285/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8659.8862 - val_loss: 7713.1753\n",
      "Epoch 4286/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8659.4241 - val_loss: 7712.7129\n",
      "Epoch 4287/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8658.9604 - val_loss: 7712.2500\n",
      "Epoch 4288/10000\n",
      "750/750 [==============================] - 0s 166us/step - loss: 8658.4983 - val_loss: 7711.7876\n",
      "Epoch 4289/10000\n",
      "750/750 [==============================] - 0s 164us/step - loss: 8658.0357 - val_loss: 7711.3247\n",
      "Epoch 4290/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8657.5728 - val_loss: 7710.8623\n",
      "Epoch 4291/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 8657.1102 - val_loss: 7710.4004\n",
      "Epoch 4292/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8656.6472 - val_loss: 7709.9360\n",
      "Epoch 4293/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8656.1845 - val_loss: 7709.4727\n",
      "Epoch 4294/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 8655.7228 - val_loss: 7709.0112\n",
      "Epoch 4295/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 8655.2592 - val_loss: 7708.5488\n",
      "Epoch 4296/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8654.7968 - val_loss: 7708.0859\n",
      "Epoch 4297/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8654.3342 - val_loss: 7707.6235\n",
      "Epoch 4298/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8653.8708 - val_loss: 7707.1597\n",
      "Epoch 4299/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8653.4091 - val_loss: 7706.6982\n",
      "Epoch 4300/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8652.9453 - val_loss: 7706.2349\n",
      "Epoch 4301/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8652.4830 - val_loss: 7705.7734\n",
      "Epoch 4302/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8652.0210 - val_loss: 7705.3101\n",
      "Epoch 4303/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8651.5573 - val_loss: 7704.8472\n",
      "Epoch 4304/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8651.0956 - val_loss: 7704.3848\n",
      "Epoch 4305/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8650.6317 - val_loss: 7703.9204\n",
      "Epoch 4306/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8650.1693 - val_loss: 7703.4595\n",
      "Epoch 4307/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8649.7073 - val_loss: 7702.9961\n",
      "Epoch 4308/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8649.2437 - val_loss: 7702.5337\n",
      "Epoch 4309/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 130us/step - loss: 8648.7809 - val_loss: 7702.0708\n",
      "Epoch 4310/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8648.3187 - val_loss: 7701.6079\n",
      "Epoch 4311/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8647.8560 - val_loss: 7701.1455\n",
      "Epoch 4312/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8647.3938 - val_loss: 7700.6836\n",
      "Epoch 4313/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8646.9306 - val_loss: 7700.2207\n",
      "Epoch 4314/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8646.4679 - val_loss: 7699.7573\n",
      "Epoch 4315/10000\n",
      "750/750 [==============================] - 0s 214us/step - loss: 8646.0056 - val_loss: 7699.2944\n",
      "Epoch 4316/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8645.5426 - val_loss: 7698.8320\n",
      "Epoch 4317/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 8645.0802 - val_loss: 7698.3696\n",
      "Epoch 4318/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8644.6166 - val_loss: 7697.9058\n",
      "Epoch 4319/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8644.1544 - val_loss: 7697.4448\n",
      "Epoch 4320/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8643.6926 - val_loss: 7696.9805\n",
      "Epoch 4321/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8643.2287 - val_loss: 7696.5190\n",
      "Epoch 4322/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 8642.7665 - val_loss: 7696.0552\n",
      "Epoch 4323/10000\n",
      "750/750 [==============================] - 0s 159us/step - loss: 8642.3033 - val_loss: 7695.5918\n",
      "Epoch 4324/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8641.8408 - val_loss: 7695.1309\n",
      "Epoch 4325/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8641.3785 - val_loss: 7694.6680\n",
      "Epoch 4326/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8640.9155 - val_loss: 7694.2056\n",
      "Epoch 4327/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8640.4529 - val_loss: 7693.7417\n",
      "Epoch 4328/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8639.9902 - val_loss: 7693.2793\n",
      "Epoch 4329/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8639.5277 - val_loss: 7692.8164\n",
      "Epoch 4330/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8639.0648 - val_loss: 7692.3545\n",
      "Epoch 4331/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8638.6017 - val_loss: 7691.8931\n",
      "Epoch 4332/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8638.1393 - val_loss: 7691.4292\n",
      "Epoch 4333/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8637.6773 - val_loss: 7690.9663\n",
      "Epoch 4334/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8637.2137 - val_loss: 7690.5029\n",
      "Epoch 4335/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8636.7517 - val_loss: 7690.0415\n",
      "Epoch 4336/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8636.2884 - val_loss: 7689.5781\n",
      "Epoch 4337/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8635.8262 - val_loss: 7689.1152\n",
      "Epoch 4338/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8635.3637 - val_loss: 7688.6523\n",
      "Epoch 4339/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8634.8999 - val_loss: 7688.1899\n",
      "Epoch 4340/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8634.4374 - val_loss: 7687.7275\n",
      "Epoch 4341/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8633.9751 - val_loss: 7687.2637\n",
      "Epoch 4342/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 8633.5120 - val_loss: 7686.8013\n",
      "Epoch 4343/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8633.0500 - val_loss: 7686.3389\n",
      "Epoch 4344/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8632.5865 - val_loss: 7685.8774\n",
      "Epoch 4345/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8632.1240 - val_loss: 7685.4141\n",
      "Epoch 4346/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8631.6619 - val_loss: 7684.9502\n",
      "Epoch 4347/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8631.1989 - val_loss: 7684.4878\n",
      "Epoch 4348/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8630.7365 - val_loss: 7684.0264\n",
      "Epoch 4349/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8630.2727 - val_loss: 7683.5625\n",
      "Epoch 4350/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8629.8106 - val_loss: 7683.1001\n",
      "Epoch 4351/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8629.3485 - val_loss: 7682.6362\n",
      "Epoch 4352/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8628.8853 - val_loss: 7682.1748\n",
      "Epoch 4353/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8628.4228 - val_loss: 7681.7129\n",
      "Epoch 4354/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8627.9596 - val_loss: 7681.2485\n",
      "Epoch 4355/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 8627.4968 - val_loss: 7680.7876\n",
      "Epoch 4356/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8627.0350 - val_loss: 7680.3237\n",
      "Epoch 4357/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8626.5711 - val_loss: 7679.8613\n",
      "Epoch 4358/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8626.1091 - val_loss: 7679.3984\n",
      "Epoch 4359/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8625.6463 - val_loss: 7678.9360\n",
      "Epoch 4360/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8625.1834 - val_loss: 7678.4722\n",
      "Epoch 4361/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8624.7211 - val_loss: 7678.0107\n",
      "Epoch 4362/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8624.2577 - val_loss: 7677.5488\n",
      "Epoch 4363/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8623.7954 - val_loss: 7677.0845\n",
      "Epoch 4364/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8623.3334 - val_loss: 7676.6221\n",
      "Epoch 4365/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8622.8699 - val_loss: 7676.1597\n",
      "Epoch 4366/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8622.4075 - val_loss: 7675.6973\n",
      "Epoch 4367/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8621.9442 - val_loss: 7675.2344\n",
      "Epoch 4368/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8621.4824 - val_loss: 7674.7710\n",
      "Epoch 4369/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8621.0198 - val_loss: 7674.3086\n",
      "Epoch 4370/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8620.5563 - val_loss: 7673.8462\n",
      "Epoch 4371/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8620.0943 - val_loss: 7673.3848\n",
      "Epoch 4372/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8619.6313 - val_loss: 7672.9204\n",
      "Epoch 4373/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8619.1685 - val_loss: 7672.4570\n",
      "Epoch 4374/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8618.7061 - val_loss: 7671.9956\n",
      "Epoch 4375/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8618.2425 - val_loss: 7671.5332\n",
      "Epoch 4376/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8617.7802 - val_loss: 7671.0698\n",
      "Epoch 4377/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8617.3181 - val_loss: 7670.6079\n",
      "Epoch 4378/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8616.8550 - val_loss: 7670.1445\n",
      "Epoch 4379/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8616.3928 - val_loss: 7669.6816\n",
      "Epoch 4380/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8615.9291 - val_loss: 7669.2183\n",
      "Epoch 4381/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 125us/step - loss: 8615.4667 - val_loss: 7668.7559\n",
      "Epoch 4382/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8615.0046 - val_loss: 7668.2935\n",
      "Epoch 4383/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8614.5411 - val_loss: 7667.8315\n",
      "Epoch 4384/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8614.0791 - val_loss: 7667.3691\n",
      "Epoch 4385/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8613.6159 - val_loss: 7666.9043\n",
      "Epoch 4386/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8613.1532 - val_loss: 7666.4419\n",
      "Epoch 4387/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8612.6914 - val_loss: 7665.9805\n",
      "Epoch 4388/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8612.2280 - val_loss: 7665.5181\n",
      "Epoch 4389/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8611.7653 - val_loss: 7665.0542\n",
      "Epoch 4390/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8611.3027 - val_loss: 7664.5918\n",
      "Epoch 4391/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8610.8395 - val_loss: 7664.1289\n",
      "Epoch 4392/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8610.3773 - val_loss: 7663.6670\n",
      "Epoch 4393/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 8609.9137 - val_loss: 7663.2056\n",
      "Epoch 4394/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 8609.4517 - val_loss: 7662.7417\n",
      "Epoch 4395/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8608.9897 - val_loss: 7662.2778\n",
      "Epoch 4396/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8608.5262 - val_loss: 7661.8164\n",
      "Epoch 4397/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8608.0643 - val_loss: 7661.3540\n",
      "Epoch 4398/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8607.6005 - val_loss: 7660.8901\n",
      "Epoch 4399/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8607.1380 - val_loss: 7660.4277\n",
      "Epoch 4400/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8606.6763 - val_loss: 7659.9648\n",
      "Epoch 4401/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8606.2125 - val_loss: 7659.5029\n",
      "Epoch 4402/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8605.7505 - val_loss: 7659.0391\n",
      "Epoch 4403/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8605.2876 - val_loss: 7658.5762\n",
      "Epoch 4404/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8604.8249 - val_loss: 7658.1138\n",
      "Epoch 4405/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8604.3622 - val_loss: 7657.6514\n",
      "Epoch 4406/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8603.8989 - val_loss: 7657.1899\n",
      "Epoch 4407/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8603.4369 - val_loss: 7656.7266\n",
      "Epoch 4408/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8602.9743 - val_loss: 7656.2637\n",
      "Epoch 4409/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8602.5108 - val_loss: 7655.8003\n",
      "Epoch 4410/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 8602.0489 - val_loss: 7655.3379\n",
      "Epoch 4411/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8601.5853 - val_loss: 7654.8774\n",
      "Epoch 4412/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8601.1233 - val_loss: 7654.4126\n",
      "Epoch 4413/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8600.6608 - val_loss: 7653.9487\n",
      "Epoch 4414/10000\n",
      "750/750 [==============================] - 0s 159us/step - loss: 8600.1972 - val_loss: 7653.4873\n",
      "Epoch 4415/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8599.7354 - val_loss: 7653.0254\n",
      "Epoch 4416/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8599.2722 - val_loss: 7652.5625\n",
      "Epoch 4417/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 8598.8100 - val_loss: 7652.0996\n",
      "Epoch 4418/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8598.3471 - val_loss: 7651.6362\n",
      "Epoch 4419/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8597.8842 - val_loss: 7651.1738\n",
      "Epoch 4420/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8597.4215 - val_loss: 7650.7109\n",
      "Epoch 4421/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8596.9590 - val_loss: 7650.2485\n",
      "Epoch 4422/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8596.4959 - val_loss: 7649.7847\n",
      "Epoch 4423/10000\n",
      "750/750 [==============================] - 0s 159us/step - loss: 8596.0342 - val_loss: 7649.3232\n",
      "Epoch 4424/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 8595.5700 - val_loss: 7648.8613\n",
      "Epoch 4425/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8595.1081 - val_loss: 7648.3984\n",
      "Epoch 4426/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 8594.6460 - val_loss: 7647.9336\n",
      "Epoch 4427/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8594.1824 - val_loss: 7647.4712\n",
      "Epoch 4428/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 8593.7200 - val_loss: 7647.0098\n",
      "Epoch 4429/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8593.2568 - val_loss: 7646.5469\n",
      "Epoch 4430/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8592.7948 - val_loss: 7646.0845\n",
      "Epoch 4431/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8592.3324 - val_loss: 7645.6211\n",
      "Epoch 4432/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8591.8689 - val_loss: 7645.1582\n",
      "Epoch 4433/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8591.4061 - val_loss: 7644.6973\n",
      "Epoch 4434/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8590.9439 - val_loss: 7644.2329\n",
      "Epoch 4435/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8590.4812 - val_loss: 7643.7705\n",
      "Epoch 4436/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8590.0187 - val_loss: 7643.3071\n",
      "Epoch 4437/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8589.5550 - val_loss: 7642.8457\n",
      "Epoch 4438/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8589.0931 - val_loss: 7642.3823\n",
      "Epoch 4439/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8588.6305 - val_loss: 7641.9194\n",
      "Epoch 4440/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8588.1674 - val_loss: 7641.4570\n",
      "Epoch 4441/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8587.7054 - val_loss: 7640.9941\n",
      "Epoch 4442/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8587.2415 - val_loss: 7640.5308\n",
      "Epoch 4443/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8586.7795 - val_loss: 7640.0684\n",
      "Epoch 4444/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8586.3170 - val_loss: 7639.6055\n",
      "Epoch 4445/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8585.8532 - val_loss: 7639.1440\n",
      "Epoch 4446/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8585.3915 - val_loss: 7638.6816\n",
      "Epoch 4447/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8584.9283 - val_loss: 7638.2183\n",
      "Epoch 4448/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8584.4657 - val_loss: 7637.7544\n",
      "Epoch 4449/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8584.0041 - val_loss: 7637.2930\n",
      "Epoch 4450/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8583.5396 - val_loss: 7636.8306\n",
      "Epoch 4451/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8583.0777 - val_loss: 7636.3667\n",
      "Epoch 4452/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8582.6154 - val_loss: 7635.9043\n",
      "Epoch 4453/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 128us/step - loss: 8582.1520 - val_loss: 7635.4414\n",
      "Epoch 4454/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8581.6900 - val_loss: 7634.9795\n",
      "Epoch 4455/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8581.2267 - val_loss: 7634.5151\n",
      "Epoch 4456/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8580.7642 - val_loss: 7634.0542\n",
      "Epoch 4457/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8580.3023 - val_loss: 7633.5913\n",
      "Epoch 4458/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8579.8385 - val_loss: 7633.1279\n",
      "Epoch 4459/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8579.3760 - val_loss: 7632.6665\n",
      "Epoch 4460/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8578.9132 - val_loss: 7632.2012\n",
      "Epoch 4461/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8578.4503 - val_loss: 7631.7402\n",
      "Epoch 4462/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8577.9887 - val_loss: 7631.2773\n",
      "Epoch 4463/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8577.5251 - val_loss: 7630.8154\n",
      "Epoch 4464/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8577.0623 - val_loss: 7630.3540\n",
      "Epoch 4465/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8576.5999 - val_loss: 7629.8887\n",
      "Epoch 4466/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8576.1372 - val_loss: 7629.4263\n",
      "Epoch 4467/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8575.6743 - val_loss: 7628.9639\n",
      "Epoch 4468/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8575.2113 - val_loss: 7628.5024\n",
      "Epoch 4469/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8574.7490 - val_loss: 7628.0391\n",
      "Epoch 4470/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8574.2868 - val_loss: 7627.5752\n",
      "Epoch 4471/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8573.8237 - val_loss: 7627.1128\n",
      "Epoch 4472/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8573.3614 - val_loss: 7626.6504\n",
      "Epoch 4473/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8572.8978 - val_loss: 7626.1875\n",
      "Epoch 4474/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8572.4358 - val_loss: 7625.7251\n",
      "Epoch 4475/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8571.9738 - val_loss: 7625.2612\n",
      "Epoch 4476/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8571.5095 - val_loss: 7624.7998\n",
      "Epoch 4477/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8571.0479 - val_loss: 7624.3379\n",
      "Epoch 4478/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8570.5846 - val_loss: 7623.8735\n",
      "Epoch 4479/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8570.1220 - val_loss: 7623.4111\n",
      "Epoch 4480/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8569.6595 - val_loss: 7622.9487\n",
      "Epoch 4481/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8569.1969 - val_loss: 7622.4863\n",
      "Epoch 4482/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8568.7338 - val_loss: 7622.0234\n",
      "Epoch 4483/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8568.2716 - val_loss: 7621.5610\n",
      "Epoch 4484/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8567.8090 - val_loss: 7621.0972\n",
      "Epoch 4485/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8567.3459 - val_loss: 7620.6357\n",
      "Epoch 4486/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8566.8827 - val_loss: 7620.1738\n",
      "Epoch 4487/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8566.4203 - val_loss: 7619.7109\n",
      "Epoch 4488/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8565.9585 - val_loss: 7619.2471\n",
      "Epoch 4489/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8565.4947 - val_loss: 7618.7837\n",
      "Epoch 4490/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8565.0330 - val_loss: 7618.3223\n",
      "Epoch 4491/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8564.5693 - val_loss: 7617.8594\n",
      "Epoch 4492/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8564.1075 - val_loss: 7617.3970\n",
      "Epoch 4493/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8563.6446 - val_loss: 7616.9336\n",
      "Epoch 4494/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8563.1812 - val_loss: 7616.4707\n",
      "Epoch 4495/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8562.7187 - val_loss: 7616.0083\n",
      "Epoch 4496/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8562.2560 - val_loss: 7615.5454\n",
      "Epoch 4497/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8561.7932 - val_loss: 7615.0830\n",
      "Epoch 4498/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8561.3313 - val_loss: 7614.6206\n",
      "Epoch 4499/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8560.8674 - val_loss: 7614.1582\n",
      "Epoch 4500/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8560.4051 - val_loss: 7613.6948\n",
      "Epoch 4501/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8559.9432 - val_loss: 7613.2310\n",
      "Epoch 4502/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8559.4797 - val_loss: 7612.7695\n",
      "Epoch 4503/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8559.0174 - val_loss: 7612.3066\n",
      "Epoch 4504/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8558.5540 - val_loss: 7611.8433\n",
      "Epoch 4505/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8558.0919 - val_loss: 7611.3823\n",
      "Epoch 4506/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8557.6302 - val_loss: 7610.9180\n",
      "Epoch 4507/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8557.1665 - val_loss: 7610.4565\n",
      "Epoch 4508/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8556.7039 - val_loss: 7609.9941\n",
      "Epoch 4509/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8556.2411 - val_loss: 7609.5293\n",
      "Epoch 4510/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8555.7780 - val_loss: 7609.0679\n",
      "Epoch 4511/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8555.3157 - val_loss: 7608.6045\n",
      "Epoch 4512/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8554.8523 - val_loss: 7608.1431\n",
      "Epoch 4513/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8554.3902 - val_loss: 7607.6792\n",
      "Epoch 4514/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8553.9277 - val_loss: 7607.2168\n",
      "Epoch 4515/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8553.4649 - val_loss: 7606.7539\n",
      "Epoch 4516/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8553.0028 - val_loss: 7606.2920\n",
      "Epoch 4517/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8552.5387 - val_loss: 7605.8306\n",
      "Epoch 4518/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8552.0768 - val_loss: 7605.3652\n",
      "Epoch 4519/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8551.6146 - val_loss: 7604.9028\n",
      "Epoch 4520/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8551.1510 - val_loss: 7604.4414\n",
      "Epoch 4521/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8550.6891 - val_loss: 7603.9790\n",
      "Epoch 4522/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8550.2258 - val_loss: 7603.5151\n",
      "Epoch 4523/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8549.7637 - val_loss: 7603.0527\n",
      "Epoch 4524/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8549.3012 - val_loss: 7602.5898\n",
      "Epoch 4525/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 136us/step - loss: 8548.8375 - val_loss: 7602.1279\n",
      "Epoch 4526/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8548.3748 - val_loss: 7601.6650\n",
      "Epoch 4527/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8547.9125 - val_loss: 7601.2012\n",
      "Epoch 4528/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8547.4490 - val_loss: 7600.7388\n",
      "Epoch 4529/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8546.9875 - val_loss: 7600.2764\n",
      "Epoch 4530/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8546.5240 - val_loss: 7599.8135\n",
      "Epoch 4531/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8546.0611 - val_loss: 7599.3516\n",
      "Epoch 4532/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8545.5992 - val_loss: 7598.8887\n",
      "Epoch 4533/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8545.1364 - val_loss: 7598.4253\n",
      "Epoch 4534/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8544.6738 - val_loss: 7597.9639\n",
      "Epoch 4535/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8544.2106 - val_loss: 7597.5000\n",
      "Epoch 4536/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 8543.7478 - val_loss: 7597.0376\n",
      "Epoch 4537/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8543.2857 - val_loss: 7596.5747\n",
      "Epoch 4538/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8542.8226 - val_loss: 7596.1123\n",
      "Epoch 4539/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8542.3602 - val_loss: 7595.6504\n",
      "Epoch 4540/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8541.8974 - val_loss: 7595.1860\n",
      "Epoch 4541/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8541.4345 - val_loss: 7594.7246\n",
      "Epoch 4542/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8540.9727 - val_loss: 7594.2607\n",
      "Epoch 4543/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8540.5086 - val_loss: 7593.7988\n",
      "Epoch 4544/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8540.0466 - val_loss: 7593.3359\n",
      "Epoch 4545/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8539.5839 - val_loss: 7592.8735\n",
      "Epoch 4546/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8539.1208 - val_loss: 7592.4097\n",
      "Epoch 4547/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8538.6587 - val_loss: 7591.9473\n",
      "Epoch 4548/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8538.1950 - val_loss: 7591.4849\n",
      "Epoch 4549/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8537.7329 - val_loss: 7591.0234\n",
      "Epoch 4550/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8537.2709 - val_loss: 7590.5596\n",
      "Epoch 4551/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8536.8074 - val_loss: 7590.0972\n",
      "Epoch 4552/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8536.3451 - val_loss: 7589.6348\n",
      "Epoch 4553/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8535.8819 - val_loss: 7589.1704\n",
      "Epoch 4554/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8535.4193 - val_loss: 7588.7095\n",
      "Epoch 4555/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8534.9567 - val_loss: 7588.2461\n",
      "Epoch 4556/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8534.4939 - val_loss: 7587.7837\n",
      "Epoch 4557/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8534.0318 - val_loss: 7587.3198\n",
      "Epoch 4558/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8533.5689 - val_loss: 7586.8579\n",
      "Epoch 4559/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8533.1060 - val_loss: 7586.3955\n",
      "Epoch 4560/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8532.6435 - val_loss: 7585.9321\n",
      "Epoch 4561/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8532.1802 - val_loss: 7585.4707\n",
      "Epoch 4562/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8531.7177 - val_loss: 7585.0073\n",
      "Epoch 4563/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 8536.75 - 0s 121us/step - loss: 8531.2554 - val_loss: 7584.5444\n",
      "Epoch 4564/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8530.7922 - val_loss: 7584.0815\n",
      "Epoch 4565/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8530.3300 - val_loss: 7583.6191\n",
      "Epoch 4566/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8529.8664 - val_loss: 7583.1567\n",
      "Epoch 4567/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8529.4045 - val_loss: 7582.6934\n",
      "Epoch 4568/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8528.9421 - val_loss: 7582.2310\n",
      "Epoch 4569/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8528.4783 - val_loss: 7581.7690\n",
      "Epoch 4570/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8528.0162 - val_loss: 7581.3052\n",
      "Epoch 4571/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8527.5534 - val_loss: 7580.8418\n",
      "Epoch 4572/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8527.0907 - val_loss: 7580.3794\n",
      "Epoch 4573/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8526.6285 - val_loss: 7579.9180\n",
      "Epoch 4574/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 8533.73 - 0s 120us/step - loss: 8526.1653 - val_loss: 7579.4556\n",
      "Epoch 4575/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8525.7028 - val_loss: 7578.9917\n",
      "Epoch 4576/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8525.2403 - val_loss: 7578.5288\n",
      "Epoch 4577/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8524.7768 - val_loss: 7578.0664\n",
      "Epoch 4578/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8524.3153 - val_loss: 7577.6045\n",
      "Epoch 4579/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8523.8512 - val_loss: 7577.1416\n",
      "Epoch 4580/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8523.3890 - val_loss: 7576.6792\n",
      "Epoch 4581/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8522.9273 - val_loss: 7576.2148\n",
      "Epoch 4582/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8522.4634 - val_loss: 7575.7529\n",
      "Epoch 4583/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8522.0011 - val_loss: 7575.2915\n",
      "Epoch 4584/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8521.5380 - val_loss: 7574.8281\n",
      "Epoch 4585/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8521.0758 - val_loss: 7574.3652\n",
      "Epoch 4586/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8520.6133 - val_loss: 7573.9023\n",
      "Epoch 4587/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8520.1497 - val_loss: 7573.4399\n",
      "Epoch 4588/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8519.6874 - val_loss: 7572.9766\n",
      "Epoch 4589/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8519.2247 - val_loss: 7572.5137\n",
      "Epoch 4590/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8518.7626 - val_loss: 7572.0513\n",
      "Epoch 4591/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8518.2998 - val_loss: 7571.5889\n",
      "Epoch 4592/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8517.8363 - val_loss: 7571.1274\n",
      "Epoch 4593/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8517.3744 - val_loss: 7570.6641\n",
      "Epoch 4594/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8516.9118 - val_loss: 7570.1997\n",
      "Epoch 4595/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8516.4485 - val_loss: 7569.7378\n",
      "Epoch 4596/10000\n",
      "750/750 [==============================] - 0s 185us/step - loss: 8515.9866 - val_loss: 7569.2754\n",
      "Epoch 4597/10000\n",
      "750/750 [==============================] - 0s 169us/step - loss: 8515.5227 - val_loss: 7568.8125\n",
      "Epoch 4598/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8515.0605 - val_loss: 7568.3496\n",
      "Epoch 4599/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8514.5981 - val_loss: 7567.8862\n",
      "Epoch 4600/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8514.1349 - val_loss: 7567.4248\n",
      "Epoch 4601/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8513.6725 - val_loss: 7566.9629\n",
      "Epoch 4602/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8513.2096 - val_loss: 7566.4985\n",
      "Epoch 4603/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8512.7465 - val_loss: 7566.0352\n",
      "Epoch 4604/10000\n",
      "750/750 [==============================] - 0s 164us/step - loss: 8512.2846 - val_loss: 7565.5737\n",
      "Epoch 4605/10000\n",
      "750/750 [==============================] - 0s 168us/step - loss: 8511.8211 - val_loss: 7565.1113\n",
      "Epoch 4606/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 8511.3589 - val_loss: 7564.6484\n",
      "Epoch 4607/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8510.8965 - val_loss: 7564.1860\n",
      "Epoch 4608/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8510.4333 - val_loss: 7563.7222\n",
      "Epoch 4609/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8509.9712 - val_loss: 7563.2607\n",
      "Epoch 4610/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8509.5077 - val_loss: 7562.7974\n",
      "Epoch 4611/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 8509.0453 - val_loss: 7562.3345\n",
      "Epoch 4612/10000\n",
      "750/750 [==============================] - 0s 164us/step - loss: 8508.5834 - val_loss: 7561.8721\n",
      "Epoch 4613/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 8508.1197 - val_loss: 7561.4087\n",
      "Epoch 4614/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 8507.6570 - val_loss: 7560.9473\n",
      "Epoch 4615/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 8507.1943 - val_loss: 7560.4829\n",
      "Epoch 4616/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8506.7316 - val_loss: 7560.0220\n",
      "Epoch 4617/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8506.2698 - val_loss: 7559.5586\n",
      "Epoch 4618/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8505.8065 - val_loss: 7559.0957\n",
      "Epoch 4619/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8505.3440 - val_loss: 7558.6348\n",
      "Epoch 4620/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8504.8812 - val_loss: 7558.1704\n",
      "Epoch 4621/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8504.4178 - val_loss: 7557.7080\n",
      "Epoch 4622/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8503.9569 - val_loss: 7557.2446\n",
      "Epoch 4623/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8503.4923 - val_loss: 7556.7832\n",
      "Epoch 4624/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8503.0305 - val_loss: 7556.3198\n",
      "Epoch 4625/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8502.5684 - val_loss: 7555.8569\n",
      "Epoch 4626/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8502.1049 - val_loss: 7555.3945\n",
      "Epoch 4627/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8501.6422 - val_loss: 7554.9316\n",
      "Epoch 4628/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8501.1790 - val_loss: 7554.4683\n",
      "Epoch 4629/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8500.7169 - val_loss: 7554.0059\n",
      "Epoch 4630/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8500.2546 - val_loss: 7553.5430\n",
      "Epoch 4631/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8499.7907 - val_loss: 7553.0815\n",
      "Epoch 4632/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8499.3288 - val_loss: 7552.6177\n",
      "Epoch 4633/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8498.8659 - val_loss: 7552.1543\n",
      "Epoch 4634/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8498.4031 - val_loss: 7551.6919\n",
      "Epoch 4635/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8497.9408 - val_loss: 7551.2295\n",
      "Epoch 4636/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8497.4779 - val_loss: 7550.7681\n",
      "Epoch 4637/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8497.0151 - val_loss: 7550.3042\n",
      "Epoch 4638/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8496.5525 - val_loss: 7549.8418\n",
      "Epoch 4639/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8496.0899 - val_loss: 7549.3789\n",
      "Epoch 4640/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8495.6274 - val_loss: 7548.9165\n",
      "Epoch 4641/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8495.1640 - val_loss: 7548.4556\n",
      "Epoch 4642/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8494.7016 - val_loss: 7547.9917\n",
      "Epoch 4643/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8494.2397 - val_loss: 7547.5278\n",
      "Epoch 4644/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8493.7758 - val_loss: 7547.0654\n",
      "Epoch 4645/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8493.3141 - val_loss: 7546.6040\n",
      "Epoch 4646/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8492.8505 - val_loss: 7546.1387\n",
      "Epoch 4647/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8492.3879 - val_loss: 7545.6768\n",
      "Epoch 4648/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8491.9260 - val_loss: 7545.2148\n",
      "Epoch 4649/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8491.4624 - val_loss: 7544.7524\n",
      "Epoch 4650/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8490.9999 - val_loss: 7544.2900\n",
      "Epoch 4651/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8490.5373 - val_loss: 7543.8262\n",
      "Epoch 4652/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8490.0746 - val_loss: 7543.3628\n",
      "Epoch 4653/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8489.6123 - val_loss: 7542.9014\n",
      "Epoch 4654/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8489.1491 - val_loss: 7542.4399\n",
      "Epoch 4655/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8488.6862 - val_loss: 7541.9766\n",
      "Epoch 4656/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8488.2242 - val_loss: 7541.5122\n",
      "Epoch 4657/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8487.7609 - val_loss: 7541.0503\n",
      "Epoch 4658/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8487.2980 - val_loss: 7540.5889\n",
      "Epoch 4659/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8486.8353 - val_loss: 7540.1250\n",
      "Epoch 4660/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8486.3732 - val_loss: 7539.6626\n",
      "Epoch 4661/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8485.9110 - val_loss: 7539.1987\n",
      "Epoch 4662/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8485.4475 - val_loss: 7538.7373\n",
      "Epoch 4663/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8484.9846 - val_loss: 7538.2739\n",
      "Epoch 4664/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8484.5222 - val_loss: 7537.8110\n",
      "Epoch 4665/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8484.0594 - val_loss: 7537.3496\n",
      "Epoch 4666/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8483.5971 - val_loss: 7536.8857\n",
      "Epoch 4667/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8483.1332 - val_loss: 7536.4238\n",
      "Epoch 4668/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 137us/step - loss: 8482.6715 - val_loss: 7535.9609\n",
      "Epoch 4669/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8482.2088 - val_loss: 7535.4976\n",
      "Epoch 4670/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8481.7460 - val_loss: 7535.0347\n",
      "Epoch 4671/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8481.2836 - val_loss: 7534.5732\n",
      "Epoch 4672/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8480.8198 - val_loss: 7534.1113\n",
      "Epoch 4673/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8480.3579 - val_loss: 7533.6470\n",
      "Epoch 4674/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8479.8960 - val_loss: 7533.1846\n",
      "Epoch 4675/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8479.4322 - val_loss: 7532.7222\n",
      "Epoch 4676/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8478.9697 - val_loss: 7532.2598\n",
      "Epoch 4677/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8478.5068 - val_loss: 7531.7969\n",
      "Epoch 4678/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8478.0445 - val_loss: 7531.3335\n",
      "Epoch 4679/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8477.5815 - val_loss: 7530.8711\n",
      "Epoch 4680/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8477.1189 - val_loss: 7530.4082\n",
      "Epoch 4681/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8476.6563 - val_loss: 7529.9448\n",
      "Epoch 4682/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8476.1938 - val_loss: 7529.4829\n",
      "Epoch 4683/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8475.7310 - val_loss: 7529.0195\n",
      "Epoch 4684/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8475.2685 - val_loss: 7528.5571\n",
      "Epoch 4685/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8474.8051 - val_loss: 7528.0957\n",
      "Epoch 4686/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8474.3425 - val_loss: 7527.6323\n",
      "Epoch 4687/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8473.8804 - val_loss: 7527.1694\n",
      "Epoch 4688/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8473.4168 - val_loss: 7526.7070\n",
      "Epoch 4689/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8472.9545 - val_loss: 7526.2446\n",
      "Epoch 4690/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8472.4915 - val_loss: 7525.7808\n",
      "Epoch 4691/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8472.0294 - val_loss: 7525.3184\n",
      "Epoch 4692/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8471.5668 - val_loss: 7524.8555\n",
      "Epoch 4693/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8471.1038 - val_loss: 7524.3940\n",
      "Epoch 4694/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8470.6415 - val_loss: 7523.9316\n",
      "Epoch 4695/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8470.1783 - val_loss: 7523.4668\n",
      "Epoch 4696/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8469.7153 - val_loss: 7523.0054\n",
      "Epoch 4697/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8469.2541 - val_loss: 7522.5420\n",
      "Epoch 4698/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8468.7900 - val_loss: 7522.0806\n",
      "Epoch 4699/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8468.3276 - val_loss: 7521.6191\n",
      "Epoch 4700/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8467.8653 - val_loss: 7521.1543\n",
      "Epoch 4701/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8467.4022 - val_loss: 7520.6914\n",
      "Epoch 4702/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8466.9398 - val_loss: 7520.2295\n",
      "Epoch 4703/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8466.4764 - val_loss: 7519.7651\n",
      "Epoch 4704/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8466.0141 - val_loss: 7519.3027\n",
      "Epoch 4705/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8465.5519 - val_loss: 7518.8403\n",
      "Epoch 4706/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8465.0881 - val_loss: 7518.3789\n",
      "Epoch 4707/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8464.6261 - val_loss: 7517.9165\n",
      "Epoch 4708/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8464.1629 - val_loss: 7517.4526\n",
      "Epoch 4709/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8463.7007 - val_loss: 7516.9902\n",
      "Epoch 4710/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8463.2382 - val_loss: 7516.5273\n",
      "Epoch 4711/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8462.7747 - val_loss: 7516.0654\n",
      "Epoch 4712/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 8458.12 - 0s 120us/step - loss: 8462.3131 - val_loss: 7515.6016\n",
      "Epoch 4713/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8461.8498 - val_loss: 7515.1387\n",
      "Epoch 4714/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8461.3875 - val_loss: 7514.6753\n",
      "Epoch 4715/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8460.9249 - val_loss: 7514.2139\n",
      "Epoch 4716/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8460.4612 - val_loss: 7513.7524\n",
      "Epoch 4717/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8459.9989 - val_loss: 7513.2891\n",
      "Epoch 4718/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8459.5366 - val_loss: 7512.8252\n",
      "Epoch 4719/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8459.0730 - val_loss: 7512.3628\n",
      "Epoch 4720/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8458.6116 - val_loss: 7511.9004\n",
      "Epoch 4721/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8458.1477 - val_loss: 7511.4375\n",
      "Epoch 4722/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8457.6853 - val_loss: 7510.9746\n",
      "Epoch 4723/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8457.2232 - val_loss: 7510.5122\n",
      "Epoch 4724/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8456.7595 - val_loss: 7510.0498\n",
      "Epoch 4725/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8456.2971 - val_loss: 7509.5879\n",
      "Epoch 4726/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8455.8345 - val_loss: 7509.1235\n",
      "Epoch 4727/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8455.3716 - val_loss: 7508.6602\n",
      "Epoch 4728/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8454.9102 - val_loss: 7508.1987\n",
      "Epoch 4729/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8454.4466 - val_loss: 7507.7363\n",
      "Epoch 4730/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8453.9837 - val_loss: 7507.2734\n",
      "Epoch 4731/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 8453.5215 - val_loss: 7506.8101\n",
      "Epoch 4732/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8453.0583 - val_loss: 7506.3472\n",
      "Epoch 4733/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8452.5959 - val_loss: 7505.8857\n",
      "Epoch 4734/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8452.1322 - val_loss: 7505.4224\n",
      "Epoch 4735/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8451.6700 - val_loss: 7504.9609\n",
      "Epoch 4736/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8451.2084 - val_loss: 7504.4961\n",
      "Epoch 4737/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8450.7446 - val_loss: 7504.0347\n",
      "Epoch 4738/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 8450.2824 - val_loss: 7503.5723\n",
      "Epoch 4739/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8449.8192 - val_loss: 7503.1079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4740/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8449.3566 - val_loss: 7502.6470\n",
      "Epoch 4741/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8448.8951 - val_loss: 7502.1836\n",
      "Epoch 4742/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8448.4307 - val_loss: 7501.7212\n",
      "Epoch 4743/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8447.9687 - val_loss: 7501.2573\n",
      "Epoch 4744/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8447.5061 - val_loss: 7500.7954\n",
      "Epoch 4745/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8447.0436 - val_loss: 7500.3330\n",
      "Epoch 4746/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8446.5813 - val_loss: 7499.8696\n",
      "Epoch 4747/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8446.1169 - val_loss: 7499.4082\n",
      "Epoch 4748/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8445.6553 - val_loss: 7498.9448\n",
      "Epoch 4749/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8445.1930 - val_loss: 7498.4810\n",
      "Epoch 4750/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8444.7292 - val_loss: 7498.0195\n",
      "Epoch 4751/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8444.2675 - val_loss: 7497.5566\n",
      "Epoch 4752/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8443.8040 - val_loss: 7497.0933\n",
      "Epoch 4753/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 8443.3415 - val_loss: 7496.6309\n",
      "Epoch 4754/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 8442.8790 - val_loss: 7496.1680\n",
      "Epoch 4755/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8442.4157 - val_loss: 7495.7056\n",
      "Epoch 4756/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8441.9540 - val_loss: 7495.2417\n",
      "Epoch 4757/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8441.4906 - val_loss: 7494.7808\n",
      "Epoch 4758/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8441.0287 - val_loss: 7494.3169\n",
      "Epoch 4759/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8440.5657 - val_loss: 7493.8555\n",
      "Epoch 4760/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8440.1027 - val_loss: 7493.3931\n",
      "Epoch 4761/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8439.6396 - val_loss: 7492.9292\n",
      "Epoch 4762/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8439.1775 - val_loss: 7492.4668\n",
      "Epoch 4763/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8438.7147 - val_loss: 7492.0039\n",
      "Epoch 4764/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8438.2522 - val_loss: 7491.5420\n",
      "Epoch 4765/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8437.7885 - val_loss: 7491.0781\n",
      "Epoch 4766/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8437.3265 - val_loss: 7490.6152\n",
      "Epoch 4767/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8436.8642 - val_loss: 7490.1528\n",
      "Epoch 4768/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8436.4009 - val_loss: 7489.6904\n",
      "Epoch 4769/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8435.9391 - val_loss: 7489.2290\n",
      "Epoch 4770/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 8435.4755 - val_loss: 7488.7637\n",
      "Epoch 4771/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8435.0125 - val_loss: 7488.3027\n",
      "Epoch 4772/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8434.5509 - val_loss: 7487.8389\n",
      "Epoch 4773/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8434.0878 - val_loss: 7487.3774\n",
      "Epoch 4774/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8433.6247 - val_loss: 7486.9165\n",
      "Epoch 4775/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8433.1623 - val_loss: 7486.4512\n",
      "Epoch 4776/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8432.6991 - val_loss: 7485.9888\n",
      "Epoch 4777/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8432.2368 - val_loss: 7485.5264\n",
      "Epoch 4778/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8431.7738 - val_loss: 7485.0649\n",
      "Epoch 4779/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8431.3119 - val_loss: 7484.6016\n",
      "Epoch 4780/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8430.8494 - val_loss: 7484.1372\n",
      "Epoch 4781/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8430.3856 - val_loss: 7483.6753\n",
      "Epoch 4782/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8429.9231 - val_loss: 7483.2129\n",
      "Epoch 4783/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8429.4604 - val_loss: 7482.7500\n",
      "Epoch 4784/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 8428.9983 - val_loss: 7482.2876\n",
      "Epoch 4785/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 8428.5355 - val_loss: 7481.8237\n",
      "Epoch 4786/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 8428.0724 - val_loss: 7481.3623\n",
      "Epoch 4787/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 8427.6101 - val_loss: 7480.8989\n",
      "Epoch 4788/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 8427.1468 - val_loss: 7480.4360\n",
      "Epoch 4789/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 8426.6845 - val_loss: 7479.9722\n",
      "Epoch 4790/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8426.2220 - val_loss: 7479.5107\n",
      "Epoch 4791/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8425.7583 - val_loss: 7479.0488\n",
      "Epoch 4792/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8425.2959 - val_loss: 7478.5859\n",
      "Epoch 4793/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8424.8340 - val_loss: 7478.1235\n",
      "Epoch 4794/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 8424.3710 - val_loss: 7477.6597\n",
      "Epoch 4795/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8423.9084 - val_loss: 7477.1973\n",
      "Epoch 4796/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8423.4449 - val_loss: 7476.7344\n",
      "Epoch 4797/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8422.9828 - val_loss: 7476.2720\n",
      "Epoch 4798/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8422.5207 - val_loss: 7475.8101\n",
      "Epoch 4799/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8422.0571 - val_loss: 7475.3462\n",
      "Epoch 4800/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8421.5955 - val_loss: 7474.8848\n",
      "Epoch 4801/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8421.1317 - val_loss: 7474.4219\n",
      "Epoch 4802/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8420.6693 - val_loss: 7473.9585\n",
      "Epoch 4803/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8420.2072 - val_loss: 7473.4961\n",
      "Epoch 4804/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8419.7438 - val_loss: 7473.0332\n",
      "Epoch 4805/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8419.2808 - val_loss: 7472.5723\n",
      "Epoch 4806/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8418.8186 - val_loss: 7472.1079\n",
      "Epoch 4807/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8418.3562 - val_loss: 7471.6445\n",
      "Epoch 4808/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8417.8931 - val_loss: 7471.1836\n",
      "Epoch 4809/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8417.4293 - val_loss: 7470.7207\n",
      "Epoch 4810/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8416.9677 - val_loss: 7470.2573\n",
      "Epoch 4811/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8416.5055 - val_loss: 7469.7935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4812/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8416.0425 - val_loss: 7469.3320\n",
      "Epoch 4813/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8415.5799 - val_loss: 7468.8691\n",
      "Epoch 4814/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8415.1165 - val_loss: 7468.4058\n",
      "Epoch 4815/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8414.6539 - val_loss: 7467.9448\n",
      "Epoch 4816/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8414.1925 - val_loss: 7467.4805\n",
      "Epoch 4817/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8413.7288 - val_loss: 7467.0190\n",
      "Epoch 4818/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8413.2665 - val_loss: 7466.5566\n",
      "Epoch 4819/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8412.8034 - val_loss: 7466.0918\n",
      "Epoch 4820/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8412.3406 - val_loss: 7465.6294\n",
      "Epoch 4821/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8411.8781 - val_loss: 7465.1670\n",
      "Epoch 4822/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8411.4143 - val_loss: 7464.7056\n",
      "Epoch 4823/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8410.9525 - val_loss: 7464.2417\n",
      "Epoch 4824/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8410.4905 - val_loss: 7463.7793\n",
      "Epoch 4825/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8410.0264 - val_loss: 7463.3164\n",
      "Epoch 4826/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8409.5647 - val_loss: 7462.8540\n",
      "Epoch 4827/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8409.1010 - val_loss: 7462.3931\n",
      "Epoch 4828/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8408.6391 - val_loss: 7461.9277\n",
      "Epoch 4829/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8408.1764 - val_loss: 7461.4653\n",
      "Epoch 4830/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8407.7134 - val_loss: 7461.0029\n",
      "Epoch 4831/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8407.2513 - val_loss: 7460.5415\n",
      "Epoch 4832/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8406.7880 - val_loss: 7460.0781\n",
      "Epoch 4833/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8406.3262 - val_loss: 7459.6138\n",
      "Epoch 4834/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8405.8632 - val_loss: 7459.1523\n",
      "Epoch 4835/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8405.3997 - val_loss: 7458.6899\n",
      "Epoch 4836/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8404.9371 - val_loss: 7458.2266\n",
      "Epoch 4837/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8404.4749 - val_loss: 7457.7637\n",
      "Epoch 4838/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8404.0117 - val_loss: 7457.3003\n",
      "Epoch 4839/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8403.5495 - val_loss: 7456.8398\n",
      "Epoch 4840/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8403.0866 - val_loss: 7456.3760\n",
      "Epoch 4841/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8402.6240 - val_loss: 7455.9141\n",
      "Epoch 4842/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8402.1614 - val_loss: 7455.4497\n",
      "Epoch 4843/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8401.6980 - val_loss: 7454.9873\n",
      "Epoch 4844/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8401.2369 - val_loss: 7454.5254\n",
      "Epoch 4845/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8400.7725 - val_loss: 7454.0625\n",
      "Epoch 4846/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8400.3105 - val_loss: 7453.6001\n",
      "Epoch 4847/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8399.8484 - val_loss: 7453.1362\n",
      "Epoch 4848/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8399.3848 - val_loss: 7452.6748\n",
      "Epoch 4849/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8398.9221 - val_loss: 7452.2129\n",
      "Epoch 4850/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8398.4595 - val_loss: 7451.7485\n",
      "Epoch 4851/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8397.9968 - val_loss: 7451.2861\n",
      "Epoch 4852/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8397.5343 - val_loss: 7450.8237\n",
      "Epoch 4853/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8397.0709 - val_loss: 7450.3613\n",
      "Epoch 4854/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8396.6089 - val_loss: 7449.8984\n",
      "Epoch 4855/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8396.1464 - val_loss: 7449.4360\n",
      "Epoch 4856/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8395.6830 - val_loss: 7448.9727\n",
      "Epoch 4857/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8395.2211 - val_loss: 7448.5098\n",
      "Epoch 4858/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8394.7574 - val_loss: 7448.0474\n",
      "Epoch 4859/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8394.2956 - val_loss: 7447.5859\n",
      "Epoch 4860/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 8393.8332 - val_loss: 7447.1211\n",
      "Epoch 4861/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8393.3703 - val_loss: 7446.6597\n",
      "Epoch 4862/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8392.9071 - val_loss: 7446.1973\n",
      "Epoch 4863/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8392.4443 - val_loss: 7445.7344\n",
      "Epoch 4864/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8391.9823 - val_loss: 7445.2705\n",
      "Epoch 4865/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8391.5194 - val_loss: 7444.8086\n",
      "Epoch 4866/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 8391.0557 - val_loss: 7444.3462\n",
      "Epoch 4867/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8390.5935 - val_loss: 7443.8823\n",
      "Epoch 4868/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8390.1311 - val_loss: 7443.4204\n",
      "Epoch 4869/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8389.6679 - val_loss: 7442.9580\n",
      "Epoch 4870/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8389.2060 - val_loss: 7442.4956\n",
      "Epoch 4871/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8388.7423 - val_loss: 7442.0308\n",
      "Epoch 4872/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8388.2804 - val_loss: 7441.5698\n",
      "Epoch 4873/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8387.8178 - val_loss: 7441.1079\n",
      "Epoch 4874/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8387.3544 - val_loss: 7440.6440\n",
      "Epoch 4875/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8386.8925 - val_loss: 7440.1816\n",
      "Epoch 4876/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8386.4290 - val_loss: 7439.7183\n",
      "Epoch 4877/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8385.9666 - val_loss: 7439.2559\n",
      "Epoch 4878/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8385.5044 - val_loss: 7438.7930\n",
      "Epoch 4879/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8385.0407 - val_loss: 7438.3315\n",
      "Epoch 4880/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8384.5780 - val_loss: 7437.8691\n",
      "Epoch 4881/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8384.1158 - val_loss: 7437.4043\n",
      "Epoch 4882/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8383.6532 - val_loss: 7436.9419\n",
      "Epoch 4883/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8383.1904 - val_loss: 7436.4805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4884/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8382.7273 - val_loss: 7436.0181\n",
      "Epoch 4885/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8382.2646 - val_loss: 7435.5542\n",
      "Epoch 4886/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8381.8026 - val_loss: 7435.0913\n",
      "Epoch 4887/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8381.3397 - val_loss: 7434.6289\n",
      "Epoch 4888/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8380.8773 - val_loss: 7434.1665\n",
      "Epoch 4889/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8380.4135 - val_loss: 7433.7031\n",
      "Epoch 4890/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8379.9515 - val_loss: 7433.2417\n",
      "Epoch 4891/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8379.4896 - val_loss: 7432.7773\n",
      "Epoch 4892/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8379.0266 - val_loss: 7432.3164\n",
      "Epoch 4893/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8378.5637 - val_loss: 7431.8540\n",
      "Epoch 4894/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8378.1003 - val_loss: 7431.3887\n",
      "Epoch 4895/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8377.6377 - val_loss: 7430.9268\n",
      "Epoch 4896/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8377.1757 - val_loss: 7430.4639\n",
      "Epoch 4897/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8376.7119 - val_loss: 7430.0029\n",
      "Epoch 4898/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8376.2501 - val_loss: 7429.5391\n",
      "Epoch 4899/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8375.7874 - val_loss: 7429.0762\n",
      "Epoch 4900/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8375.3239 - val_loss: 7428.6138\n",
      "Epoch 4901/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8374.8616 - val_loss: 7428.1514\n",
      "Epoch 4902/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8374.3990 - val_loss: 7427.6899\n",
      "Epoch 4903/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8373.9367 - val_loss: 7427.2266\n",
      "Epoch 4904/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8373.4741 - val_loss: 7426.7637\n",
      "Epoch 4905/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8373.0109 - val_loss: 7426.2998\n",
      "Epoch 4906/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8372.5488 - val_loss: 7425.8379\n",
      "Epoch 4907/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8372.0853 - val_loss: 7425.3774\n",
      "Epoch 4908/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8371.6232 - val_loss: 7424.9111\n",
      "Epoch 4909/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8371.1607 - val_loss: 7424.4497\n",
      "Epoch 4910/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 8372.53 - 0s 120us/step - loss: 8370.6971 - val_loss: 7423.9873\n",
      "Epoch 4911/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8370.2345 - val_loss: 7423.5254\n",
      "Epoch 4912/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8369.7723 - val_loss: 7423.0610\n",
      "Epoch 4913/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8369.3090 - val_loss: 7422.5977\n",
      "Epoch 4914/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8368.8469 - val_loss: 7422.1362\n",
      "Epoch 4915/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8368.3840 - val_loss: 7421.6738\n",
      "Epoch 4916/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8367.9212 - val_loss: 7421.2109\n",
      "Epoch 4917/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8367.4591 - val_loss: 7420.7485\n",
      "Epoch 4918/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8366.9954 - val_loss: 7420.2847\n",
      "Epoch 4919/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8366.5343 - val_loss: 7419.8232\n",
      "Epoch 4920/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8366.0698 - val_loss: 7419.3594\n",
      "Epoch 4921/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8365.6076 - val_loss: 7418.8984\n",
      "Epoch 4922/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8365.1462 - val_loss: 7418.4336\n",
      "Epoch 4923/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8364.6822 - val_loss: 7417.9712\n",
      "Epoch 4924/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 8364.2199 - val_loss: 7417.5098\n",
      "Epoch 4925/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8363.7568 - val_loss: 7417.0454\n",
      "Epoch 4926/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8363.2939 - val_loss: 7416.5845\n",
      "Epoch 4927/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8362.8317 - val_loss: 7416.1211\n",
      "Epoch 4928/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8362.3679 - val_loss: 7415.6582\n",
      "Epoch 4929/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 8361.9061 - val_loss: 7415.1948\n",
      "Epoch 4930/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8361.4434 - val_loss: 7414.7329\n",
      "Epoch 4931/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 8360.9807 - val_loss: 7414.2705\n",
      "Epoch 4932/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8360.5181 - val_loss: 7413.8066\n",
      "Epoch 4933/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8360.0543 - val_loss: 7413.3457\n",
      "Epoch 4934/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8359.5928 - val_loss: 7412.8823\n",
      "Epoch 4935/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8359.1305 - val_loss: 7412.4185\n",
      "Epoch 4936/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8358.6675 - val_loss: 7411.9570\n",
      "Epoch 4937/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8358.2046 - val_loss: 7411.4941\n",
      "Epoch 4938/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8357.7415 - val_loss: 7411.0308\n",
      "Epoch 4939/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8357.2793 - val_loss: 7410.5669\n",
      "Epoch 4940/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8356.8167 - val_loss: 7410.1055\n",
      "Epoch 4941/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8356.3532 - val_loss: 7409.6445\n",
      "Epoch 4942/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8355.8913 - val_loss: 7409.1802\n",
      "Epoch 4943/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8355.4284 - val_loss: 7408.7168\n",
      "Epoch 4944/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8354.9649 - val_loss: 7408.2544\n",
      "Epoch 4945/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8354.5031 - val_loss: 7407.7930\n",
      "Epoch 4946/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8354.0395 - val_loss: 7407.3281\n",
      "Epoch 4947/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8353.5774 - val_loss: 7406.8667\n",
      "Epoch 4948/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8353.1150 - val_loss: 7406.4043\n",
      "Epoch 4949/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8352.6519 - val_loss: 7405.9414\n",
      "Epoch 4950/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8352.1902 - val_loss: 7405.4795\n",
      "Epoch 4951/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8351.7263 - val_loss: 7405.0151\n",
      "Epoch 4952/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8351.2638 - val_loss: 7404.5542\n",
      "Epoch 4953/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8350.8021 - val_loss: 7404.0903\n",
      "Epoch 4954/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8350.3382 - val_loss: 7403.6279\n",
      "Epoch 4955/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8349.8758 - val_loss: 7403.1665\n",
      "Epoch 4956/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8349.4131 - val_loss: 7402.7012\n",
      "Epoch 4957/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8348.9502 - val_loss: 7402.2393\n",
      "Epoch 4958/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8348.4883 - val_loss: 7401.7773\n",
      "Epoch 4959/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8348.0246 - val_loss: 7401.3149\n",
      "Epoch 4960/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8347.5622 - val_loss: 7400.8516\n",
      "Epoch 4961/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8347.0997 - val_loss: 7400.3887\n",
      "Epoch 4962/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8346.6365 - val_loss: 7399.9263\n",
      "Epoch 4963/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8346.1749 - val_loss: 7399.4639\n",
      "Epoch 4964/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 8347.88 - 0s 171us/step - loss: 8345.7105 - val_loss: 7399.0010\n",
      "Epoch 4965/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8345.2484 - val_loss: 7398.5391\n",
      "Epoch 4966/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8344.7865 - val_loss: 7398.0747\n",
      "Epoch 4967/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8344.3237 - val_loss: 7397.6128\n",
      "Epoch 4968/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8343.8613 - val_loss: 7397.1504\n",
      "Epoch 4969/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8343.3972 - val_loss: 7396.6875\n",
      "Epoch 4970/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8342.9358 - val_loss: 7396.2246\n",
      "Epoch 4971/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8342.4728 - val_loss: 7395.7612\n",
      "Epoch 4972/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8342.0094 - val_loss: 7395.2998\n",
      "Epoch 4973/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8341.5476 - val_loss: 7394.8359\n",
      "Epoch 4974/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8341.0843 - val_loss: 7394.3735\n",
      "Epoch 4975/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8340.6217 - val_loss: 7393.9126\n",
      "Epoch 4976/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8340.1593 - val_loss: 7393.4482\n",
      "Epoch 4977/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8339.6958 - val_loss: 7392.9863\n",
      "Epoch 4978/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8339.2341 - val_loss: 7392.5234\n",
      "Epoch 4979/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8338.7711 - val_loss: 7392.0610\n",
      "Epoch 4980/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8338.3087 - val_loss: 7391.5962\n",
      "Epoch 4981/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8337.8457 - val_loss: 7391.1348\n",
      "Epoch 4982/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8337.3826 - val_loss: 7390.6738\n",
      "Epoch 4983/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 8336.9202 - val_loss: 7390.2095\n",
      "Epoch 4984/10000\n",
      "750/750 [==============================] - 0s 162us/step - loss: 8336.4576 - val_loss: 7389.7471\n",
      "Epoch 4985/10000\n",
      "750/750 [==============================] - 0s 164us/step - loss: 8335.9948 - val_loss: 7389.2837\n",
      "Epoch 4986/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8335.5315 - val_loss: 7388.8223\n",
      "Epoch 4987/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8335.0693 - val_loss: 7388.3579\n",
      "Epoch 4988/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8334.6066 - val_loss: 7387.8955\n",
      "Epoch 4989/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8334.1441 - val_loss: 7387.4336\n",
      "Epoch 4990/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8333.6811 - val_loss: 7386.9707\n",
      "Epoch 4991/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8333.2187 - val_loss: 7386.5073\n",
      "Epoch 4992/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8332.7560 - val_loss: 7386.0454\n",
      "Epoch 4993/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8332.2927 - val_loss: 7385.5830\n",
      "Epoch 4994/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8331.8310 - val_loss: 7385.1206\n",
      "Epoch 4995/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8331.3681 - val_loss: 7384.6567\n",
      "Epoch 4996/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8330.9046 - val_loss: 7384.1948\n",
      "Epoch 4997/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8330.4429 - val_loss: 7383.7305\n",
      "Epoch 4998/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8329.9793 - val_loss: 7383.2690\n",
      "Epoch 4999/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8329.5168 - val_loss: 7382.8071\n",
      "Epoch 5000/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8329.0540 - val_loss: 7382.3433\n",
      "Epoch 5001/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8328.5919 - val_loss: 7381.8809\n",
      "Epoch 5002/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8328.1291 - val_loss: 7381.4180\n",
      "Epoch 5003/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8327.6656 - val_loss: 7380.9565\n",
      "Epoch 5004/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8327.2034 - val_loss: 7380.4917\n",
      "Epoch 5005/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8326.7410 - val_loss: 7380.0293\n",
      "Epoch 5006/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8326.2781 - val_loss: 7379.5684\n",
      "Epoch 5007/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8325.8159 - val_loss: 7379.1045\n",
      "Epoch 5008/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8325.3520 - val_loss: 7378.6431\n",
      "Epoch 5009/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8324.8901 - val_loss: 7378.1792\n",
      "Epoch 5010/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8324.4275 - val_loss: 7377.7168\n",
      "Epoch 5011/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8323.9650 - val_loss: 7377.2539\n",
      "Epoch 5012/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8323.5023 - val_loss: 7376.7920\n",
      "Epoch 5013/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8323.0383 - val_loss: 7376.3306\n",
      "Epoch 5014/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8322.5769 - val_loss: 7375.8652\n",
      "Epoch 5015/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8322.1141 - val_loss: 7375.4023\n",
      "Epoch 5016/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8321.6507 - val_loss: 7374.9414\n",
      "Epoch 5017/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8321.1886 - val_loss: 7374.4790\n",
      "Epoch 5018/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8320.7254 - val_loss: 7374.0137\n",
      "Epoch 5019/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8320.2626 - val_loss: 7373.5527\n",
      "Epoch 5020/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8319.8008 - val_loss: 7373.0898\n",
      "Epoch 5021/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8319.3375 - val_loss: 7372.6274\n",
      "Epoch 5022/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8318.8756 - val_loss: 7372.1641\n",
      "Epoch 5023/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8318.4121 - val_loss: 7371.7012\n",
      "Epoch 5024/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8317.9489 - val_loss: 7371.2378\n",
      "Epoch 5025/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8317.4874 - val_loss: 7370.7764\n",
      "Epoch 5026/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8317.0238 - val_loss: 7370.3135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5027/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8316.5612 - val_loss: 7369.8516\n",
      "Epoch 5028/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8316.0993 - val_loss: 7369.3872\n",
      "Epoch 5029/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8315.6356 - val_loss: 7368.9248\n",
      "Epoch 5030/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8315.1729 - val_loss: 7368.4639\n",
      "Epoch 5031/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8314.7103 - val_loss: 7367.9985\n",
      "Epoch 5032/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8314.2474 - val_loss: 7367.5376\n",
      "Epoch 5033/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8313.7856 - val_loss: 7367.0747\n",
      "Epoch 5034/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8313.3225 - val_loss: 7366.6123\n",
      "Epoch 5035/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8312.8592 - val_loss: 7366.1489\n",
      "Epoch 5036/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8312.3970 - val_loss: 7365.6860\n",
      "Epoch 5037/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8311.9338 - val_loss: 7365.2227\n",
      "Epoch 5038/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8311.4726 - val_loss: 7364.7607\n",
      "Epoch 5039/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8311.0086 - val_loss: 7364.2988\n",
      "Epoch 5040/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8310.5464 - val_loss: 7363.8364\n",
      "Epoch 5041/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8310.0838 - val_loss: 7363.3735\n",
      "Epoch 5042/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8309.6209 - val_loss: 7362.9097\n",
      "Epoch 5043/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8309.1585 - val_loss: 7362.4473\n",
      "Epoch 5044/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8308.6944 - val_loss: 7361.9844\n",
      "Epoch 5045/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8308.2329 - val_loss: 7361.5220\n",
      "Epoch 5046/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8307.7704 - val_loss: 7361.0586\n",
      "Epoch 5047/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8307.3063 - val_loss: 7360.5972\n",
      "Epoch 5048/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8306.8448 - val_loss: 7360.1333\n",
      "Epoch 5049/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8306.3814 - val_loss: 7359.6704\n",
      "Epoch 5050/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8305.9190 - val_loss: 7359.2095\n",
      "Epoch 5051/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8305.4566 - val_loss: 7358.7461\n",
      "Epoch 5052/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8304.9935 - val_loss: 7358.2832\n",
      "Epoch 5053/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8304.5313 - val_loss: 7357.8198\n",
      "Epoch 5054/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8304.0682 - val_loss: 7357.3579\n",
      "Epoch 5055/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8303.6061 - val_loss: 7356.8945\n",
      "Epoch 5056/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8303.1431 - val_loss: 7356.4321\n",
      "Epoch 5057/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8302.6800 - val_loss: 7355.9707\n",
      "Epoch 5058/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8302.2172 - val_loss: 7355.5059\n",
      "Epoch 5059/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8301.7550 - val_loss: 7355.0435\n",
      "Epoch 5060/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8301.2918 - val_loss: 7354.5820\n",
      "Epoch 5061/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8300.8289 - val_loss: 7354.1191\n",
      "Epoch 5062/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8300.3663 - val_loss: 7353.6543\n",
      "Epoch 5063/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8299.9034 - val_loss: 7353.1934\n",
      "Epoch 5064/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8299.4416 - val_loss: 7352.7310\n",
      "Epoch 5065/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8298.9781 - val_loss: 7352.2681\n",
      "Epoch 5066/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8298.5169 - val_loss: 7351.8052\n",
      "Epoch 5067/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8298.0530 - val_loss: 7351.3418\n",
      "Epoch 5068/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8297.5904 - val_loss: 7350.8794\n",
      "Epoch 5069/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8297.1285 - val_loss: 7350.4170\n",
      "Epoch 5070/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8296.6648 - val_loss: 7349.9556\n",
      "Epoch 5071/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8296.2021 - val_loss: 7349.4917\n",
      "Epoch 5072/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8295.7402 - val_loss: 7349.0273\n",
      "Epoch 5073/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8295.2769 - val_loss: 7348.5664\n",
      "Epoch 5074/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8294.8141 - val_loss: 7348.1045\n",
      "Epoch 5075/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8294.3510 - val_loss: 7347.6401\n",
      "Epoch 5076/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8293.8888 - val_loss: 7347.1792\n",
      "Epoch 5077/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8293.4271 - val_loss: 7346.7153\n",
      "Epoch 5078/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8292.9633 - val_loss: 7346.2529\n",
      "Epoch 5079/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8292.5008 - val_loss: 7345.7891\n",
      "Epoch 5080/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8292.0378 - val_loss: 7345.3276\n",
      "Epoch 5081/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8291.5753 - val_loss: 7344.8652\n",
      "Epoch 5082/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8291.1133 - val_loss: 7344.4014\n",
      "Epoch 5083/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8290.6505 - val_loss: 7343.9399\n",
      "Epoch 5084/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8290.1870 - val_loss: 7343.4766\n",
      "Epoch 5085/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8289.7244 - val_loss: 7343.0137\n",
      "Epoch 5086/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8289.2623 - val_loss: 7342.5503\n",
      "Epoch 5087/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8288.7991 - val_loss: 7342.0889\n",
      "Epoch 5088/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8288.3358 - val_loss: 7341.6274\n",
      "Epoch 5089/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8287.8739 - val_loss: 7341.1626\n",
      "Epoch 5090/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8287.4111 - val_loss: 7340.6997\n",
      "Epoch 5091/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8286.9480 - val_loss: 7340.2378\n",
      "Epoch 5092/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8286.4862 - val_loss: 7339.7754\n",
      "Epoch 5093/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8286.0227 - val_loss: 7339.3125\n",
      "Epoch 5094/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8285.5605 - val_loss: 7338.8496\n",
      "Epoch 5095/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8285.0981 - val_loss: 7338.3862\n",
      "Epoch 5096/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8284.6345 - val_loss: 7337.9238\n",
      "Epoch 5097/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8284.1724 - val_loss: 7337.4609\n",
      "Epoch 5098/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8283.7096 - val_loss: 7337.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5099/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8283.2469 - val_loss: 7336.5347\n",
      "Epoch 5100/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8282.7845 - val_loss: 7336.0732\n",
      "Epoch 5101/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8282.3212 - val_loss: 7335.6113\n",
      "Epoch 5102/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8281.8581 - val_loss: 7335.1484\n",
      "Epoch 5103/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8281.3965 - val_loss: 7334.6860\n",
      "Epoch 5104/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8280.9334 - val_loss: 7334.2222\n",
      "Epoch 5105/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8280.4702 - val_loss: 7333.7607\n",
      "Epoch 5106/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8280.0072 - val_loss: 7333.2969\n",
      "Epoch 5107/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8279.5449 - val_loss: 7332.8345\n",
      "Epoch 5108/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8279.0830 - val_loss: 7332.3721\n",
      "Epoch 5109/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8278.6195 - val_loss: 7331.9087\n",
      "Epoch 5110/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8278.1574 - val_loss: 7331.4473\n",
      "Epoch 5111/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8277.6943 - val_loss: 7330.9829\n",
      "Epoch 5112/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8277.2313 - val_loss: 7330.5220\n",
      "Epoch 5113/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8276.7696 - val_loss: 7330.0571\n",
      "Epoch 5114/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8276.3065 - val_loss: 7329.5957\n",
      "Epoch 5115/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8275.8433 - val_loss: 7329.1348\n",
      "Epoch 5116/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8275.3810 - val_loss: 7328.6704\n",
      "Epoch 5117/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8274.9179 - val_loss: 7328.2070\n",
      "Epoch 5118/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8274.4555 - val_loss: 7327.7446\n",
      "Epoch 5119/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8273.9916 - val_loss: 7327.2832\n",
      "Epoch 5120/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8273.5300 - val_loss: 7326.8198\n",
      "Epoch 5121/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 8273.0682 - val_loss: 7326.3560\n",
      "Epoch 5122/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8272.6035 - val_loss: 7325.8945\n",
      "Epoch 5123/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8272.1417 - val_loss: 7325.4316\n",
      "Epoch 5124/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8271.6790 - val_loss: 7324.9683\n",
      "Epoch 5125/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8271.2170 - val_loss: 7324.5059\n",
      "Epoch 5126/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8270.7540 - val_loss: 7324.0430\n",
      "Epoch 5127/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8270.2914 - val_loss: 7323.5815\n",
      "Epoch 5128/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8269.8285 - val_loss: 7323.1167\n",
      "Epoch 5129/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8269.3657 - val_loss: 7322.6558\n",
      "Epoch 5130/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8268.9035 - val_loss: 7322.1914\n",
      "Epoch 5131/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8268.4409 - val_loss: 7321.7295\n",
      "Epoch 5132/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8267.9774 - val_loss: 7321.2690\n",
      "Epoch 5133/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8267.5146 - val_loss: 7320.8042\n",
      "Epoch 5134/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8267.0525 - val_loss: 7320.3418\n",
      "Epoch 5135/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8266.5891 - val_loss: 7319.8789\n",
      "Epoch 5136/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8266.1270 - val_loss: 7319.4170\n",
      "Epoch 5137/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8265.6638 - val_loss: 7318.9531\n",
      "Epoch 5138/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8265.2017 - val_loss: 7318.4902\n",
      "Epoch 5139/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8264.7388 - val_loss: 7318.0293\n",
      "Epoch 5140/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8264.2756 - val_loss: 7317.5649\n",
      "Epoch 5141/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8263.8142 - val_loss: 7317.1040\n",
      "Epoch 5142/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8263.3502 - val_loss: 7316.6387\n",
      "Epoch 5143/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8262.8875 - val_loss: 7316.1763\n",
      "Epoch 5144/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8262.4261 - val_loss: 7315.7148\n",
      "Epoch 5145/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8261.9622 - val_loss: 7315.2524\n",
      "Epoch 5146/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8261.4997 - val_loss: 7314.7915\n",
      "Epoch 5147/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8261.0373 - val_loss: 7314.3247\n",
      "Epoch 5148/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8260.5742 - val_loss: 7313.8628\n",
      "Epoch 5149/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8260.1118 - val_loss: 7313.4023\n",
      "Epoch 5150/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8259.6484 - val_loss: 7312.9375\n",
      "Epoch 5151/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8259.1863 - val_loss: 7312.4766\n",
      "Epoch 5152/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8258.7241 - val_loss: 7312.0122\n",
      "Epoch 5153/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8258.2606 - val_loss: 7311.5503\n",
      "Epoch 5154/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 8257.7983 - val_loss: 7311.0879\n",
      "Epoch 5155/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 8257.3351 - val_loss: 7310.6250\n",
      "Epoch 5156/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8256.8723 - val_loss: 7310.1641\n",
      "Epoch 5157/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8256.4111 - val_loss: 7309.6982\n",
      "Epoch 5158/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8255.9473 - val_loss: 7309.2373\n",
      "Epoch 5159/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8255.4846 - val_loss: 7308.7734\n",
      "Epoch 5160/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8255.0218 - val_loss: 7308.3110\n",
      "Epoch 5161/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8254.5594 - val_loss: 7307.8477\n",
      "Epoch 5162/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8254.0968 - val_loss: 7307.3857\n",
      "Epoch 5163/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8253.6331 - val_loss: 7306.9238\n",
      "Epoch 5164/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8253.1710 - val_loss: 7306.4595\n",
      "Epoch 5165/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8252.7085 - val_loss: 7305.9985\n",
      "Epoch 5166/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8252.2449 - val_loss: 7305.5352\n",
      "Epoch 5167/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8251.7832 - val_loss: 7305.0723\n",
      "Epoch 5168/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8251.3196 - val_loss: 7304.6094\n",
      "Epoch 5169/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8250.8578 - val_loss: 7304.1484\n",
      "Epoch 5170/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8250.3957 - val_loss: 7303.6846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5171/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8249.9315 - val_loss: 7303.2212\n",
      "Epoch 5172/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8249.4699 - val_loss: 7302.7598\n",
      "Epoch 5173/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8249.0066 - val_loss: 7302.2969\n",
      "Epoch 5174/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8248.5446 - val_loss: 7301.8330\n",
      "Epoch 5175/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8248.0817 - val_loss: 7301.3711\n",
      "Epoch 5176/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8247.6185 - val_loss: 7300.9087\n",
      "Epoch 5177/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8247.1553 - val_loss: 7300.4448\n",
      "Epoch 5178/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8246.6935 - val_loss: 7299.9829\n",
      "Epoch 5179/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8246.2303 - val_loss: 7299.5195\n",
      "Epoch 5180/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8245.7675 - val_loss: 7299.0586\n",
      "Epoch 5181/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8245.3045 - val_loss: 7298.5933\n",
      "Epoch 5182/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8244.8422 - val_loss: 7298.1323\n",
      "Epoch 5183/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8244.3802 - val_loss: 7297.6704\n",
      "Epoch 5184/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8243.9169 - val_loss: 7297.2065\n",
      "Epoch 5185/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8243.4552 - val_loss: 7296.7441\n",
      "Epoch 5186/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8242.9914 - val_loss: 7296.2793\n",
      "Epoch 5187/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8242.5283 - val_loss: 7295.8184\n",
      "Epoch 5188/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8242.0668 - val_loss: 7295.3545\n",
      "Epoch 5189/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8241.6039 - val_loss: 7294.8931\n",
      "Epoch 5190/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8241.1406 - val_loss: 7294.4316\n",
      "Epoch 5191/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8240.6779 - val_loss: 7293.9668\n",
      "Epoch 5192/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 8240.2153 - val_loss: 7293.5044\n",
      "Epoch 5193/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 8239.7524 - val_loss: 7293.0420\n",
      "Epoch 5194/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 8239.2893 - val_loss: 7292.5806\n",
      "Epoch 5195/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 8238.8275 - val_loss: 7292.1167\n",
      "Epoch 5196/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8238.3651 - val_loss: 7291.6528\n",
      "Epoch 5197/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8237.9014 - val_loss: 7291.1919\n",
      "Epoch 5198/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8237.4392 - val_loss: 7290.7290\n",
      "Epoch 5199/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8236.9760 - val_loss: 7290.2651\n",
      "Epoch 5200/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8236.5140 - val_loss: 7289.8042\n",
      "Epoch 5201/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8236.0518 - val_loss: 7289.3403\n",
      "Epoch 5202/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8235.5886 - val_loss: 7288.8779\n",
      "Epoch 5203/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8235.1259 - val_loss: 7288.4141\n",
      "Epoch 5204/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8234.6627 - val_loss: 7287.9531\n",
      "Epoch 5205/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8234.2009 - val_loss: 7287.4878\n",
      "Epoch 5206/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8233.7377 - val_loss: 7287.0264\n",
      "Epoch 5207/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8233.2744 - val_loss: 7286.5654\n",
      "Epoch 5208/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8232.8117 - val_loss: 7286.1016\n",
      "Epoch 5209/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8232.3496 - val_loss: 7285.6387\n",
      "Epoch 5210/10000\n",
      "750/750 [==============================] - 0s 164us/step - loss: 8231.8867 - val_loss: 7285.1753\n",
      "Epoch 5211/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 8231.4240 - val_loss: 7284.7139\n",
      "Epoch 5212/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8230.9608 - val_loss: 7284.2500\n",
      "Epoch 5213/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 8230.4989 - val_loss: 7283.7876\n",
      "Epoch 5214/10000\n",
      "750/750 [==============================] - 0s 159us/step - loss: 8230.0362 - val_loss: 7283.3262\n",
      "Epoch 5215/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 8229.5730 - val_loss: 7282.8623\n",
      "Epoch 5216/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 8229.1111 - val_loss: 7282.4004\n",
      "Epoch 5217/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8228.6481 - val_loss: 7281.9375\n",
      "Epoch 5218/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8228.1851 - val_loss: 7281.4746\n",
      "Epoch 5219/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8227.7231 - val_loss: 7281.0112\n",
      "Epoch 5220/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8227.2599 - val_loss: 7280.5498\n",
      "Epoch 5221/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8226.7966 - val_loss: 7280.0879\n",
      "Epoch 5222/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8226.3348 - val_loss: 7279.6235\n",
      "Epoch 5223/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8225.8713 - val_loss: 7279.1597\n",
      "Epoch 5224/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8225.4092 - val_loss: 7278.6987\n",
      "Epoch 5225/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8224.9453 - val_loss: 7278.2363\n",
      "Epoch 5226/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8224.4835 - val_loss: 7277.7734\n",
      "Epoch 5227/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8224.0216 - val_loss: 7277.3110\n",
      "Epoch 5228/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8223.5582 - val_loss: 7276.8472\n",
      "Epoch 5229/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8223.0954 - val_loss: 7276.3848\n",
      "Epoch 5230/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 8222.6323 - val_loss: 7275.9219\n",
      "Epoch 5231/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8222.1703 - val_loss: 7275.4609\n",
      "Epoch 5232/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8221.7081 - val_loss: 7274.9961\n",
      "Epoch 5233/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8221.2449 - val_loss: 7274.5337\n",
      "Epoch 5234/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8220.7824 - val_loss: 7274.0723\n",
      "Epoch 5235/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8220.3188 - val_loss: 7273.6084\n",
      "Epoch 5236/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8219.8564 - val_loss: 7273.1470\n",
      "Epoch 5237/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 8219.3942 - val_loss: 7272.6821\n",
      "Epoch 5238/10000\n",
      "750/750 [==============================] - 0s 164us/step - loss: 8218.9302 - val_loss: 7272.2212\n",
      "Epoch 5239/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 8218.4688 - val_loss: 7271.7568\n",
      "Epoch 5240/10000\n",
      "750/750 [==============================] - 0s 220us/step - loss: 8218.0061 - val_loss: 7271.2954\n",
      "Epoch 5241/10000\n",
      "750/750 [==============================] - 0s 176us/step - loss: 8217.5425 - val_loss: 7270.8345\n",
      "Epoch 5242/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8217.0808 - val_loss: 7270.3691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5243/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8216.6169 - val_loss: 7269.9082\n",
      "Epoch 5244/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8216.1554 - val_loss: 7269.4443\n",
      "Epoch 5245/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8215.6928 - val_loss: 7268.9829\n",
      "Epoch 5246/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8215.2292 - val_loss: 7268.5190\n",
      "Epoch 5247/10000\n",
      "750/750 [==============================] - 0s 211us/step - loss: 8214.7674 - val_loss: 7268.0566\n",
      "Epoch 5248/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 8214.3037 - val_loss: 7267.5957\n",
      "Epoch 5249/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8213.8420 - val_loss: 7267.1309\n",
      "Epoch 5250/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8213.3788 - val_loss: 7266.6680\n",
      "Epoch 5251/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8212.9156 - val_loss: 7266.2065\n",
      "Epoch 5252/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8212.4526 - val_loss: 7265.7417\n",
      "Epoch 5253/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8211.9910 - val_loss: 7265.2793\n",
      "Epoch 5254/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8211.5276 - val_loss: 7264.8164\n",
      "Epoch 5255/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8211.0657 - val_loss: 7264.3555\n",
      "Epoch 5256/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8210.6023 - val_loss: 7263.8931\n",
      "Epoch 5257/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8210.1393 - val_loss: 7263.4292\n",
      "Epoch 5258/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8209.6774 - val_loss: 7262.9668\n",
      "Epoch 5259/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8209.2141 - val_loss: 7262.5039\n",
      "Epoch 5260/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8208.7528 - val_loss: 7262.0415\n",
      "Epoch 5261/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8208.2885 - val_loss: 7261.5781\n",
      "Epoch 5262/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8207.8263 - val_loss: 7261.1157\n",
      "Epoch 5263/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8207.3647 - val_loss: 7260.6523\n",
      "Epoch 5264/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8206.9009 - val_loss: 7260.1904\n",
      "Epoch 5265/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8206.4386 - val_loss: 7259.7295\n",
      "Epoch 5266/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8205.9751 - val_loss: 7259.2642\n",
      "Epoch 5267/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8205.5128 - val_loss: 7258.8013\n",
      "Epoch 5268/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8205.0503 - val_loss: 7258.3398\n",
      "Epoch 5269/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8204.5864 - val_loss: 7257.8774\n",
      "Epoch 5270/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8204.1248 - val_loss: 7257.4141\n",
      "Epoch 5271/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8203.6621 - val_loss: 7256.9512\n",
      "Epoch 5272/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8203.1989 - val_loss: 7256.4902\n",
      "Epoch 5273/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8202.7368 - val_loss: 7256.0254\n",
      "Epoch 5274/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 8202.2730 - val_loss: 7255.5625\n",
      "Epoch 5275/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8201.8113 - val_loss: 7255.1016\n",
      "Epoch 5276/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8201.3490 - val_loss: 7254.6372\n",
      "Epoch 5277/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8200.8860 - val_loss: 7254.1748\n",
      "Epoch 5278/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8200.4231 - val_loss: 7253.7129\n",
      "Epoch 5279/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 8199.9599 - val_loss: 7253.2500\n",
      "Epoch 5280/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8199.4976 - val_loss: 7252.7852\n",
      "Epoch 5281/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8199.0348 - val_loss: 7252.3237\n",
      "Epoch 5282/10000\n",
      "750/750 [==============================] - 0s 155us/step - loss: 8198.5719 - val_loss: 7251.8628\n",
      "Epoch 5283/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8198.1094 - val_loss: 7251.3984\n",
      "Epoch 5284/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8197.6468 - val_loss: 7250.9360\n",
      "Epoch 5285/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8197.1841 - val_loss: 7250.4727\n",
      "Epoch 5286/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8196.7217 - val_loss: 7250.0107\n",
      "Epoch 5287/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8196.2581 - val_loss: 7249.5488\n",
      "Epoch 5288/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8195.7966 - val_loss: 7249.0850\n",
      "Epoch 5289/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8195.3335 - val_loss: 7248.6235\n",
      "Epoch 5290/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8194.8699 - val_loss: 7248.1587\n",
      "Epoch 5291/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8194.4082 - val_loss: 7247.6973\n",
      "Epoch 5292/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 8193.9449 - val_loss: 7247.2344\n",
      "Epoch 5293/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8193.4824 - val_loss: 7246.7720\n",
      "Epoch 5294/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8193.0200 - val_loss: 7246.3086\n",
      "Epoch 5295/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8192.5570 - val_loss: 7245.8462\n",
      "Epoch 5296/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8192.0937 - val_loss: 7245.3848\n",
      "Epoch 5297/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8191.6318 - val_loss: 7244.9204\n",
      "Epoch 5298/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8191.1689 - val_loss: 7244.4580\n",
      "Epoch 5299/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8190.7065 - val_loss: 7243.9971\n",
      "Epoch 5300/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8190.2428 - val_loss: 7243.5332\n",
      "Epoch 5301/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8189.7803 - val_loss: 7243.0698\n",
      "Epoch 5302/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8189.3184 - val_loss: 7242.6079\n",
      "Epoch 5303/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8188.8552 - val_loss: 7242.1445\n",
      "Epoch 5304/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8188.3932 - val_loss: 7241.6821\n",
      "Epoch 5305/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8187.9303 - val_loss: 7241.2207\n",
      "Epoch 5306/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8187.4673 - val_loss: 7240.7573\n",
      "Epoch 5307/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8187.0052 - val_loss: 7240.2930\n",
      "Epoch 5308/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8186.5424 - val_loss: 7239.8315\n",
      "Epoch 5309/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8186.0793 - val_loss: 7239.3691\n",
      "Epoch 5310/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8185.6160 - val_loss: 7238.9058\n",
      "Epoch 5311/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8185.1536 - val_loss: 7238.4434\n",
      "Epoch 5312/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8184.6913 - val_loss: 7237.9805\n",
      "Epoch 5313/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8184.2277 - val_loss: 7237.5190\n",
      "Epoch 5314/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8183.7657 - val_loss: 7237.0542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5315/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8183.3031 - val_loss: 7236.5918\n",
      "Epoch 5316/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8182.8399 - val_loss: 7236.1309\n",
      "Epoch 5317/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8182.3783 - val_loss: 7235.6670\n",
      "Epoch 5318/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8181.9143 - val_loss: 7235.2056\n",
      "Epoch 5319/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8181.4525 - val_loss: 7234.7417\n",
      "Epoch 5320/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8180.9899 - val_loss: 7234.2793\n",
      "Epoch 5321/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8180.5275 - val_loss: 7233.8154\n",
      "Epoch 5322/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8180.0646 - val_loss: 7233.3540\n",
      "Epoch 5323/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8179.6012 - val_loss: 7232.8931\n",
      "Epoch 5324/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8179.1388 - val_loss: 7232.4277\n",
      "Epoch 5325/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8178.6763 - val_loss: 7231.9648\n",
      "Epoch 5326/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8178.2127 - val_loss: 7231.5029\n",
      "Epoch 5327/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8177.7501 - val_loss: 7231.0391\n",
      "Epoch 5328/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8177.2881 - val_loss: 7230.5767\n",
      "Epoch 5329/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8176.8250 - val_loss: 7230.1138\n",
      "Epoch 5330/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8176.3626 - val_loss: 7229.6523\n",
      "Epoch 5331/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8175.8995 - val_loss: 7229.1899\n",
      "Epoch 5332/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8175.4374 - val_loss: 7228.7266\n",
      "Epoch 5333/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8174.9746 - val_loss: 7228.2637\n",
      "Epoch 5334/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8174.5115 - val_loss: 7227.8003\n",
      "Epoch 5335/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8174.0498 - val_loss: 7227.3389\n",
      "Epoch 5336/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8173.5864 - val_loss: 7226.8750\n",
      "Epoch 5337/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8173.1234 - val_loss: 7226.4136\n",
      "Epoch 5338/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8172.6617 - val_loss: 7225.9487\n",
      "Epoch 5339/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8172.1978 - val_loss: 7225.4873\n",
      "Epoch 5340/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8171.7354 - val_loss: 7225.0264\n",
      "Epoch 5341/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8171.2726 - val_loss: 7224.5625\n",
      "Epoch 5342/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8170.8098 - val_loss: 7224.0996\n",
      "Epoch 5343/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8170.3479 - val_loss: 7223.6362\n",
      "Epoch 5344/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8169.8835 - val_loss: 7223.1738\n",
      "Epoch 5345/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8169.4218 - val_loss: 7222.7109\n",
      "Epoch 5346/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 8168.9594 - val_loss: 7222.2485\n",
      "Epoch 5347/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8168.4959 - val_loss: 7221.7876\n",
      "Epoch 5348/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8168.0339 - val_loss: 7221.3232\n",
      "Epoch 5349/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8167.5707 - val_loss: 7220.8613\n",
      "Epoch 5350/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8167.1084 - val_loss: 7220.3984\n",
      "Epoch 5351/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8166.6460 - val_loss: 7219.9346\n",
      "Epoch 5352/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8166.1832 - val_loss: 7219.4722\n",
      "Epoch 5353/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8165.7206 - val_loss: 7219.0098\n",
      "Epoch 5354/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8165.2569 - val_loss: 7218.5469\n",
      "Epoch 5355/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8164.7953 - val_loss: 7218.0845\n",
      "Epoch 5356/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8164.3325 - val_loss: 7217.6211\n",
      "Epoch 5357/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8163.8687 - val_loss: 7217.1597\n",
      "Epoch 5358/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8163.4072 - val_loss: 7216.6948\n",
      "Epoch 5359/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8162.9441 - val_loss: 7216.2334\n",
      "Epoch 5360/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 8162.4816 - val_loss: 7215.7705\n",
      "Epoch 5361/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8162.0187 - val_loss: 7215.3086\n",
      "Epoch 5362/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8161.5552 - val_loss: 7214.8457\n",
      "Epoch 5363/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8161.0940 - val_loss: 7214.3823\n",
      "Epoch 5364/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8160.6305 - val_loss: 7213.9209\n",
      "Epoch 5365/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8160.1677 - val_loss: 7213.4570\n",
      "Epoch 5366/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8159.7061 - val_loss: 7212.9941\n",
      "Epoch 5367/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8159.2421 - val_loss: 7212.5308\n",
      "Epoch 5368/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8158.7796 - val_loss: 7212.0684\n",
      "Epoch 5369/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8158.3175 - val_loss: 7211.6060\n",
      "Epoch 5370/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8157.8539 - val_loss: 7211.1440\n",
      "Epoch 5371/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8157.3912 - val_loss: 7210.6816\n",
      "Epoch 5372/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8156.9288 - val_loss: 7210.2178\n",
      "Epoch 5373/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8156.4659 - val_loss: 7209.7544\n",
      "Epoch 5374/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8156.0039 - val_loss: 7209.2935\n",
      "Epoch 5375/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8155.5404 - val_loss: 7208.8306\n",
      "Epoch 5376/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8155.0778 - val_loss: 7208.3667\n",
      "Epoch 5377/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8154.6156 - val_loss: 7207.9053\n",
      "Epoch 5378/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8154.1520 - val_loss: 7207.4419\n",
      "Epoch 5379/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8153.6914 - val_loss: 7206.9795\n",
      "Epoch 5380/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8153.2272 - val_loss: 7206.5181\n",
      "Epoch 5381/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8152.7644 - val_loss: 7206.0542\n",
      "Epoch 5382/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8152.3024 - val_loss: 7205.5903\n",
      "Epoch 5383/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8151.8392 - val_loss: 7205.1289\n",
      "Epoch 5384/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8151.3766 - val_loss: 7204.6670\n",
      "Epoch 5385/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8150.9130 - val_loss: 7204.2031\n",
      "Epoch 5386/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8150.4514 - val_loss: 7203.7402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5387/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8149.9885 - val_loss: 7203.2773\n",
      "Epoch 5388/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8149.5249 - val_loss: 7202.8154\n",
      "Epoch 5389/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8149.0632 - val_loss: 7202.3516\n",
      "Epoch 5390/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8148.6000 - val_loss: 7201.8892\n",
      "Epoch 5391/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8148.1374 - val_loss: 7201.4277\n",
      "Epoch 5392/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8147.6754 - val_loss: 7200.9639\n",
      "Epoch 5393/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8147.2114 - val_loss: 7200.5024\n",
      "Epoch 5394/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8146.7496 - val_loss: 7200.0391\n",
      "Epoch 5395/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8146.2869 - val_loss: 7199.5762\n",
      "Epoch 5396/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8145.8246 - val_loss: 7199.1128\n",
      "Epoch 5397/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8145.3615 - val_loss: 7198.6504\n",
      "Epoch 5398/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8144.8983 - val_loss: 7198.1899\n",
      "Epoch 5399/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8144.4358 - val_loss: 7197.7246\n",
      "Epoch 5400/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8143.9735 - val_loss: 7197.2622\n",
      "Epoch 5401/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8143.5104 - val_loss: 7196.7998\n",
      "Epoch 5402/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8143.0474 - val_loss: 7196.3379\n",
      "Epoch 5403/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8142.5850 - val_loss: 7195.8750\n",
      "Epoch 5404/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8142.1229 - val_loss: 7195.4126\n",
      "Epoch 5405/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8141.6599 - val_loss: 7194.9487\n",
      "Epoch 5406/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8141.1968 - val_loss: 7194.4863\n",
      "Epoch 5407/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8140.7343 - val_loss: 7194.0234\n",
      "Epoch 5408/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8140.2718 - val_loss: 7193.5625\n",
      "Epoch 5409/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8139.8084 - val_loss: 7193.0972\n",
      "Epoch 5410/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8139.3468 - val_loss: 7192.6357\n",
      "Epoch 5411/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8138.8837 - val_loss: 7192.1719\n",
      "Epoch 5412/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8138.4204 - val_loss: 7191.7109\n",
      "Epoch 5413/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8137.9587 - val_loss: 7191.2471\n",
      "Epoch 5414/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8137.4949 - val_loss: 7190.7837\n",
      "Epoch 5415/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8137.0326 - val_loss: 7190.3232\n",
      "Epoch 5416/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8136.5696 - val_loss: 7189.8594\n",
      "Epoch 5417/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8136.1072 - val_loss: 7189.3970\n",
      "Epoch 5418/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8135.6452 - val_loss: 7188.9336\n",
      "Epoch 5419/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8135.1816 - val_loss: 7188.4712\n",
      "Epoch 5420/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8134.7189 - val_loss: 7188.0073\n",
      "Epoch 5421/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8134.2566 - val_loss: 7187.5459\n",
      "Epoch 5422/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8133.7938 - val_loss: 7187.0845\n",
      "Epoch 5423/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8133.3321 - val_loss: 7186.6206\n",
      "Epoch 5424/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8132.8689 - val_loss: 7186.1582\n",
      "Epoch 5425/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8132.4059 - val_loss: 7185.6948\n",
      "Epoch 5426/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8131.9431 - val_loss: 7185.2329\n",
      "Epoch 5427/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8131.4808 - val_loss: 7184.7695\n",
      "Epoch 5428/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8131.0179 - val_loss: 7184.3071\n",
      "Epoch 5429/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8130.5540 - val_loss: 7183.8457\n",
      "Epoch 5430/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8130.0925 - val_loss: 7183.3809\n",
      "Epoch 5431/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8129.6297 - val_loss: 7182.9180\n",
      "Epoch 5432/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8129.1667 - val_loss: 7182.4570\n",
      "Epoch 5433/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8128.7041 - val_loss: 7181.9941\n",
      "Epoch 5434/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8128.2414 - val_loss: 7181.5308\n",
      "Epoch 5435/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8127.7787 - val_loss: 7181.0684\n",
      "Epoch 5436/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8127.3164 - val_loss: 7180.6055\n",
      "Epoch 5437/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8126.8529 - val_loss: 7180.1431\n",
      "Epoch 5438/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8126.3912 - val_loss: 7179.6792\n",
      "Epoch 5439/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8125.9279 - val_loss: 7179.2183\n",
      "Epoch 5440/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8125.4657 - val_loss: 7178.7539\n",
      "Epoch 5441/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8125.0033 - val_loss: 7178.2920\n",
      "Epoch 5442/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 8124.5394 - val_loss: 7177.8306\n",
      "Epoch 5443/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8124.0767 - val_loss: 7177.3657\n",
      "Epoch 5444/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8123.6152 - val_loss: 7176.9028\n",
      "Epoch 5445/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8123.1513 - val_loss: 7176.4404\n",
      "Epoch 5446/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8122.6889 - val_loss: 7175.9795\n",
      "Epoch 5447/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8122.2262 - val_loss: 7175.5151\n",
      "Epoch 5448/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8121.7632 - val_loss: 7175.0527\n",
      "Epoch 5449/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8121.3012 - val_loss: 7174.5918\n",
      "Epoch 5450/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8120.8383 - val_loss: 7174.1279\n",
      "Epoch 5451/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8120.3751 - val_loss: 7173.6665\n",
      "Epoch 5452/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8119.9128 - val_loss: 7173.2017\n",
      "Epoch 5453/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8119.4500 - val_loss: 7172.7388\n",
      "Epoch 5454/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8118.9883 - val_loss: 7172.2764\n",
      "Epoch 5455/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8118.5249 - val_loss: 7171.8149\n",
      "Epoch 5456/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8118.0619 - val_loss: 7171.3540\n",
      "Epoch 5457/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8117.5998 - val_loss: 7170.8887\n",
      "Epoch 5458/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8117.1366 - val_loss: 7170.4253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5459/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 8116.6742 - val_loss: 7169.9639\n",
      "Epoch 5460/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8116.2102 - val_loss: 7169.5000\n",
      "Epoch 5461/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8115.7486 - val_loss: 7169.0376\n",
      "Epoch 5462/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8115.2863 - val_loss: 7168.5737\n",
      "Epoch 5463/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8114.8226 - val_loss: 7168.1128\n",
      "Epoch 5464/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8114.3605 - val_loss: 7167.6504\n",
      "Epoch 5465/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8113.8974 - val_loss: 7167.1875\n",
      "Epoch 5466/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8113.4347 - val_loss: 7166.7251\n",
      "Epoch 5467/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8112.9733 - val_loss: 7166.2612\n",
      "Epoch 5468/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8112.5094 - val_loss: 7165.7988\n",
      "Epoch 5469/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8112.0473 - val_loss: 7165.3359\n",
      "Epoch 5470/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8111.5842 - val_loss: 7164.8740\n",
      "Epoch 5471/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8111.1217 - val_loss: 7164.4097\n",
      "Epoch 5472/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8110.6587 - val_loss: 7163.9482\n",
      "Epoch 5473/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8110.1955 - val_loss: 7163.4863\n",
      "Epoch 5474/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 8109.7331 - val_loss: 7163.0225\n",
      "Epoch 5475/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8109.2711 - val_loss: 7162.5610\n",
      "Epoch 5476/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8108.8075 - val_loss: 7162.0972\n",
      "Epoch 5477/10000\n",
      "750/750 [==============================] - 0s 176us/step - loss: 8108.3456 - val_loss: 7161.6348\n",
      "Epoch 5478/10000\n",
      "750/750 [==============================] - 0s 167us/step - loss: 8107.8822 - val_loss: 7161.1719\n",
      "Epoch 5479/10000\n",
      "750/750 [==============================] - 0s 199us/step - loss: 8107.4199 - val_loss: 7160.7095\n",
      "Epoch 5480/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8106.9573 - val_loss: 7160.2485\n",
      "Epoch 5481/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8106.4940 - val_loss: 7159.7832\n",
      "Epoch 5482/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 8106.0326 - val_loss: 7159.3198\n",
      "Epoch 5483/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8105.5686 - val_loss: 7158.8594\n",
      "Epoch 5484/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8105.1066 - val_loss: 7158.3955\n",
      "Epoch 5485/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8104.6438 - val_loss: 7157.9321\n",
      "Epoch 5486/10000\n",
      "750/750 [==============================] - 0s 162us/step - loss: 8104.1809 - val_loss: 7157.4707\n",
      "Epoch 5487/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 8103.7184 - val_loss: 7157.0073\n",
      "Epoch 5488/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8103.2557 - val_loss: 7156.5435\n",
      "Epoch 5489/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 8102.7927 - val_loss: 7156.0820\n",
      "Epoch 5490/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8102.3303 - val_loss: 7155.6211\n",
      "Epoch 5491/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8101.8667 - val_loss: 7155.1558\n",
      "Epoch 5492/10000\n",
      "750/750 [==============================] - 0s 155us/step - loss: 8101.4047 - val_loss: 7154.6943\n",
      "Epoch 5493/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8100.9427 - val_loss: 7154.2310\n",
      "Epoch 5494/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8100.4788 - val_loss: 7153.7690\n",
      "Epoch 5495/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8100.0167 - val_loss: 7153.3066\n",
      "Epoch 5496/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8099.5536 - val_loss: 7152.8433\n",
      "Epoch 5497/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8099.0906 - val_loss: 7152.3818\n",
      "Epoch 5498/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8098.6300 - val_loss: 7151.9180\n",
      "Epoch 5499/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8098.1658 - val_loss: 7151.4556\n",
      "Epoch 5500/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8097.7026 - val_loss: 7150.9941\n",
      "Epoch 5501/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8097.2404 - val_loss: 7150.5293\n",
      "Epoch 5502/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 8096.7779 - val_loss: 7150.0664\n",
      "Epoch 5503/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 8096.3149 - val_loss: 7149.6045\n",
      "Epoch 5504/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8095.8514 - val_loss: 7149.1431\n",
      "Epoch 5505/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8095.3894 - val_loss: 7148.6782\n",
      "Epoch 5506/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8094.9275 - val_loss: 7148.2153\n",
      "Epoch 5507/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8094.4635 - val_loss: 7147.7544\n",
      "Epoch 5508/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8094.0012 - val_loss: 7147.2915\n",
      "Epoch 5509/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8093.5382 - val_loss: 7146.8281\n",
      "Epoch 5510/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8093.0769 - val_loss: 7146.3652\n",
      "Epoch 5511/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8092.6137 - val_loss: 7145.9023\n",
      "Epoch 5512/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8092.1501 - val_loss: 7145.4404\n",
      "Epoch 5513/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8091.6883 - val_loss: 7144.9766\n",
      "Epoch 5514/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8091.2250 - val_loss: 7144.5151\n",
      "Epoch 5515/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8090.7632 - val_loss: 7144.0513\n",
      "Epoch 5516/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8090.3002 - val_loss: 7143.5889\n",
      "Epoch 5517/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8089.8369 - val_loss: 7143.1274\n",
      "Epoch 5518/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8089.3737 - val_loss: 7142.6641\n",
      "Epoch 5519/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8088.9121 - val_loss: 7142.2012\n",
      "Epoch 5520/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8088.4488 - val_loss: 7141.7378\n",
      "Epoch 5521/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8087.9865 - val_loss: 7141.2764\n",
      "Epoch 5522/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8087.5234 - val_loss: 7140.8125\n",
      "Epoch 5523/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8087.0609 - val_loss: 7140.3496\n",
      "Epoch 5524/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8086.5982 - val_loss: 7139.8887\n",
      "Epoch 5525/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8086.1355 - val_loss: 7139.4248\n",
      "Epoch 5526/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 8085.6724 - val_loss: 7138.9629\n",
      "Epoch 5527/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8085.2105 - val_loss: 7138.5000\n",
      "Epoch 5528/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8084.7476 - val_loss: 7138.0376\n",
      "Epoch 5529/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8084.2853 - val_loss: 7137.5737\n",
      "Epoch 5530/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8083.8225 - val_loss: 7137.1113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5531/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8083.3591 - val_loss: 7136.6504\n",
      "Epoch 5532/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8082.8967 - val_loss: 7136.1860\n",
      "Epoch 5533/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8082.4343 - val_loss: 7135.7222\n",
      "Epoch 5534/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8081.9711 - val_loss: 7135.2607\n",
      "Epoch 5535/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8081.5078 - val_loss: 7134.7988\n",
      "Epoch 5536/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8081.0459 - val_loss: 7134.3359\n",
      "Epoch 5537/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8080.5838 - val_loss: 7133.8721\n",
      "Epoch 5538/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8080.1204 - val_loss: 7133.4097\n",
      "Epoch 5539/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8079.6580 - val_loss: 7132.9473\n",
      "Epoch 5540/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8079.1947 - val_loss: 7132.4844\n",
      "Epoch 5541/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8078.7326 - val_loss: 7132.0225\n",
      "Epoch 5542/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 8078.2706 - val_loss: 7131.5586\n",
      "Epoch 5543/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 8077.8075 - val_loss: 7131.0962\n",
      "Epoch 5544/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8077.3446 - val_loss: 7130.6348\n",
      "Epoch 5545/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8076.8815 - val_loss: 7130.1709\n",
      "Epoch 5546/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8076.4189 - val_loss: 7129.7080\n",
      "Epoch 5547/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8075.9564 - val_loss: 7129.2456\n",
      "Epoch 5548/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 8075.4932 - val_loss: 7128.7837\n",
      "Epoch 5549/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 8075.0303 - val_loss: 7128.3198\n",
      "Epoch 5550/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8074.5683 - val_loss: 7127.8579\n",
      "Epoch 5551/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8074.1052 - val_loss: 7127.3945\n",
      "Epoch 5552/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8073.6427 - val_loss: 7126.9321\n",
      "Epoch 5553/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8073.1797 - val_loss: 7126.4683\n",
      "Epoch 5554/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8072.7177 - val_loss: 7126.0059\n",
      "Epoch 5555/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8072.2546 - val_loss: 7125.5454\n",
      "Epoch 5556/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8071.7916 - val_loss: 7125.0815\n",
      "Epoch 5557/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8071.3297 - val_loss: 7124.6191\n",
      "Epoch 5558/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8070.8664 - val_loss: 7124.1558\n",
      "Epoch 5559/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8070.4039 - val_loss: 7123.6934\n",
      "Epoch 5560/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8069.9418 - val_loss: 7123.2305\n",
      "Epoch 5561/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8069.4781 - val_loss: 7122.7681\n",
      "Epoch 5562/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8069.0154 - val_loss: 7122.3042\n",
      "Epoch 5563/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8068.5530 - val_loss: 7121.8418\n",
      "Epoch 5564/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8068.0898 - val_loss: 7121.3789\n",
      "Epoch 5565/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8067.6281 - val_loss: 7120.9180\n",
      "Epoch 5566/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 8067.1642 - val_loss: 7120.4531\n",
      "Epoch 5567/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8066.7018 - val_loss: 7119.9917\n",
      "Epoch 5568/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8066.2400 - val_loss: 7119.5278\n",
      "Epoch 5569/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8065.7760 - val_loss: 7119.0654\n",
      "Epoch 5570/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8065.3139 - val_loss: 7118.6040\n",
      "Epoch 5571/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 8064.8506 - val_loss: 7118.1401\n",
      "Epoch 5572/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8064.3883 - val_loss: 7117.6782\n",
      "Epoch 5573/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8063.9270 - val_loss: 7117.2148\n",
      "Epoch 5574/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8063.4633 - val_loss: 7116.7529\n",
      "Epoch 5575/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8063.0002 - val_loss: 7116.2915\n",
      "Epoch 5576/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8062.5376 - val_loss: 7115.8267\n",
      "Epoch 5577/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8062.0750 - val_loss: 7115.3638\n",
      "Epoch 5578/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8061.6126 - val_loss: 7114.9014\n",
      "Epoch 5579/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8061.1488 - val_loss: 7114.4399\n",
      "Epoch 5580/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8060.6873 - val_loss: 7113.9766\n",
      "Epoch 5581/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8060.2245 - val_loss: 7113.5137\n",
      "Epoch 5582/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8059.7613 - val_loss: 7113.0527\n",
      "Epoch 5583/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8059.2991 - val_loss: 7112.5879\n",
      "Epoch 5584/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8058.8352 - val_loss: 7112.1250\n",
      "Epoch 5585/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8058.3740 - val_loss: 7111.6636\n",
      "Epoch 5586/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8057.9113 - val_loss: 7111.1997\n",
      "Epoch 5587/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8057.4477 - val_loss: 7110.7373\n",
      "Epoch 5588/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8056.9860 - val_loss: 7110.2754\n",
      "Epoch 5589/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8056.5224 - val_loss: 7109.8125\n",
      "Epoch 5590/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8056.0604 - val_loss: 7109.3477\n",
      "Epoch 5591/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8055.5978 - val_loss: 7108.8862\n",
      "Epoch 5592/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8055.1341 - val_loss: 7108.4248\n",
      "Epoch 5593/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8054.6712 - val_loss: 7107.9609\n",
      "Epoch 5594/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8054.2094 - val_loss: 7107.4985\n",
      "Epoch 5595/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8053.7460 - val_loss: 7107.0347\n",
      "Epoch 5596/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8053.2841 - val_loss: 7106.5732\n",
      "Epoch 5597/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8052.8204 - val_loss: 7106.1094\n",
      "Epoch 5598/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8052.3579 - val_loss: 7105.6475\n",
      "Epoch 5599/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8051.8961 - val_loss: 7105.1865\n",
      "Epoch 5600/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 8051.4325 - val_loss: 7104.7212\n",
      "Epoch 5601/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8050.9708 - val_loss: 7104.2598\n",
      "Epoch 5602/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 8050.5073 - val_loss: 7103.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5603/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8050.0447 - val_loss: 7103.3345\n",
      "Epoch 5604/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8049.5824 - val_loss: 7102.8711\n",
      "Epoch 5605/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 8049.1196 - val_loss: 7102.4082\n",
      "Epoch 5606/10000\n",
      "750/750 [==============================] - 0s 164us/step - loss: 8048.6560 - val_loss: 7101.9473\n",
      "Epoch 5607/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8048.1936 - val_loss: 7101.4829\n",
      "Epoch 5608/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 8047.7314 - val_loss: 7101.0205\n",
      "Epoch 5609/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8047.2680 - val_loss: 7100.5571\n",
      "Epoch 5610/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8046.8049 - val_loss: 7100.0957\n",
      "Epoch 5611/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8046.3434 - val_loss: 7099.6323\n",
      "Epoch 5612/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8045.8807 - val_loss: 7099.1685\n",
      "Epoch 5613/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8045.4176 - val_loss: 7098.7070\n",
      "Epoch 5614/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8044.9554 - val_loss: 7098.2441\n",
      "Epoch 5615/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8044.4917 - val_loss: 7097.7808\n",
      "Epoch 5616/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8044.0296 - val_loss: 7097.3198\n",
      "Epoch 5617/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8043.5677 - val_loss: 7096.8560\n",
      "Epoch 5618/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8043.1046 - val_loss: 7096.3945\n",
      "Epoch 5619/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 8042.6417 - val_loss: 7095.9316\n",
      "Epoch 5620/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8042.1785 - val_loss: 7095.4683\n",
      "Epoch 5621/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8041.7165 - val_loss: 7095.0044\n",
      "Epoch 5622/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8041.2535 - val_loss: 7094.5430\n",
      "Epoch 5623/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8040.7904 - val_loss: 7094.0815\n",
      "Epoch 5624/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 8040.3278 - val_loss: 7093.6167\n",
      "Epoch 5625/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8039.8655 - val_loss: 7093.1553\n",
      "Epoch 5626/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8039.4029 - val_loss: 7092.6919\n",
      "Epoch 5627/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8038.9396 - val_loss: 7092.2295\n",
      "Epoch 5628/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8038.4767 - val_loss: 7091.7681\n",
      "Epoch 5629/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8038.0145 - val_loss: 7091.3042\n",
      "Epoch 5630/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8037.5524 - val_loss: 7090.8418\n",
      "Epoch 5631/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8037.0887 - val_loss: 7090.3779\n",
      "Epoch 5632/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 8036.6267 - val_loss: 7089.9165\n",
      "Epoch 5633/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8036.1636 - val_loss: 7089.4556\n",
      "Epoch 5634/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8035.7012 - val_loss: 7088.9902\n",
      "Epoch 5635/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8035.2389 - val_loss: 7088.5273\n",
      "Epoch 5636/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8034.7751 - val_loss: 7088.0654\n",
      "Epoch 5637/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 8034.3124 - val_loss: 7087.6040\n",
      "Epoch 5638/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8033.8502 - val_loss: 7087.1387\n",
      "Epoch 5639/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8033.3873 - val_loss: 7086.6763\n",
      "Epoch 5640/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8032.9252 - val_loss: 7086.2148\n",
      "Epoch 5641/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8032.4618 - val_loss: 7085.7524\n",
      "Epoch 5642/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8031.9988 - val_loss: 7085.2891\n",
      "Epoch 5643/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8031.5373 - val_loss: 7084.8262\n",
      "Epoch 5644/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8031.0739 - val_loss: 7084.3628\n",
      "Epoch 5645/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8030.6112 - val_loss: 7083.9014\n",
      "Epoch 5646/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8030.1488 - val_loss: 7083.4375\n",
      "Epoch 5647/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8029.6858 - val_loss: 7082.9766\n",
      "Epoch 5648/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 8029.2239 - val_loss: 7082.5112\n",
      "Epoch 5649/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 8028.7602 - val_loss: 7082.0498\n",
      "Epoch 5650/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8028.2978 - val_loss: 7081.5889\n",
      "Epoch 5651/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8027.8348 - val_loss: 7081.1240\n",
      "Epoch 5652/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 8027.3724 - val_loss: 7080.6626\n",
      "Epoch 5653/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8026.9100 - val_loss: 7080.1982\n",
      "Epoch 5654/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 8026.4464 - val_loss: 7079.7373\n",
      "Epoch 5655/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 8025.9844 - val_loss: 7079.2734\n",
      "Epoch 5656/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8025.5216 - val_loss: 7078.8110\n",
      "Epoch 5657/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8025.0585 - val_loss: 7078.3496\n",
      "Epoch 5658/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8024.5963 - val_loss: 7077.8857\n",
      "Epoch 5659/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 8024.1328 - val_loss: 7077.4238\n",
      "Epoch 5660/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8023.6713 - val_loss: 7076.9600\n",
      "Epoch 5661/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8023.2085 - val_loss: 7076.4971\n",
      "Epoch 5662/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8022.7454 - val_loss: 7076.0347\n",
      "Epoch 5663/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8022.2833 - val_loss: 7075.5723\n",
      "Epoch 5664/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8021.8193 - val_loss: 7075.1113\n",
      "Epoch 5665/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8021.3576 - val_loss: 7074.6470\n",
      "Epoch 5666/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8020.8950 - val_loss: 7074.1836\n",
      "Epoch 5667/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 8020.4312 - val_loss: 7073.7212\n",
      "Epoch 5668/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8019.9694 - val_loss: 7073.2573\n",
      "Epoch 5669/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8019.5066 - val_loss: 7072.7959\n",
      "Epoch 5670/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8019.0433 - val_loss: 7072.3330\n",
      "Epoch 5671/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 8018.5810 - val_loss: 7071.8711\n",
      "Epoch 5672/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8018.1182 - val_loss: 7071.4082\n",
      "Epoch 5673/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 8017.6551 - val_loss: 7070.9448\n",
      "Epoch 5674/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8017.1932 - val_loss: 7070.4834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5675/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8016.7300 - val_loss: 7070.0195\n",
      "Epoch 5676/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8016.2682 - val_loss: 7069.5571\n",
      "Epoch 5677/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8015.8046 - val_loss: 7069.0933\n",
      "Epoch 5678/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 8015.3422 - val_loss: 7068.6318\n",
      "Epoch 5679/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8014.8806 - val_loss: 7068.1685\n",
      "Epoch 5680/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8014.4164 - val_loss: 7067.7065\n",
      "Epoch 5681/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8013.9533 - val_loss: 7067.2441\n",
      "Epoch 5682/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8013.4910 - val_loss: 7066.7793\n",
      "Epoch 5683/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8013.0286 - val_loss: 7066.3184\n",
      "Epoch 5684/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8012.5660 - val_loss: 7065.8555\n",
      "Epoch 5685/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 8012.1025 - val_loss: 7065.3931\n",
      "Epoch 5686/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8011.6403 - val_loss: 7064.9292\n",
      "Epoch 5687/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 8011.1777 - val_loss: 7064.4668\n",
      "Epoch 5688/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 8010.7148 - val_loss: 7064.0044\n",
      "Epoch 5689/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 8010.2532 - val_loss: 7063.5420\n",
      "Epoch 5690/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8009.7888 - val_loss: 7063.0781\n",
      "Epoch 5691/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8009.3268 - val_loss: 7062.6167\n",
      "Epoch 5692/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8008.8649 - val_loss: 7062.1528\n",
      "Epoch 5693/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 8008.4018 - val_loss: 7061.6914\n",
      "Epoch 5694/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 8007.9388 - val_loss: 7061.2290\n",
      "Epoch 5695/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 8007.4756 - val_loss: 7060.7651\n",
      "Epoch 5696/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 8007.0136 - val_loss: 7060.3027\n",
      "Epoch 5697/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 8006.5508 - val_loss: 7059.8398\n",
      "Epoch 5698/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 8006.0874 - val_loss: 7059.3789\n",
      "Epoch 5699/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8005.6256 - val_loss: 7058.9141\n",
      "Epoch 5700/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 8005.1628 - val_loss: 7058.4517\n",
      "Epoch 5701/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 8004.6999 - val_loss: 7057.9888\n",
      "Epoch 5702/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 8004.2372 - val_loss: 7057.5264\n",
      "Epoch 5703/10000\n",
      "750/750 [==============================] - 0s 174us/step - loss: 8003.7739 - val_loss: 7057.0649\n",
      "Epoch 5704/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 8003.3127 - val_loss: 7056.6016\n",
      "Epoch 5705/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 8002.8491 - val_loss: 7056.1387\n",
      "Epoch 5706/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 8002.3869 - val_loss: 7055.6748\n",
      "Epoch 5707/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 8001.9238 - val_loss: 7055.2139\n",
      "Epoch 5708/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 8001.4606 - val_loss: 7054.7524\n",
      "Epoch 5709/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 8000.9983 - val_loss: 7054.2876\n",
      "Epoch 5710/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 8000.5358 - val_loss: 7053.8247\n",
      "Epoch 5711/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 8000.0724 - val_loss: 7053.3623\n",
      "Epoch 5712/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7999.6096 - val_loss: 7052.9004\n",
      "Epoch 5713/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7999.1474 - val_loss: 7052.4375\n",
      "Epoch 5714/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 7998.6847 - val_loss: 7051.9746\n",
      "Epoch 5715/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7998.2227 - val_loss: 7051.5122\n",
      "Epoch 5716/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7997.7590 - val_loss: 7051.0488\n",
      "Epoch 5717/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7997.2964 - val_loss: 7050.5859\n",
      "Epoch 5718/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7996.8344 - val_loss: 7050.1235\n",
      "Epoch 5719/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7996.3710 - val_loss: 7049.6597\n",
      "Epoch 5720/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7995.9094 - val_loss: 7049.1982\n",
      "Epoch 5721/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7995.4460 - val_loss: 7048.7344\n",
      "Epoch 5722/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7994.9829 - val_loss: 7048.2734\n",
      "Epoch 5723/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7994.5211 - val_loss: 7047.8096\n",
      "Epoch 5724/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7994.0580 - val_loss: 7047.3472\n",
      "Epoch 5725/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7993.5947 - val_loss: 7046.8857\n",
      "Epoch 5726/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7993.1318 - val_loss: 7046.4219\n",
      "Epoch 5727/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7992.6698 - val_loss: 7045.9595\n",
      "Epoch 5728/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7992.2073 - val_loss: 7045.4961\n",
      "Epoch 5729/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 7991.7442 - val_loss: 7045.0337\n",
      "Epoch 5730/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 7991.2818 - val_loss: 7044.5698\n",
      "Epoch 5731/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7990.8187 - val_loss: 7044.1079\n",
      "Epoch 5732/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7990.3564 - val_loss: 7043.6470\n",
      "Epoch 5733/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7989.8942 - val_loss: 7043.1821\n",
      "Epoch 5734/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7989.4301 - val_loss: 7042.7207\n",
      "Epoch 5735/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7988.9684 - val_loss: 7042.2573\n",
      "Epoch 5736/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 7988.5056 - val_loss: 7041.7954\n",
      "Epoch 5737/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7988.0431 - val_loss: 7041.3320\n",
      "Epoch 5738/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7987.5803 - val_loss: 7040.8691\n",
      "Epoch 5739/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7987.1165 - val_loss: 7040.4082\n",
      "Epoch 5740/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7986.6548 - val_loss: 7039.9434\n",
      "Epoch 5741/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7986.1923 - val_loss: 7039.4810\n",
      "Epoch 5742/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7985.7286 - val_loss: 7039.0190\n",
      "Epoch 5743/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7985.2663 - val_loss: 7038.5566\n",
      "Epoch 5744/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7984.8038 - val_loss: 7038.0933\n",
      "Epoch 5745/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 7984.3414 - val_loss: 7037.6294\n",
      "Epoch 5746/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7983.8783 - val_loss: 7037.1680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5747/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7983.4157 - val_loss: 7036.7056\n",
      "Epoch 5748/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7982.9528 - val_loss: 7036.2417\n",
      "Epoch 5749/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7982.4904 - val_loss: 7035.7808\n",
      "Epoch 5750/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7982.0279 - val_loss: 7035.3164\n",
      "Epoch 5751/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7981.5654 - val_loss: 7034.8545\n",
      "Epoch 5752/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7981.1017 - val_loss: 7034.3931\n",
      "Epoch 5753/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7980.6392 - val_loss: 7033.9292\n",
      "Epoch 5754/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7980.1775 - val_loss: 7033.4653\n",
      "Epoch 5755/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7979.7137 - val_loss: 7033.0039\n",
      "Epoch 5756/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7979.2511 - val_loss: 7032.5420\n",
      "Epoch 5757/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7978.7883 - val_loss: 7032.0776\n",
      "Epoch 5758/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7978.3257 - val_loss: 7031.6152\n",
      "Epoch 5759/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7977.8639 - val_loss: 7031.1523\n",
      "Epoch 5760/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 7977.4001 - val_loss: 7030.6904\n",
      "Epoch 5761/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 7976.9375 - val_loss: 7030.2290\n",
      "Epoch 5762/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7976.4752 - val_loss: 7029.7637\n",
      "Epoch 5763/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7976.0126 - val_loss: 7029.3027\n",
      "Epoch 5764/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7975.5509 - val_loss: 7028.8389\n",
      "Epoch 5765/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7975.0868 - val_loss: 7028.3774\n",
      "Epoch 5766/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7974.6245 - val_loss: 7027.9141\n",
      "Epoch 5767/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7974.1624 - val_loss: 7027.4497\n",
      "Epoch 5768/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7973.6989 - val_loss: 7026.9878\n",
      "Epoch 5769/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7973.2364 - val_loss: 7026.5264\n",
      "Epoch 5770/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7972.7727 - val_loss: 7026.0625\n",
      "Epoch 5771/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7972.3108 - val_loss: 7025.5996\n",
      "Epoch 5772/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7971.8483 - val_loss: 7025.1372\n",
      "Epoch 5773/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7971.3846 - val_loss: 7024.6753\n",
      "Epoch 5774/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7970.9228 - val_loss: 7024.2109\n",
      "Epoch 5775/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 7970.4598 - val_loss: 7023.7500\n",
      "Epoch 5776/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7969.9979 - val_loss: 7023.2876\n",
      "Epoch 5777/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7969.5348 - val_loss: 7022.8237\n",
      "Epoch 5778/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7969.0716 - val_loss: 7022.3613\n",
      "Epoch 5779/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7968.6096 - val_loss: 7021.8984\n",
      "Epoch 5780/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7968.1466 - val_loss: 7021.4365\n",
      "Epoch 5781/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 7967.6842 - val_loss: 7020.9722\n",
      "Epoch 5782/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 7967.2219 - val_loss: 7020.5107\n",
      "Epoch 5783/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7966.7577 - val_loss: 7020.0488\n",
      "Epoch 5784/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7966.2955 - val_loss: 7019.5850\n",
      "Epoch 5785/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7965.8336 - val_loss: 7019.1221\n",
      "Epoch 5786/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7965.3698 - val_loss: 7018.6587\n",
      "Epoch 5787/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7964.9073 - val_loss: 7018.1973\n",
      "Epoch 5788/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7964.4447 - val_loss: 7017.7344\n",
      "Epoch 5789/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 7963.9816 - val_loss: 7017.2720\n",
      "Epoch 5790/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7963.5197 - val_loss: 7016.8096\n",
      "Epoch 5791/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7963.0567 - val_loss: 7016.3457\n",
      "Epoch 5792/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7962.5937 - val_loss: 7015.8848\n",
      "Epoch 5793/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7962.1316 - val_loss: 7015.4219\n",
      "Epoch 5794/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 7961.6688 - val_loss: 7014.9580\n",
      "Epoch 5795/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7961.2070 - val_loss: 7014.4956\n",
      "Epoch 5796/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7960.7430 - val_loss: 7014.0332\n",
      "Epoch 5797/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7960.2803 - val_loss: 7013.5698\n",
      "Epoch 5798/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7959.8183 - val_loss: 7013.1060\n",
      "Epoch 5799/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7959.3551 - val_loss: 7012.6445\n",
      "Epoch 5800/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7958.8925 - val_loss: 7012.1821\n",
      "Epoch 5801/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7958.4291 - val_loss: 7011.7183\n",
      "Epoch 5802/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7957.9672 - val_loss: 7011.2559\n",
      "Epoch 5803/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7957.5046 - val_loss: 7010.7930\n",
      "Epoch 5804/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 7957.0416 - val_loss: 7010.3315\n",
      "Epoch 5805/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7956.5789 - val_loss: 7009.8667\n",
      "Epoch 5806/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7956.1163 - val_loss: 7009.4058\n",
      "Epoch 5807/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7955.6532 - val_loss: 7008.9443\n",
      "Epoch 5808/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7955.1919 - val_loss: 7008.4795\n",
      "Epoch 5809/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7954.7278 - val_loss: 7008.0190\n",
      "Epoch 5810/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7954.2652 - val_loss: 7007.5542\n",
      "Epoch 5811/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7953.8029 - val_loss: 7007.0918\n",
      "Epoch 5812/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7953.3404 - val_loss: 7006.6294\n",
      "Epoch 5813/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7952.8773 - val_loss: 7006.1670\n",
      "Epoch 5814/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7952.4137 - val_loss: 7005.7056\n",
      "Epoch 5815/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7951.9521 - val_loss: 7005.2402\n",
      "Epoch 5816/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7951.4895 - val_loss: 7004.7793\n",
      "Epoch 5817/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7951.0262 - val_loss: 7004.3164\n",
      "Epoch 5818/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7950.5635 - val_loss: 7003.8540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5819/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7950.1007 - val_loss: 7003.3901\n",
      "Epoch 5820/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7949.6384 - val_loss: 7002.9277\n",
      "Epoch 5821/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7949.1761 - val_loss: 7002.4648\n",
      "Epoch 5822/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7948.7126 - val_loss: 7002.0024\n",
      "Epoch 5823/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7948.2501 - val_loss: 7001.5415\n",
      "Epoch 5824/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7947.7875 - val_loss: 7001.0781\n",
      "Epoch 5825/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7947.3256 - val_loss: 7000.6128\n",
      "Epoch 5826/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7946.8626 - val_loss: 7000.1514\n",
      "Epoch 5827/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7946.3995 - val_loss: 6999.6899\n",
      "Epoch 5828/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7945.9363 - val_loss: 6999.2266\n",
      "Epoch 5829/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7945.4745 - val_loss: 6998.7622\n",
      "Epoch 5830/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7945.0113 - val_loss: 6998.3003\n",
      "Epoch 5831/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7944.5481 - val_loss: 6997.8389\n",
      "Epoch 5832/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7944.0853 - val_loss: 6997.3750\n",
      "Epoch 5833/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7943.6234 - val_loss: 6996.9136\n",
      "Epoch 5834/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7943.1615 - val_loss: 6996.4497\n",
      "Epoch 5835/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7942.6973 - val_loss: 6995.9873\n",
      "Epoch 5836/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7942.2353 - val_loss: 6995.5254\n",
      "Epoch 5837/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7941.7724 - val_loss: 6995.0625\n",
      "Epoch 5838/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7941.3095 - val_loss: 6994.5996\n",
      "Epoch 5839/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7940.8479 - val_loss: 6994.1357\n",
      "Epoch 5840/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7940.3844 - val_loss: 6993.6738\n",
      "Epoch 5841/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7939.9215 - val_loss: 6993.2129\n",
      "Epoch 5842/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7939.4594 - val_loss: 6992.7485\n",
      "Epoch 5843/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7938.9961 - val_loss: 6992.2852\n",
      "Epoch 5844/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7938.5335 - val_loss: 6991.8232\n",
      "Epoch 5845/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7938.0699 - val_loss: 6991.3613\n",
      "Epoch 5846/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7937.6081 - val_loss: 6990.8984\n",
      "Epoch 5847/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7937.1463 - val_loss: 6990.4346\n",
      "Epoch 5848/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7936.6825 - val_loss: 6989.9722\n",
      "Epoch 5849/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7936.2197 - val_loss: 6989.5098\n",
      "Epoch 5850/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7935.7568 - val_loss: 6989.0469\n",
      "Epoch 5851/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7935.2953 - val_loss: 6988.5845\n",
      "Epoch 5852/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7934.8325 - val_loss: 6988.1211\n",
      "Epoch 5853/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7934.3690 - val_loss: 6987.6587\n",
      "Epoch 5854/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7933.9069 - val_loss: 6987.1948\n",
      "Epoch 5855/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7933.4435 - val_loss: 6986.7344\n",
      "Epoch 5856/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7932.9820 - val_loss: 6986.2695\n",
      "Epoch 5857/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7932.5188 - val_loss: 6985.8071\n",
      "Epoch 5858/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7932.0550 - val_loss: 6985.3462\n",
      "Epoch 5859/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7931.5928 - val_loss: 6984.8823\n",
      "Epoch 5860/10000\n",
      "750/750 [==============================] - 0s 165us/step - loss: 7931.1307 - val_loss: 6984.4204\n",
      "Epoch 5861/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 7930.6677 - val_loss: 6983.9570\n",
      "Epoch 5862/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7930.2050 - val_loss: 6983.4941\n",
      "Epoch 5863/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7929.7415 - val_loss: 6983.0308\n",
      "Epoch 5864/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7929.2794 - val_loss: 6982.5693\n",
      "Epoch 5865/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7928.8177 - val_loss: 6982.1079\n",
      "Epoch 5866/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7928.3542 - val_loss: 6981.6440\n",
      "Epoch 5867/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7927.8907 - val_loss: 6981.1816\n",
      "Epoch 5868/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7927.4286 - val_loss: 6980.7183\n",
      "Epoch 5869/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7926.9658 - val_loss: 6980.2544\n",
      "Epoch 5870/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7926.5039 - val_loss: 6979.7930\n",
      "Epoch 5871/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7926.0406 - val_loss: 6979.3306\n",
      "Epoch 5872/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7925.5774 - val_loss: 6978.8691\n",
      "Epoch 5873/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7925.1156 - val_loss: 6978.4043\n",
      "Epoch 5874/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7924.6523 - val_loss: 6977.9414\n",
      "Epoch 5875/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7924.1896 - val_loss: 6977.4795\n",
      "Epoch 5876/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7923.7261 - val_loss: 6977.0151\n",
      "Epoch 5877/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7923.2642 - val_loss: 6976.5542\n",
      "Epoch 5878/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7922.8022 - val_loss: 6976.0903\n",
      "Epoch 5879/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7922.3390 - val_loss: 6975.6289\n",
      "Epoch 5880/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7921.8763 - val_loss: 6975.1665\n",
      "Epoch 5881/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7921.4131 - val_loss: 6974.7031\n",
      "Epoch 5882/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7920.9511 - val_loss: 6974.2407\n",
      "Epoch 5883/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7920.4895 - val_loss: 6973.7773\n",
      "Epoch 5884/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7920.0254 - val_loss: 6973.3154\n",
      "Epoch 5885/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7919.5631 - val_loss: 6972.8540\n",
      "Epoch 5886/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7919.1000 - val_loss: 6972.3887\n",
      "Epoch 5887/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7918.6379 - val_loss: 6971.9263\n",
      "Epoch 5888/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7918.1745 - val_loss: 6971.4639\n",
      "Epoch 5889/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7917.7110 - val_loss: 6971.0029\n",
      "Epoch 5890/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7917.2489 - val_loss: 6970.5391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5891/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7916.7869 - val_loss: 6970.0762\n",
      "Epoch 5892/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7916.3232 - val_loss: 6969.6138\n",
      "Epoch 5893/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7915.8612 - val_loss: 6969.1504\n",
      "Epoch 5894/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7915.3981 - val_loss: 6968.6875\n",
      "Epoch 5895/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7914.9358 - val_loss: 6968.2246\n",
      "Epoch 5896/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7914.4731 - val_loss: 6967.7622\n",
      "Epoch 5897/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7914.0103 - val_loss: 6967.2998\n",
      "Epoch 5898/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7913.5481 - val_loss: 6966.8379\n",
      "Epoch 5899/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7913.0847 - val_loss: 6966.3750\n",
      "Epoch 5900/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7912.6230 - val_loss: 6965.9102\n",
      "Epoch 5901/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7912.1597 - val_loss: 6965.4487\n",
      "Epoch 5902/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7911.6964 - val_loss: 6964.9863\n",
      "Epoch 5903/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7911.2334 - val_loss: 6964.5234\n",
      "Epoch 5904/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7910.7716 - val_loss: 6964.0610\n",
      "Epoch 5905/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7910.3086 - val_loss: 6963.5972\n",
      "Epoch 5906/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7909.8454 - val_loss: 6963.1362\n",
      "Epoch 5907/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7909.3824 - val_loss: 6962.6719\n",
      "Epoch 5908/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7908.9203 - val_loss: 6962.2100\n",
      "Epoch 5909/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7908.4584 - val_loss: 6961.7471\n",
      "Epoch 5910/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7907.9947 - val_loss: 6961.2837\n",
      "Epoch 5911/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7907.5332 - val_loss: 6960.8223\n",
      "Epoch 5912/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7907.0694 - val_loss: 6960.3594\n",
      "Epoch 5913/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7906.6072 - val_loss: 6959.8970\n",
      "Epoch 5914/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7906.1450 - val_loss: 6959.4321\n",
      "Epoch 5915/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7905.6816 - val_loss: 6958.9712\n",
      "Epoch 5916/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7905.2183 - val_loss: 6958.5098\n",
      "Epoch 5917/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7904.7563 - val_loss: 6958.0454\n",
      "Epoch 5918/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7904.2939 - val_loss: 6957.5830\n",
      "Epoch 5919/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7903.8304 - val_loss: 6957.1206\n",
      "Epoch 5920/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7903.3673 - val_loss: 6956.6582\n",
      "Epoch 5921/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7902.9056 - val_loss: 6956.1948\n",
      "Epoch 5922/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7902.4433 - val_loss: 6955.7329\n",
      "Epoch 5923/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7901.9801 - val_loss: 6955.2705\n",
      "Epoch 5924/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 7901.5176 - val_loss: 6954.8066\n",
      "Epoch 5925/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7901.0542 - val_loss: 6954.3433\n",
      "Epoch 5926/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7900.5924 - val_loss: 6953.8823\n",
      "Epoch 5927/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7900.1300 - val_loss: 6953.4180\n",
      "Epoch 5928/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7899.6666 - val_loss: 6952.9565\n",
      "Epoch 5929/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7899.2038 - val_loss: 6952.4941\n",
      "Epoch 5930/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7898.7410 - val_loss: 6952.0308\n",
      "Epoch 5931/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7898.2793 - val_loss: 6951.5669\n",
      "Epoch 5932/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7897.8159 - val_loss: 6951.1045\n",
      "Epoch 5933/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7897.3519 - val_loss: 6950.6440\n",
      "Epoch 5934/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7896.8899 - val_loss: 6950.1792\n",
      "Epoch 5935/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7896.4279 - val_loss: 6949.7168\n",
      "Epoch 5936/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7895.9648 - val_loss: 6949.2539\n",
      "Epoch 5937/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7895.5021 - val_loss: 6948.7920\n",
      "Epoch 5938/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7895.0390 - val_loss: 6948.3281\n",
      "Epoch 5939/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7894.5767 - val_loss: 6947.8657\n",
      "Epoch 5940/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7894.1149 - val_loss: 6947.4043\n",
      "Epoch 5941/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7893.6512 - val_loss: 6946.9404\n",
      "Epoch 5942/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7893.1894 - val_loss: 6946.4790\n",
      "Epoch 5943/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7892.7262 - val_loss: 6946.0151\n",
      "Epoch 5944/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7892.2637 - val_loss: 6945.5527\n",
      "Epoch 5945/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7891.8014 - val_loss: 6945.0898\n",
      "Epoch 5946/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7891.3375 - val_loss: 6944.6279\n",
      "Epoch 5947/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7890.8746 - val_loss: 6944.1665\n",
      "Epoch 5948/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7890.4127 - val_loss: 6943.7012\n",
      "Epoch 5949/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7889.9503 - val_loss: 6943.2388\n",
      "Epoch 5950/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7889.4868 - val_loss: 6942.7773\n",
      "Epoch 5951/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7889.0238 - val_loss: 6942.3149\n",
      "Epoch 5952/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7888.5615 - val_loss: 6941.8516\n",
      "Epoch 5953/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7888.0994 - val_loss: 6941.3887\n",
      "Epoch 5954/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7887.6363 - val_loss: 6940.9253\n",
      "Epoch 5955/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7887.1741 - val_loss: 6940.4629\n",
      "Epoch 5956/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7886.7102 - val_loss: 6940.0000\n",
      "Epoch 5957/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7886.2484 - val_loss: 6939.5391\n",
      "Epoch 5958/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7885.7864 - val_loss: 6939.0737\n",
      "Epoch 5959/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7885.3231 - val_loss: 6938.6123\n",
      "Epoch 5960/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7884.8603 - val_loss: 6938.1504\n",
      "Epoch 5961/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7884.3972 - val_loss: 6937.6875\n",
      "Epoch 5962/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7883.9348 - val_loss: 6937.2227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5963/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7883.4723 - val_loss: 6936.7612\n",
      "Epoch 5964/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7883.0082 - val_loss: 6936.2998\n",
      "Epoch 5965/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7882.5466 - val_loss: 6935.8359\n",
      "Epoch 5966/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7882.0841 - val_loss: 6935.3735\n",
      "Epoch 5967/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7881.6210 - val_loss: 6934.9102\n",
      "Epoch 5968/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7881.1584 - val_loss: 6934.4482\n",
      "Epoch 5969/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7880.6956 - val_loss: 6933.9863\n",
      "Epoch 5970/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7880.2331 - val_loss: 6933.5234\n",
      "Epoch 5971/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7879.7712 - val_loss: 6933.0596\n",
      "Epoch 5972/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7879.3084 - val_loss: 6932.5962\n",
      "Epoch 5973/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7878.8452 - val_loss: 6932.1348\n",
      "Epoch 5974/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7878.3815 - val_loss: 6931.6738\n",
      "Epoch 5975/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7877.9203 - val_loss: 6931.2095\n",
      "Epoch 5976/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7877.4576 - val_loss: 6930.7461\n",
      "Epoch 5977/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7876.9935 - val_loss: 6930.2837\n",
      "Epoch 5978/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7876.5309 - val_loss: 6929.8198\n",
      "Epoch 5979/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7876.0687 - val_loss: 6929.3579\n",
      "Epoch 5980/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7875.6057 - val_loss: 6928.8955\n",
      "Epoch 5981/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7875.1436 - val_loss: 6928.4336\n",
      "Epoch 5982/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7874.6800 - val_loss: 6927.9683\n",
      "Epoch 5983/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7874.2181 - val_loss: 6927.5073\n",
      "Epoch 5984/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7873.7558 - val_loss: 6927.0454\n",
      "Epoch 5985/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7873.2928 - val_loss: 6926.5820\n",
      "Epoch 5986/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7872.8307 - val_loss: 6926.1191\n",
      "Epoch 5987/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7872.3670 - val_loss: 6925.6558\n",
      "Epoch 5988/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7871.9043 - val_loss: 6925.1943\n",
      "Epoch 5989/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 7871.4432 - val_loss: 6924.7305\n",
      "Epoch 5990/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7870.9790 - val_loss: 6924.2690\n",
      "Epoch 5991/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7870.5154 - val_loss: 6923.8071\n",
      "Epoch 5992/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7870.0535 - val_loss: 6923.3428\n",
      "Epoch 5993/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7869.5908 - val_loss: 6922.8809\n",
      "Epoch 5994/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7869.1284 - val_loss: 6922.4180\n",
      "Epoch 5995/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7868.6643 - val_loss: 6921.9556\n",
      "Epoch 5996/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7868.2028 - val_loss: 6921.4917\n",
      "Epoch 5997/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7867.7403 - val_loss: 6921.0293\n",
      "Epoch 5998/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7867.2771 - val_loss: 6920.5684\n",
      "Epoch 5999/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7866.8148 - val_loss: 6920.1045\n",
      "Epoch 6000/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7866.3515 - val_loss: 6919.6431\n",
      "Epoch 6001/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 7865.8896 - val_loss: 6919.1792\n",
      "Epoch 6002/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7865.4271 - val_loss: 6918.7153\n",
      "Epoch 6003/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7864.9643 - val_loss: 6918.2529\n",
      "Epoch 6004/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7864.5011 - val_loss: 6917.7915\n",
      "Epoch 6005/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7864.0378 - val_loss: 6917.3281\n",
      "Epoch 6006/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7863.5762 - val_loss: 6916.8638\n",
      "Epoch 6007/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7863.1136 - val_loss: 6916.4023\n",
      "Epoch 6008/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7862.6497 - val_loss: 6915.9404\n",
      "Epoch 6009/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7862.1873 - val_loss: 6915.4766\n",
      "Epoch 6010/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7861.7251 - val_loss: 6915.0137\n",
      "Epoch 6011/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7861.2618 - val_loss: 6914.5513\n",
      "Epoch 6012/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7860.8000 - val_loss: 6914.0889\n",
      "Epoch 6013/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7860.3360 - val_loss: 6913.6274\n",
      "Epoch 6014/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7859.8740 - val_loss: 6913.1641\n",
      "Epoch 6015/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7859.4119 - val_loss: 6912.7017\n",
      "Epoch 6016/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7858.9486 - val_loss: 6912.2373\n",
      "Epoch 6017/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7858.4869 - val_loss: 6911.7754\n",
      "Epoch 6018/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7858.0230 - val_loss: 6911.3125\n",
      "Epoch 6019/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7857.5606 - val_loss: 6910.8496\n",
      "Epoch 6020/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7857.0985 - val_loss: 6910.3872\n",
      "Epoch 6021/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7856.6351 - val_loss: 6909.9248\n",
      "Epoch 6022/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7856.1716 - val_loss: 6909.4629\n",
      "Epoch 6023/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7855.7099 - val_loss: 6908.9985\n",
      "Epoch 6024/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7855.2471 - val_loss: 6908.5352\n",
      "Epoch 6025/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7854.7840 - val_loss: 6908.0737\n",
      "Epoch 6026/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7854.3211 - val_loss: 6907.6113\n",
      "Epoch 6027/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7853.8587 - val_loss: 6907.1484\n",
      "Epoch 6028/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7853.3966 - val_loss: 6906.6860\n",
      "Epoch 6029/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7852.9335 - val_loss: 6906.2222\n",
      "Epoch 6030/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7852.4723 - val_loss: 6905.7607\n",
      "Epoch 6031/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7852.0081 - val_loss: 6905.2969\n",
      "Epoch 6032/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7851.5455 - val_loss: 6904.8359\n",
      "Epoch 6033/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7851.0835 - val_loss: 6904.3721\n",
      "Epoch 6034/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7850.6206 - val_loss: 6903.9087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6035/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7850.1572 - val_loss: 6903.4473\n",
      "Epoch 6036/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7849.6941 - val_loss: 6902.9844\n",
      "Epoch 6037/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7849.2325 - val_loss: 6902.5220\n",
      "Epoch 6038/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7848.7694 - val_loss: 6902.0586\n",
      "Epoch 6039/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7848.3060 - val_loss: 6901.5962\n",
      "Epoch 6040/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7847.8436 - val_loss: 6901.1323\n",
      "Epoch 6041/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7847.3811 - val_loss: 6900.6704\n",
      "Epoch 6042/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7846.9186 - val_loss: 6900.2095\n",
      "Epoch 6043/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 7846.4558 - val_loss: 6899.7456\n",
      "Epoch 6044/10000\n",
      "750/750 [==============================] - 0s 164us/step - loss: 7845.9925 - val_loss: 6899.2832\n",
      "Epoch 6045/10000\n",
      "750/750 [==============================] - 0s 163us/step - loss: 7845.5307 - val_loss: 6898.8198\n",
      "Epoch 6046/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7845.0679 - val_loss: 6898.3579\n",
      "Epoch 6047/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 7844.6059 - val_loss: 6897.8940\n",
      "Epoch 6048/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7844.1425 - val_loss: 6897.4316\n",
      "Epoch 6049/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 7843.6792 - val_loss: 6896.9707\n",
      "Epoch 6050/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 7843.2170 - val_loss: 6896.5059\n",
      "Epoch 6051/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 7842.7547 - val_loss: 6896.0430\n",
      "Epoch 6052/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 7842.2907 - val_loss: 6895.5815\n",
      "Epoch 6053/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 7841.8282 - val_loss: 6895.1191\n",
      "Epoch 6054/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7841.3661 - val_loss: 6894.6558\n",
      "Epoch 6055/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7840.9034 - val_loss: 6894.1919\n",
      "Epoch 6056/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7840.4407 - val_loss: 6893.7305\n",
      "Epoch 6057/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7839.9774 - val_loss: 6893.2681\n",
      "Epoch 6058/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7839.5155 - val_loss: 6892.8042\n",
      "Epoch 6059/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7839.0531 - val_loss: 6892.3418\n",
      "Epoch 6060/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7838.5896 - val_loss: 6891.8789\n",
      "Epoch 6061/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7838.1280 - val_loss: 6891.4170\n",
      "Epoch 6062/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 7837.6646 - val_loss: 6890.9531\n",
      "Epoch 6063/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7837.2015 - val_loss: 6890.4917\n",
      "Epoch 6064/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7836.7402 - val_loss: 6890.0273\n",
      "Epoch 6065/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7836.2761 - val_loss: 6889.5654\n",
      "Epoch 6066/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7835.8134 - val_loss: 6889.1045\n",
      "Epoch 6067/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7835.3507 - val_loss: 6888.6401\n",
      "Epoch 6068/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7834.8880 - val_loss: 6888.1777\n",
      "Epoch 6069/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7834.4258 - val_loss: 6887.7148\n",
      "Epoch 6070/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7833.9623 - val_loss: 6887.2524\n",
      "Epoch 6071/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7833.4998 - val_loss: 6886.7891\n",
      "Epoch 6072/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7833.0377 - val_loss: 6886.3262\n",
      "Epoch 6073/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7832.5750 - val_loss: 6885.8652\n",
      "Epoch 6074/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7832.1127 - val_loss: 6885.4014\n",
      "Epoch 6075/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7831.6493 - val_loss: 6884.9399\n",
      "Epoch 6076/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7831.1868 - val_loss: 6884.4766\n",
      "Epoch 6077/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7830.7243 - val_loss: 6884.0122\n",
      "Epoch 6078/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7830.2616 - val_loss: 6883.5503\n",
      "Epoch 6079/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7829.7990 - val_loss: 6883.0889\n",
      "Epoch 6080/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7829.3350 - val_loss: 6882.6274\n",
      "Epoch 6081/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7828.8736 - val_loss: 6882.1626\n",
      "Epoch 6082/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7828.4109 - val_loss: 6881.6987\n",
      "Epoch 6083/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7827.9468 - val_loss: 6881.2378\n",
      "Epoch 6084/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7827.4851 - val_loss: 6880.7734\n",
      "Epoch 6085/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7827.0223 - val_loss: 6880.3125\n",
      "Epoch 6086/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7826.5589 - val_loss: 6879.8496\n",
      "Epoch 6087/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7826.0973 - val_loss: 6879.3862\n",
      "Epoch 6088/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7825.6337 - val_loss: 6878.9238\n",
      "Epoch 6089/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7825.1721 - val_loss: 6878.4609\n",
      "Epoch 6090/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7824.7091 - val_loss: 6877.9990\n",
      "Epoch 6091/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7824.2459 - val_loss: 6877.5347\n",
      "Epoch 6092/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7823.7840 - val_loss: 6877.0732\n",
      "Epoch 6093/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7823.3203 - val_loss: 6876.6113\n",
      "Epoch 6094/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7822.8578 - val_loss: 6876.1470\n",
      "Epoch 6095/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7822.3958 - val_loss: 6875.6846\n",
      "Epoch 6096/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7821.9321 - val_loss: 6875.2212\n",
      "Epoch 6097/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7821.4695 - val_loss: 6874.7607\n",
      "Epoch 6098/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7821.0070 - val_loss: 6874.2969\n",
      "Epoch 6099/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7820.5442 - val_loss: 6873.8345\n",
      "Epoch 6100/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 7820.74 - 0s 121us/step - loss: 7820.0824 - val_loss: 6873.3721\n",
      "Epoch 6101/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7819.6184 - val_loss: 6872.9082\n",
      "Epoch 6102/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7819.1564 - val_loss: 6872.4448\n",
      "Epoch 6103/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7818.6939 - val_loss: 6871.9829\n",
      "Epoch 6104/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7818.2306 - val_loss: 6871.5205\n",
      "Epoch 6105/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7817.7693 - val_loss: 6871.0571\n",
      "Epoch 6106/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7817.3053 - val_loss: 6870.5957\n",
      "Epoch 6107/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7816.8430 - val_loss: 6870.1348\n",
      "Epoch 6108/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7816.3809 - val_loss: 6869.6685\n",
      "Epoch 6109/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7815.9177 - val_loss: 6869.2065\n",
      "Epoch 6110/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7815.4542 - val_loss: 6868.7446\n",
      "Epoch 6111/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7814.9913 - val_loss: 6868.2808\n",
      "Epoch 6112/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7814.5294 - val_loss: 6867.8184\n",
      "Epoch 6113/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7814.0672 - val_loss: 6867.3555\n",
      "Epoch 6114/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7813.6029 - val_loss: 6866.8945\n",
      "Epoch 6115/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7813.1406 - val_loss: 6866.4292\n",
      "Epoch 6116/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7812.6786 - val_loss: 6865.9683\n",
      "Epoch 6117/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7812.2155 - val_loss: 6865.5059\n",
      "Epoch 6118/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7811.7534 - val_loss: 6865.0420\n",
      "Epoch 6119/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7811.2901 - val_loss: 6864.5806\n",
      "Epoch 6120/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7810.8282 - val_loss: 6864.1167\n",
      "Epoch 6121/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7810.3651 - val_loss: 6863.6543\n",
      "Epoch 6122/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7809.9031 - val_loss: 6863.1904\n",
      "Epoch 6123/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7809.4398 - val_loss: 6862.7295\n",
      "Epoch 6124/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7808.9763 - val_loss: 6862.2681\n",
      "Epoch 6125/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7808.5141 - val_loss: 6861.8027\n",
      "Epoch 6126/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7808.0518 - val_loss: 6861.3403\n",
      "Epoch 6127/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7807.5886 - val_loss: 6860.8789\n",
      "Epoch 6128/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7807.1251 - val_loss: 6860.4165\n",
      "Epoch 6129/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7806.6632 - val_loss: 6859.9526\n",
      "Epoch 6130/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7806.2003 - val_loss: 6859.4902\n",
      "Epoch 6131/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7805.7385 - val_loss: 6859.0278\n",
      "Epoch 6132/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7805.2746 - val_loss: 6858.5649\n",
      "Epoch 6133/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7804.8131 - val_loss: 6858.1016\n",
      "Epoch 6134/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7804.3499 - val_loss: 6857.6401\n",
      "Epoch 6135/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7803.8866 - val_loss: 6857.1753\n",
      "Epoch 6136/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7803.4252 - val_loss: 6856.7139\n",
      "Epoch 6137/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7802.9616 - val_loss: 6856.2524\n",
      "Epoch 6138/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7802.4988 - val_loss: 6855.7891\n",
      "Epoch 6139/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7802.0370 - val_loss: 6855.3247\n",
      "Epoch 6140/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7801.5738 - val_loss: 6854.8623\n",
      "Epoch 6141/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7801.1103 - val_loss: 6854.4014\n",
      "Epoch 6142/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7800.6478 - val_loss: 6853.9375\n",
      "Epoch 6143/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7800.1857 - val_loss: 6853.4746\n",
      "Epoch 6144/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7799.7233 - val_loss: 6853.0122\n",
      "Epoch 6145/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7799.2597 - val_loss: 6852.5498\n",
      "Epoch 6146/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7798.7967 - val_loss: 6852.0859\n",
      "Epoch 6147/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7798.3349 - val_loss: 6851.6240\n",
      "Epoch 6148/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7797.8720 - val_loss: 6851.1636\n",
      "Epoch 6149/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7797.4104 - val_loss: 6850.6982\n",
      "Epoch 6150/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7796.9469 - val_loss: 6850.2363\n",
      "Epoch 6151/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7796.4839 - val_loss: 6849.7734\n",
      "Epoch 6152/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7796.0214 - val_loss: 6849.3110\n",
      "Epoch 6153/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7795.5593 - val_loss: 6848.8472\n",
      "Epoch 6154/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7795.0960 - val_loss: 6848.3857\n",
      "Epoch 6155/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7794.6320 - val_loss: 6847.9238\n",
      "Epoch 6156/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7794.1706 - val_loss: 6847.4595\n",
      "Epoch 6157/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7793.7080 - val_loss: 6846.9971\n",
      "Epoch 6158/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7793.2447 - val_loss: 6846.5347\n",
      "Epoch 6159/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7792.7821 - val_loss: 6846.0723\n",
      "Epoch 6160/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7792.3193 - val_loss: 6845.6094\n",
      "Epoch 6161/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7791.8575 - val_loss: 6845.1470\n",
      "Epoch 6162/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7791.3943 - val_loss: 6844.6836\n",
      "Epoch 6163/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7790.9312 - val_loss: 6844.2207\n",
      "Epoch 6164/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7790.4693 - val_loss: 6843.7573\n",
      "Epoch 6165/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7790.0063 - val_loss: 6843.2969\n",
      "Epoch 6166/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7789.5443 - val_loss: 6842.8320\n",
      "Epoch 6167/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7789.0813 - val_loss: 6842.3706\n",
      "Epoch 6168/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7788.6182 - val_loss: 6841.9082\n",
      "Epoch 6169/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7788.1551 - val_loss: 6841.4448\n",
      "Epoch 6170/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7787.6932 - val_loss: 6840.9829\n",
      "Epoch 6171/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7787.2296 - val_loss: 6840.5195\n",
      "Epoch 6172/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7786.7666 - val_loss: 6840.0571\n",
      "Epoch 6173/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7786.3039 - val_loss: 6839.5933\n",
      "Epoch 6174/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7785.8419 - val_loss: 6839.1309\n",
      "Epoch 6175/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7785.3795 - val_loss: 6838.6685\n",
      "Epoch 6176/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7784.9164 - val_loss: 6838.2065\n",
      "Epoch 6177/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7784.4541 - val_loss: 6837.7441\n",
      "Epoch 6178/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 150us/step - loss: 7783.9913 - val_loss: 6837.2808\n",
      "Epoch 6179/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7783.5283 - val_loss: 6836.8184\n",
      "Epoch 6180/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7783.0666 - val_loss: 6836.3545\n",
      "Epoch 6181/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7782.6029 - val_loss: 6835.8931\n",
      "Epoch 6182/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7782.1399 - val_loss: 6835.4316\n",
      "Epoch 6183/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7781.6781 - val_loss: 6834.9653\n",
      "Epoch 6184/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7781.2147 - val_loss: 6834.5039\n",
      "Epoch 6185/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7780.7521 - val_loss: 6834.0420\n",
      "Epoch 6186/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7780.2886 - val_loss: 6833.5781\n",
      "Epoch 6187/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7779.8264 - val_loss: 6833.1152\n",
      "Epoch 6188/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7779.3641 - val_loss: 6832.6528\n",
      "Epoch 6189/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7778.9007 - val_loss: 6832.1914\n",
      "Epoch 6190/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7778.4383 - val_loss: 6831.7290\n",
      "Epoch 6191/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7777.9759 - val_loss: 6831.2651\n",
      "Epoch 6192/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7777.5138 - val_loss: 6830.8027\n",
      "Epoch 6193/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7777.0511 - val_loss: 6830.3398\n",
      "Epoch 6194/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7776.5879 - val_loss: 6829.8779\n",
      "Epoch 6195/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7776.1252 - val_loss: 6829.4141\n",
      "Epoch 6196/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7775.6622 - val_loss: 6828.9517\n",
      "Epoch 6197/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7775.2004 - val_loss: 6828.4878\n",
      "Epoch 6198/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7774.7375 - val_loss: 6828.0264\n",
      "Epoch 6199/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7774.2734 - val_loss: 6827.5654\n",
      "Epoch 6200/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7773.8115 - val_loss: 6827.1011\n",
      "Epoch 6201/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7773.3494 - val_loss: 6826.6372\n",
      "Epoch 6202/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7772.8855 - val_loss: 6826.1753\n",
      "Epoch 6203/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7772.4235 - val_loss: 6825.7129\n",
      "Epoch 6204/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7771.9603 - val_loss: 6825.2500\n",
      "Epoch 6205/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7771.4981 - val_loss: 6824.7876\n",
      "Epoch 6206/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7771.0356 - val_loss: 6824.3262\n",
      "Epoch 6207/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7770.5723 - val_loss: 6823.8623\n",
      "Epoch 6208/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7770.1105 - val_loss: 6823.4004\n",
      "Epoch 6209/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7769.6471 - val_loss: 6822.9375\n",
      "Epoch 6210/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7769.1850 - val_loss: 6822.4727\n",
      "Epoch 6211/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7768.7231 - val_loss: 6822.0107\n",
      "Epoch 6212/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7768.2590 - val_loss: 6821.5488\n",
      "Epoch 6213/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7767.7959 - val_loss: 6821.0859\n",
      "Epoch 6214/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7767.3341 - val_loss: 6820.6221\n",
      "Epoch 6215/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7766.8707 - val_loss: 6820.1597\n",
      "Epoch 6216/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7766.4076 - val_loss: 6819.6987\n",
      "Epoch 6217/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7765.9448 - val_loss: 6819.2344\n",
      "Epoch 6218/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7765.4829 - val_loss: 6818.7720\n",
      "Epoch 6219/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7765.0203 - val_loss: 6818.3110\n",
      "Epoch 6220/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7764.5572 - val_loss: 6817.8462\n",
      "Epoch 6221/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7764.0947 - val_loss: 6817.3848\n",
      "Epoch 6222/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7763.6318 - val_loss: 6816.9219\n",
      "Epoch 6223/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7763.1691 - val_loss: 6816.4600\n",
      "Epoch 6224/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7762.7078 - val_loss: 6815.9956\n",
      "Epoch 6225/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7762.2442 - val_loss: 6815.5332\n",
      "Epoch 6226/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7761.7811 - val_loss: 6815.0723\n",
      "Epoch 6227/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7761.3186 - val_loss: 6814.6079\n",
      "Epoch 6228/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7760.8564 - val_loss: 6814.1455\n",
      "Epoch 6229/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7760.3929 - val_loss: 6813.6821\n",
      "Epoch 6230/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7759.9296 - val_loss: 6813.2207\n",
      "Epoch 6231/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7759.4677 - val_loss: 6812.7559\n",
      "Epoch 6232/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7759.0051 - val_loss: 6812.2935\n",
      "Epoch 6233/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7758.5417 - val_loss: 6811.8330\n",
      "Epoch 6234/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7758.0798 - val_loss: 6811.3691\n",
      "Epoch 6235/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7757.6164 - val_loss: 6810.9058\n",
      "Epoch 6236/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7757.1545 - val_loss: 6810.4434\n",
      "Epoch 6237/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7756.6921 - val_loss: 6809.9805\n",
      "Epoch 6238/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 7756.2287 - val_loss: 6809.5190\n",
      "Epoch 6239/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7755.7662 - val_loss: 6809.0566\n",
      "Epoch 6240/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7755.3034 - val_loss: 6808.5933\n",
      "Epoch 6241/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7754.8416 - val_loss: 6808.1294\n",
      "Epoch 6242/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7754.3784 - val_loss: 6807.6680\n",
      "Epoch 6243/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7753.9151 - val_loss: 6807.2065\n",
      "Epoch 6244/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7753.4520 - val_loss: 6806.7417\n",
      "Epoch 6245/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7752.9904 - val_loss: 6806.2793\n",
      "Epoch 6246/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7752.5273 - val_loss: 6805.8164\n",
      "Epoch 6247/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7752.0645 - val_loss: 6805.3545\n",
      "Epoch 6248/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7751.6015 - val_loss: 6804.8901\n",
      "Epoch 6249/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7751.1391 - val_loss: 6804.4277\n",
      "Epoch 6250/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 126us/step - loss: 7750.6767 - val_loss: 6803.9668\n",
      "Epoch 6251/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7750.2135 - val_loss: 6803.5029\n",
      "Epoch 6252/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7749.7524 - val_loss: 6803.0415\n",
      "Epoch 6253/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7749.2881 - val_loss: 6802.5776\n",
      "Epoch 6254/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7748.8254 - val_loss: 6802.1152\n",
      "Epoch 6255/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7748.3638 - val_loss: 6801.6523\n",
      "Epoch 6256/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7747.9005 - val_loss: 6801.1904\n",
      "Epoch 6257/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7747.4371 - val_loss: 6800.7290\n",
      "Epoch 6258/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7746.9749 - val_loss: 6800.2637\n",
      "Epoch 6259/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7746.5120 - val_loss: 6799.8003\n",
      "Epoch 6260/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7746.0492 - val_loss: 6799.3389\n",
      "Epoch 6261/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7745.5856 - val_loss: 6798.8774\n",
      "Epoch 6262/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7745.1240 - val_loss: 6798.4141\n",
      "Epoch 6263/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7744.6619 - val_loss: 6797.9497\n",
      "Epoch 6264/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 7744.1985 - val_loss: 6797.4878\n",
      "Epoch 6265/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7743.7354 - val_loss: 6797.0254\n",
      "Epoch 6266/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7743.2729 - val_loss: 6796.5625\n",
      "Epoch 6267/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7742.8109 - val_loss: 6796.1016\n",
      "Epoch 6268/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7742.3488 - val_loss: 6795.6362\n",
      "Epoch 6269/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7741.8857 - val_loss: 6795.1748\n",
      "Epoch 6270/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7741.4223 - val_loss: 6794.7129\n",
      "Epoch 6271/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7740.9598 - val_loss: 6794.2500\n",
      "Epoch 6272/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7740.4979 - val_loss: 6793.7847\n",
      "Epoch 6273/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7740.0347 - val_loss: 6793.3237\n",
      "Epoch 6274/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7739.5706 - val_loss: 6792.8623\n",
      "Epoch 6275/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7739.1085 - val_loss: 6792.3984\n",
      "Epoch 6276/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7738.6465 - val_loss: 6791.9360\n",
      "Epoch 6277/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7738.1835 - val_loss: 6791.4722\n",
      "Epoch 6278/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7737.7207 - val_loss: 6791.0107\n",
      "Epoch 6279/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7737.2571 - val_loss: 6790.5469\n",
      "Epoch 6280/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7736.7956 - val_loss: 6790.0850\n",
      "Epoch 6281/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7736.3337 - val_loss: 6789.6235\n",
      "Epoch 6282/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7735.8699 - val_loss: 6789.1587\n",
      "Epoch 6283/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7735.4078 - val_loss: 6788.6973\n",
      "Epoch 6284/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7734.9447 - val_loss: 6788.2344\n",
      "Epoch 6285/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7734.4821 - val_loss: 6787.7720\n",
      "Epoch 6286/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7734.0203 - val_loss: 6787.3086\n",
      "Epoch 6287/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 7733.5561 - val_loss: 6786.8462\n",
      "Epoch 6288/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7733.0933 - val_loss: 6786.3823\n",
      "Epoch 6289/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 7732.6314 - val_loss: 6785.9204\n",
      "Epoch 6290/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7732.1680 - val_loss: 6785.4580\n",
      "Epoch 6291/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7731.7054 - val_loss: 6784.9961\n",
      "Epoch 6292/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7731.2419 - val_loss: 6784.5332\n",
      "Epoch 6293/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 7730.7801 - val_loss: 6784.0698\n",
      "Epoch 6294/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 7730.3183 - val_loss: 6783.6079\n",
      "Epoch 6295/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7729.8551 - val_loss: 6783.1445\n",
      "Epoch 6296/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7729.3921 - val_loss: 6782.6816\n",
      "Epoch 6297/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7728.9292 - val_loss: 6782.2183\n",
      "Epoch 6298/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7728.4669 - val_loss: 6781.7568\n",
      "Epoch 6299/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7728.0051 - val_loss: 6781.2930\n",
      "Epoch 6300/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7727.5414 - val_loss: 6780.8315\n",
      "Epoch 6301/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7727.0782 - val_loss: 6780.3691\n",
      "Epoch 6302/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7726.6159 - val_loss: 6779.9043\n",
      "Epoch 6303/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7726.1534 - val_loss: 6779.4419\n",
      "Epoch 6304/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7725.6908 - val_loss: 6778.9795\n",
      "Epoch 6305/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7725.2268 - val_loss: 6778.5181\n",
      "Epoch 6306/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 7724.7652 - val_loss: 6778.0542\n",
      "Epoch 6307/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 7724.3028 - val_loss: 6777.5918\n",
      "Epoch 6308/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 7723.8396 - val_loss: 6777.1294\n",
      "Epoch 6309/10000\n",
      "750/750 [==============================] - 0s 92us/step - loss: 7723.3770 - val_loss: 6776.6670\n",
      "Epoch 6310/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 7722.9137 - val_loss: 6776.2031\n",
      "Epoch 6311/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 7722.4517 - val_loss: 6775.7417\n",
      "Epoch 6312/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7721.9898 - val_loss: 6775.2778\n",
      "Epoch 6313/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7721.5259 - val_loss: 6774.8154\n",
      "Epoch 6314/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7721.0640 - val_loss: 6774.3540\n",
      "Epoch 6315/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7720.6004 - val_loss: 6773.8931\n",
      "Epoch 6316/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7720.1389 - val_loss: 6773.4263\n",
      "Epoch 6317/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7719.6755 - val_loss: 6772.9648\n",
      "Epoch 6318/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7719.2122 - val_loss: 6772.5029\n",
      "Epoch 6319/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7718.7497 - val_loss: 6772.0391\n",
      "Epoch 6320/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7718.2874 - val_loss: 6771.5762\n",
      "Epoch 6321/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7717.8244 - val_loss: 6771.1138\n",
      "Epoch 6322/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 124us/step - loss: 7717.3613 - val_loss: 6770.6523\n",
      "Epoch 6323/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7716.8986 - val_loss: 6770.1875\n",
      "Epoch 6324/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7716.4365 - val_loss: 6769.7266\n",
      "Epoch 6325/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7715.9743 - val_loss: 6769.2637\n",
      "Epoch 6326/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7715.5107 - val_loss: 6768.8003\n",
      "Epoch 6327/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7715.0492 - val_loss: 6768.3379\n",
      "Epoch 6328/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7714.5854 - val_loss: 6767.8750\n",
      "Epoch 6329/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7714.1231 - val_loss: 6767.4126\n",
      "Epoch 6330/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7713.6610 - val_loss: 6766.9487\n",
      "Epoch 6331/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7713.1976 - val_loss: 6766.4873\n",
      "Epoch 6332/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7712.7344 - val_loss: 6766.0254\n",
      "Epoch 6333/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7712.2721 - val_loss: 6765.5610\n",
      "Epoch 6334/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7711.8096 - val_loss: 6765.0977\n",
      "Epoch 6335/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7711.3471 - val_loss: 6764.6362\n",
      "Epoch 6336/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7710.8829 - val_loss: 6764.1738\n",
      "Epoch 6337/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7710.4210 - val_loss: 6763.7109\n",
      "Epoch 6338/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7709.9590 - val_loss: 6763.2485\n",
      "Epoch 6339/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7709.4957 - val_loss: 6762.7852\n",
      "Epoch 6340/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7709.0335 - val_loss: 6762.3223\n",
      "Epoch 6341/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7708.5697 - val_loss: 6761.8594\n",
      "Epoch 6342/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7708.1079 - val_loss: 6761.3984\n",
      "Epoch 6343/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7707.6460 - val_loss: 6760.9346\n",
      "Epoch 6344/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7707.1830 - val_loss: 6760.4712\n",
      "Epoch 6345/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7706.7197 - val_loss: 6760.0098\n",
      "Epoch 6346/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7706.2566 - val_loss: 6759.5469\n",
      "Epoch 6347/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7705.7948 - val_loss: 6759.0830\n",
      "Epoch 6348/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7705.3317 - val_loss: 6758.6211\n",
      "Epoch 6349/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7704.8685 - val_loss: 6758.1597\n",
      "Epoch 6350/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7704.4057 - val_loss: 6757.6948\n",
      "Epoch 6351/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7703.9436 - val_loss: 6757.2329\n",
      "Epoch 6352/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7703.4812 - val_loss: 6756.7705\n",
      "Epoch 6353/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7703.0178 - val_loss: 6756.3071\n",
      "Epoch 6354/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7702.5548 - val_loss: 6755.8433\n",
      "Epoch 6355/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7702.0927 - val_loss: 6755.3823\n",
      "Epoch 6356/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7701.6305 - val_loss: 6754.9204\n",
      "Epoch 6357/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7701.1677 - val_loss: 6754.4565\n",
      "Epoch 6358/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 7700.7050 - val_loss: 6753.9941\n",
      "Epoch 6359/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7700.2417 - val_loss: 6753.5308\n",
      "Epoch 6360/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7699.7794 - val_loss: 6753.0684\n",
      "Epoch 6361/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7699.3172 - val_loss: 6752.6060\n",
      "Epoch 6362/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7698.8539 - val_loss: 6752.1440\n",
      "Epoch 6363/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7698.3902 - val_loss: 6751.6816\n",
      "Epoch 6364/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7697.9284 - val_loss: 6751.2168\n",
      "Epoch 6365/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7697.4659 - val_loss: 6750.7544\n",
      "Epoch 6366/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 7697.0032 - val_loss: 6750.2935\n",
      "Epoch 6367/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7696.5392 - val_loss: 6749.8306\n",
      "Epoch 6368/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 7696.0774 - val_loss: 6749.3667\n",
      "Epoch 6369/10000\n",
      "750/750 [==============================] - 0s 180us/step - loss: 7695.6153 - val_loss: 6748.9053\n",
      "Epoch 6370/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 7695.1522 - val_loss: 6748.4414\n",
      "Epoch 6371/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7694.6904 - val_loss: 6747.9795\n",
      "Epoch 6372/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7694.2271 - val_loss: 6747.5151\n",
      "Epoch 6373/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7693.7640 - val_loss: 6747.0542\n",
      "Epoch 6374/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7693.3022 - val_loss: 6746.5903\n",
      "Epoch 6375/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7692.8393 - val_loss: 6746.1279\n",
      "Epoch 6376/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7692.3760 - val_loss: 6745.6670\n",
      "Epoch 6377/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7691.9128 - val_loss: 6745.2017\n",
      "Epoch 6378/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7691.4506 - val_loss: 6744.7402\n",
      "Epoch 6379/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7690.9879 - val_loss: 6744.2773\n",
      "Epoch 6380/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7690.5245 - val_loss: 6743.8154\n",
      "Epoch 6381/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7690.0624 - val_loss: 6743.3516\n",
      "Epoch 6382/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7689.5998 - val_loss: 6742.8887\n",
      "Epoch 6383/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7689.1367 - val_loss: 6742.4277\n",
      "Epoch 6384/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7688.6745 - val_loss: 6741.9639\n",
      "Epoch 6385/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7688.2111 - val_loss: 6741.5024\n",
      "Epoch 6386/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7687.7493 - val_loss: 6741.0391\n",
      "Epoch 6387/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7687.2868 - val_loss: 6740.5747\n",
      "Epoch 6388/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7686.8243 - val_loss: 6740.1123\n",
      "Epoch 6389/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7686.3612 - val_loss: 6739.6504\n",
      "Epoch 6390/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7685.8976 - val_loss: 6739.1899\n",
      "Epoch 6391/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7685.4358 - val_loss: 6738.7246\n",
      "Epoch 6392/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7684.9728 - val_loss: 6738.2622\n",
      "Epoch 6393/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7684.5094 - val_loss: 6737.7998\n",
      "Epoch 6394/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 132us/step - loss: 7684.0469 - val_loss: 6737.3359\n",
      "Epoch 6395/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7683.5846 - val_loss: 6736.8740\n",
      "Epoch 6396/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7683.1221 - val_loss: 6736.4102\n",
      "Epoch 6397/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7682.6594 - val_loss: 6735.9487\n",
      "Epoch 6398/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7682.1962 - val_loss: 6735.4863\n",
      "Epoch 6399/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7681.7341 - val_loss: 6735.0234\n",
      "Epoch 6400/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7681.2717 - val_loss: 6734.5610\n",
      "Epoch 6401/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7680.8084 - val_loss: 6734.0972\n",
      "Epoch 6402/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7680.3464 - val_loss: 6733.6357\n",
      "Epoch 6403/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7679.8824 - val_loss: 6733.1719\n",
      "Epoch 6404/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7679.4201 - val_loss: 6732.7100\n",
      "Epoch 6405/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7678.9589 - val_loss: 6732.2461\n",
      "Epoch 6406/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7678.4948 - val_loss: 6731.7837\n",
      "Epoch 6407/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7678.0320 - val_loss: 6731.3232\n",
      "Epoch 6408/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7677.5693 - val_loss: 6730.8584\n",
      "Epoch 6409/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7677.1065 - val_loss: 6730.3970\n",
      "Epoch 6410/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7676.6442 - val_loss: 6729.9336\n",
      "Epoch 6411/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7676.1800 - val_loss: 6729.4707\n",
      "Epoch 6412/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7675.7182 - val_loss: 6729.0073\n",
      "Epoch 6413/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7675.2564 - val_loss: 6728.5454\n",
      "Epoch 6414/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7674.7928 - val_loss: 6728.0845\n",
      "Epoch 6415/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7674.3311 - val_loss: 6727.6206\n",
      "Epoch 6416/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7673.8672 - val_loss: 6727.1558\n",
      "Epoch 6417/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7673.4053 - val_loss: 6726.6948\n",
      "Epoch 6418/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7672.9430 - val_loss: 6726.2310\n",
      "Epoch 6419/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7672.4803 - val_loss: 6725.7695\n",
      "Epoch 6420/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 7672.0168 - val_loss: 6725.3071\n",
      "Epoch 6421/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 7671.5537 - val_loss: 6724.8433\n",
      "Epoch 6422/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7671.0920 - val_loss: 6724.3794\n",
      "Epoch 6423/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7670.6292 - val_loss: 6723.9180\n",
      "Epoch 6424/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7670.1654 - val_loss: 6723.4570\n",
      "Epoch 6425/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7669.7030 - val_loss: 6722.9917\n",
      "Epoch 6426/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7669.2408 - val_loss: 6722.5293\n",
      "Epoch 6427/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7668.7782 - val_loss: 6722.0684\n",
      "Epoch 6428/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7668.3157 - val_loss: 6721.6045\n",
      "Epoch 6429/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7667.8520 - val_loss: 6721.1421\n",
      "Epoch 6430/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7667.3907 - val_loss: 6720.6792\n",
      "Epoch 6431/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7666.9275 - val_loss: 6720.2178\n",
      "Epoch 6432/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7666.4653 - val_loss: 6719.7539\n",
      "Epoch 6433/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7666.0023 - val_loss: 6719.2920\n",
      "Epoch 6434/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7665.5389 - val_loss: 6718.8296\n",
      "Epoch 6435/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7665.0765 - val_loss: 6718.3652\n",
      "Epoch 6436/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7664.6143 - val_loss: 6717.9028\n",
      "Epoch 6437/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7664.1507 - val_loss: 6717.4404\n",
      "Epoch 6438/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7663.6875 - val_loss: 6716.9790\n",
      "Epoch 6439/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 7663.2256 - val_loss: 6716.5137\n",
      "Epoch 6440/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7662.7629 - val_loss: 6716.0527\n",
      "Epoch 6441/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7662.3003 - val_loss: 6715.5903\n",
      "Epoch 6442/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7661.8373 - val_loss: 6715.1274\n",
      "Epoch 6443/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7661.3748 - val_loss: 6714.6641\n",
      "Epoch 6444/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7660.9124 - val_loss: 6714.2017\n",
      "Epoch 6445/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7660.4498 - val_loss: 6713.7388\n",
      "Epoch 6446/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7659.9879 - val_loss: 6713.2764\n",
      "Epoch 6447/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7659.5242 - val_loss: 6712.8140\n",
      "Epoch 6448/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7659.0612 - val_loss: 6712.3516\n",
      "Epoch 6449/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7658.5996 - val_loss: 6711.8872\n",
      "Epoch 6450/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7658.1362 - val_loss: 6711.4253\n",
      "Epoch 6451/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7657.6728 - val_loss: 6710.9639\n",
      "Epoch 6452/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7657.2098 - val_loss: 6710.5000\n",
      "Epoch 6453/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7656.7482 - val_loss: 6710.0376\n",
      "Epoch 6454/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7656.2851 - val_loss: 6709.5737\n",
      "Epoch 6455/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7655.8217 - val_loss: 6709.1123\n",
      "Epoch 6456/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7655.3593 - val_loss: 6708.6484\n",
      "Epoch 6457/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7654.8970 - val_loss: 6708.1860\n",
      "Epoch 6458/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7654.4342 - val_loss: 6707.7246\n",
      "Epoch 6459/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7653.9722 - val_loss: 6707.2607\n",
      "Epoch 6460/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7653.5087 - val_loss: 6706.7983\n",
      "Epoch 6461/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7653.0463 - val_loss: 6706.3359\n",
      "Epoch 6462/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7652.5840 - val_loss: 6705.8735\n",
      "Epoch 6463/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7652.1217 - val_loss: 6705.4097\n",
      "Epoch 6464/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7651.6581 - val_loss: 6704.9482\n",
      "Epoch 6465/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7651.1948 - val_loss: 6704.4863\n",
      "Epoch 6466/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 136us/step - loss: 7650.7328 - val_loss: 6704.0220\n",
      "Epoch 6467/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7650.2704 - val_loss: 6703.5596\n",
      "Epoch 6468/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7649.8073 - val_loss: 6703.0972\n",
      "Epoch 6469/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 7649.3439 - val_loss: 6702.6323\n",
      "Epoch 6470/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7648.8816 - val_loss: 6702.1709\n",
      "Epoch 6471/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7648.4191 - val_loss: 6701.7095\n",
      "Epoch 6472/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7647.9565 - val_loss: 6701.2471\n",
      "Epoch 6473/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7647.4935 - val_loss: 6700.7832\n",
      "Epoch 6474/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7647.0318 - val_loss: 6700.3198\n",
      "Epoch 6475/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7646.5687 - val_loss: 6699.8594\n",
      "Epoch 6476/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7646.1061 - val_loss: 6699.3945\n",
      "Epoch 6477/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7645.6438 - val_loss: 6698.9321\n",
      "Epoch 6478/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7645.1802 - val_loss: 6698.4697\n",
      "Epoch 6479/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7644.7176 - val_loss: 6698.0068\n",
      "Epoch 6480/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7644.2558 - val_loss: 6697.5435\n",
      "Epoch 6481/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7643.7919 - val_loss: 6697.0815\n",
      "Epoch 6482/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7643.3293 - val_loss: 6696.6206\n",
      "Epoch 6483/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7642.8665 - val_loss: 6696.1553\n",
      "Epoch 6484/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7642.4036 - val_loss: 6695.6934\n",
      "Epoch 6485/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7641.9420 - val_loss: 6695.2310\n",
      "Epoch 6486/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7641.4786 - val_loss: 6694.7690\n",
      "Epoch 6487/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7641.0157 - val_loss: 6694.3042\n",
      "Epoch 6488/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 7640.5534 - val_loss: 6693.8428\n",
      "Epoch 6489/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 7640.0900 - val_loss: 6693.3809\n",
      "Epoch 6490/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 7639.6291 - val_loss: 6692.9170\n",
      "Epoch 6491/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 7639.1657 - val_loss: 6692.4546\n",
      "Epoch 6492/10000\n",
      "750/750 [==============================] - 0s 162us/step - loss: 7638.7024 - val_loss: 6691.9917\n",
      "Epoch 6493/10000\n",
      "750/750 [==============================] - 0s 162us/step - loss: 7638.2403 - val_loss: 6691.5293\n",
      "Epoch 6494/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7637.7773 - val_loss: 6691.0664\n",
      "Epoch 6495/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 7637.3145 - val_loss: 6690.6045\n",
      "Epoch 6496/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 7636.8508 - val_loss: 6690.1401\n",
      "Epoch 6497/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 7636.3892 - val_loss: 6689.6777\n",
      "Epoch 6498/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7635.9267 - val_loss: 6689.2153\n",
      "Epoch 6499/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7635.4634 - val_loss: 6688.7544\n",
      "Epoch 6500/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7635.0009 - val_loss: 6688.2905\n",
      "Epoch 6501/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7634.5379 - val_loss: 6687.8267\n",
      "Epoch 6502/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7634.0755 - val_loss: 6687.3652\n",
      "Epoch 6503/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7633.6135 - val_loss: 6686.9023\n",
      "Epoch 6504/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7633.1498 - val_loss: 6686.4404\n",
      "Epoch 6505/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7632.6877 - val_loss: 6685.9766\n",
      "Epoch 6506/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7632.2247 - val_loss: 6685.5151\n",
      "Epoch 6507/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7631.7629 - val_loss: 6685.0503\n",
      "Epoch 6508/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7631.3000 - val_loss: 6684.5889\n",
      "Epoch 6509/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7630.8359 - val_loss: 6684.1279\n",
      "Epoch 6510/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7630.3736 - val_loss: 6683.6636\n",
      "Epoch 6511/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7629.9120 - val_loss: 6683.2012\n",
      "Epoch 6512/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7629.4482 - val_loss: 6682.7378\n",
      "Epoch 6513/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7628.9852 - val_loss: 6682.2754\n",
      "Epoch 6514/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7628.5226 - val_loss: 6681.8115\n",
      "Epoch 6515/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7628.0597 - val_loss: 6681.3496\n",
      "Epoch 6516/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7627.5979 - val_loss: 6680.8887\n",
      "Epoch 6517/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7627.1345 - val_loss: 6680.4238\n",
      "Epoch 6518/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7626.6720 - val_loss: 6679.9609\n",
      "Epoch 6519/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7626.2093 - val_loss: 6679.5000\n",
      "Epoch 6520/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7625.7476 - val_loss: 6679.0352\n",
      "Epoch 6521/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7625.2848 - val_loss: 6678.5732\n",
      "Epoch 6522/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7624.8215 - val_loss: 6678.1108\n",
      "Epoch 6523/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7624.3582 - val_loss: 6677.6499\n",
      "Epoch 6524/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7623.8968 - val_loss: 6677.1846\n",
      "Epoch 6525/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7623.4333 - val_loss: 6676.7222\n",
      "Epoch 6526/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7622.9700 - val_loss: 6676.2607\n",
      "Epoch 6527/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7622.5070 - val_loss: 6675.7969\n",
      "Epoch 6528/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7622.0451 - val_loss: 6675.3345\n",
      "Epoch 6529/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7621.5829 - val_loss: 6674.8721\n",
      "Epoch 6530/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7621.1185 - val_loss: 6674.4097\n",
      "Epoch 6531/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7620.6572 - val_loss: 6673.9448\n",
      "Epoch 6532/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7620.1944 - val_loss: 6673.4829\n",
      "Epoch 6533/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7619.7315 - val_loss: 6673.0220\n",
      "Epoch 6534/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7619.2697 - val_loss: 6672.5586\n",
      "Epoch 6535/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7618.8056 - val_loss: 6672.0957\n",
      "Epoch 6536/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 7618.3440 - val_loss: 6671.6323\n",
      "Epoch 6537/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7617.8809 - val_loss: 6671.1704\n",
      "Epoch 6538/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 141us/step - loss: 7617.4191 - val_loss: 6670.7070\n",
      "Epoch 6539/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7616.9553 - val_loss: 6670.2456\n",
      "Epoch 6540/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7616.4920 - val_loss: 6669.7832\n",
      "Epoch 6541/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 7616.0300 - val_loss: 6669.3184\n",
      "Epoch 6542/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 7615.5674 - val_loss: 6668.8560\n",
      "Epoch 6543/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 7615.1043 - val_loss: 6668.3945\n",
      "Epoch 6544/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7614.6414 - val_loss: 6667.9307\n",
      "Epoch 6545/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7614.1788 - val_loss: 6667.4683\n",
      "Epoch 6546/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7613.7168 - val_loss: 6667.0059\n",
      "Epoch 6547/10000\n",
      "750/750 [==============================] - 0s 168us/step - loss: 7613.2537 - val_loss: 6666.5435\n",
      "Epoch 6548/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7612.7907 - val_loss: 6666.0815\n",
      "Epoch 6549/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7612.3294 - val_loss: 6665.6167\n",
      "Epoch 6550/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 7611.8656 - val_loss: 6665.1558\n",
      "Epoch 6551/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 7611.4039 - val_loss: 6664.6919\n",
      "Epoch 6552/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 7610.9410 - val_loss: 6664.2295\n",
      "Epoch 6553/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7610.4775 - val_loss: 6663.7671\n",
      "Epoch 6554/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 7610.0145 - val_loss: 6663.3042\n",
      "Epoch 6555/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7609.5528 - val_loss: 6662.8418\n",
      "Epoch 6556/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7609.0890 - val_loss: 6662.3779\n",
      "Epoch 6557/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7608.6269 - val_loss: 6661.9180\n",
      "Epoch 6558/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7608.1635 - val_loss: 6661.4531\n",
      "Epoch 6559/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7607.7015 - val_loss: 6660.9902\n",
      "Epoch 6560/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7607.2390 - val_loss: 6660.5278\n",
      "Epoch 6561/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7606.7757 - val_loss: 6660.0654\n",
      "Epoch 6562/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7606.3128 - val_loss: 6659.6030\n",
      "Epoch 6563/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7605.8506 - val_loss: 6659.1392\n",
      "Epoch 6564/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7605.3876 - val_loss: 6658.6782\n",
      "Epoch 6565/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7604.9266 - val_loss: 6658.2148\n",
      "Epoch 6566/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7604.4630 - val_loss: 6657.7515\n",
      "Epoch 6567/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7603.9995 - val_loss: 6657.2905\n",
      "Epoch 6568/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7603.5374 - val_loss: 6656.8262\n",
      "Epoch 6569/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7603.0751 - val_loss: 6656.3638\n",
      "Epoch 6570/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7602.6117 - val_loss: 6655.9014\n",
      "Epoch 6571/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7602.1483 - val_loss: 6655.4390\n",
      "Epoch 6572/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7601.6865 - val_loss: 6654.9746\n",
      "Epoch 6573/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7601.2238 - val_loss: 6654.5122\n",
      "Epoch 6574/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7600.7603 - val_loss: 6654.0513\n",
      "Epoch 6575/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 7600.2978 - val_loss: 6653.5874\n",
      "Epoch 6576/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7599.8354 - val_loss: 6653.1240\n",
      "Epoch 6577/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7599.3729 - val_loss: 6652.6636\n",
      "Epoch 6578/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7598.9113 - val_loss: 6652.1987\n",
      "Epoch 6579/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7598.4476 - val_loss: 6651.7373\n",
      "Epoch 6580/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7597.9849 - val_loss: 6651.2749\n",
      "Epoch 6581/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7597.5219 - val_loss: 6650.8125\n",
      "Epoch 6582/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7597.0603 - val_loss: 6650.3477\n",
      "Epoch 6583/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7596.5972 - val_loss: 6649.8857\n",
      "Epoch 6584/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7596.1332 - val_loss: 6649.4248\n",
      "Epoch 6585/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7595.6708 - val_loss: 6648.9600\n",
      "Epoch 6586/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7595.2092 - val_loss: 6648.4985\n",
      "Epoch 6587/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7594.7457 - val_loss: 6648.0347\n",
      "Epoch 6588/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7594.2824 - val_loss: 6647.5732\n",
      "Epoch 6589/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7593.8196 - val_loss: 6647.1094\n",
      "Epoch 6590/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7593.3576 - val_loss: 6646.6470\n",
      "Epoch 6591/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7592.8951 - val_loss: 6646.1860\n",
      "Epoch 6592/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7592.4316 - val_loss: 6645.7212\n",
      "Epoch 6593/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7591.9703 - val_loss: 6645.2593\n",
      "Epoch 6594/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7591.5072 - val_loss: 6644.7969\n",
      "Epoch 6595/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7591.0448 - val_loss: 6644.3330\n",
      "Epoch 6596/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7590.5820 - val_loss: 6643.8706\n",
      "Epoch 6597/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7590.1191 - val_loss: 6643.4087\n",
      "Epoch 6598/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7589.6560 - val_loss: 6642.9473\n",
      "Epoch 6599/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7589.1936 - val_loss: 6642.4829\n",
      "Epoch 6600/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7588.7308 - val_loss: 6642.0195\n",
      "Epoch 6601/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7588.2676 - val_loss: 6641.5571\n",
      "Epoch 6602/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7587.8047 - val_loss: 6641.0933\n",
      "Epoch 6603/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7587.3425 - val_loss: 6640.6318\n",
      "Epoch 6604/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7586.8806 - val_loss: 6640.1685\n",
      "Epoch 6605/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7586.4165 - val_loss: 6639.7070\n",
      "Epoch 6606/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7585.9544 - val_loss: 6639.2432\n",
      "Epoch 6607/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7585.4916 - val_loss: 6638.7803\n",
      "Epoch 6608/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7585.0289 - val_loss: 6638.3193\n",
      "Epoch 6609/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7584.5676 - val_loss: 6637.8555\n",
      "Epoch 6610/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 137us/step - loss: 7584.1040 - val_loss: 6637.3940\n",
      "Epoch 6611/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7583.6410 - val_loss: 6636.9307\n",
      "Epoch 6612/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7583.1783 - val_loss: 6636.4678\n",
      "Epoch 6613/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7582.7159 - val_loss: 6636.0039\n",
      "Epoch 6614/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7582.2527 - val_loss: 6635.5420\n",
      "Epoch 6615/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7581.7890 - val_loss: 6635.0815\n",
      "Epoch 6616/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7581.3271 - val_loss: 6634.6157\n",
      "Epoch 6617/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7580.8652 - val_loss: 6634.1543\n",
      "Epoch 6618/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7580.4015 - val_loss: 6633.6919\n",
      "Epoch 6619/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7579.9393 - val_loss: 6633.2280\n",
      "Epoch 6620/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7579.4761 - val_loss: 6632.7651\n",
      "Epoch 6621/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 7579.0143 - val_loss: 6632.3032\n",
      "Epoch 6622/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7578.5520 - val_loss: 6631.8418\n",
      "Epoch 6623/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7578.0879 - val_loss: 6631.3779\n",
      "Epoch 6624/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7577.6264 - val_loss: 6630.9141\n",
      "Epoch 6625/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7577.1626 - val_loss: 6630.4531\n",
      "Epoch 6626/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7576.7009 - val_loss: 6629.9888\n",
      "Epoch 6627/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7576.2380 - val_loss: 6629.5264\n",
      "Epoch 6628/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7575.7747 - val_loss: 6629.0654\n",
      "Epoch 6629/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7575.3121 - val_loss: 6628.6016\n",
      "Epoch 6630/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7574.8498 - val_loss: 6628.1387\n",
      "Epoch 6631/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7574.3866 - val_loss: 6627.6753\n",
      "Epoch 6632/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7573.9239 - val_loss: 6627.2148\n",
      "Epoch 6633/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7573.4604 - val_loss: 6626.7500\n",
      "Epoch 6634/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7572.9986 - val_loss: 6626.2886\n",
      "Epoch 6635/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7572.5370 - val_loss: 6625.8262\n",
      "Epoch 6636/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7572.0731 - val_loss: 6625.3623\n",
      "Epoch 6637/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7571.6105 - val_loss: 6624.8999\n",
      "Epoch 6638/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7571.1476 - val_loss: 6624.4375\n",
      "Epoch 6639/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7570.6852 - val_loss: 6623.9761\n",
      "Epoch 6640/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7570.2238 - val_loss: 6623.5112\n",
      "Epoch 6641/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7569.7604 - val_loss: 6623.0498\n",
      "Epoch 6642/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7569.2966 - val_loss: 6622.5879\n",
      "Epoch 6643/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7568.8344 - val_loss: 6622.1235\n",
      "Epoch 6644/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7568.3721 - val_loss: 6621.6602\n",
      "Epoch 6645/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7567.9088 - val_loss: 6621.1982\n",
      "Epoch 6646/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7567.4453 - val_loss: 6620.7363\n",
      "Epoch 6647/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7566.9834 - val_loss: 6620.2725\n",
      "Epoch 6648/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7566.5214 - val_loss: 6619.8096\n",
      "Epoch 6649/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7566.0573 - val_loss: 6619.3496\n",
      "Epoch 6650/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7565.5952 - val_loss: 6618.8848\n",
      "Epoch 6651/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7565.1323 - val_loss: 6618.4219\n",
      "Epoch 6652/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7564.6706 - val_loss: 6617.9600\n",
      "Epoch 6653/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7564.2082 - val_loss: 6617.4971\n",
      "Epoch 6654/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7563.7451 - val_loss: 6617.0337\n",
      "Epoch 6655/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7563.2824 - val_loss: 6616.5718\n",
      "Epoch 6656/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7562.8190 - val_loss: 6616.1108\n",
      "Epoch 6657/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7562.3576 - val_loss: 6615.6455\n",
      "Epoch 6658/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7561.8941 - val_loss: 6615.1821\n",
      "Epoch 6659/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7561.4306 - val_loss: 6614.7212\n",
      "Epoch 6660/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7560.9680 - val_loss: 6614.2573\n",
      "Epoch 6661/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7560.5062 - val_loss: 6613.7954\n",
      "Epoch 6662/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7560.0431 - val_loss: 6613.3330\n",
      "Epoch 6663/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7559.5801 - val_loss: 6612.8706\n",
      "Epoch 6664/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7559.1176 - val_loss: 6612.4058\n",
      "Epoch 6665/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 7558.6550 - val_loss: 6611.9434\n",
      "Epoch 6666/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 7558.1923 - val_loss: 6611.4834\n",
      "Epoch 6667/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7557.7293 - val_loss: 6611.0190\n",
      "Epoch 6668/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7557.2679 - val_loss: 6610.5557\n",
      "Epoch 6669/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7556.8042 - val_loss: 6610.0947\n",
      "Epoch 6670/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7556.3420 - val_loss: 6609.6309\n",
      "Epoch 6671/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7555.8796 - val_loss: 6609.1680\n",
      "Epoch 6672/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7555.4163 - val_loss: 6608.7065\n",
      "Epoch 6673/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 7554.9528 - val_loss: 6608.2441\n",
      "Epoch 6674/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7554.4907 - val_loss: 6607.7793\n",
      "Epoch 6675/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7554.0284 - val_loss: 6607.3169\n",
      "Epoch 6676/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7553.5652 - val_loss: 6606.8555\n",
      "Epoch 6677/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7553.1016 - val_loss: 6606.3921\n",
      "Epoch 6678/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7552.6396 - val_loss: 6605.9292\n",
      "Epoch 6679/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7552.1779 - val_loss: 6605.4668\n",
      "Epoch 6680/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7551.7143 - val_loss: 6605.0044\n",
      "Epoch 6681/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7551.2523 - val_loss: 6604.5415\n",
      "Epoch 6682/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 141us/step - loss: 7550.7885 - val_loss: 6604.0767\n",
      "Epoch 6683/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7550.3258 - val_loss: 6603.6167\n",
      "Epoch 6684/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7549.8646 - val_loss: 6603.1523\n",
      "Epoch 6685/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7549.4016 - val_loss: 6602.6904\n",
      "Epoch 6686/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7548.9382 - val_loss: 6602.2290\n",
      "Epoch 6687/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7548.4753 - val_loss: 6601.7651\n",
      "Epoch 6688/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7548.0135 - val_loss: 6601.3013\n",
      "Epoch 6689/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7547.5505 - val_loss: 6600.8398\n",
      "Epoch 6690/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7547.0866 - val_loss: 6600.3789\n",
      "Epoch 6691/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7546.6244 - val_loss: 6599.9141\n",
      "Epoch 6692/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7546.1621 - val_loss: 6599.4512\n",
      "Epoch 6693/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7545.6992 - val_loss: 6598.9888\n",
      "Epoch 6694/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7545.2365 - val_loss: 6598.5264\n",
      "Epoch 6695/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7544.7736 - val_loss: 6598.0640\n",
      "Epoch 6696/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7544.3127 - val_loss: 6597.6011\n",
      "Epoch 6697/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7543.8492 - val_loss: 6597.1387\n",
      "Epoch 6698/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7543.3863 - val_loss: 6596.6748\n",
      "Epoch 6699/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7542.9236 - val_loss: 6596.2124\n",
      "Epoch 6700/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7542.4602 - val_loss: 6595.7500\n",
      "Epoch 6701/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7541.9982 - val_loss: 6595.2876\n",
      "Epoch 6702/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7541.5358 - val_loss: 6594.8237\n",
      "Epoch 6703/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7541.0719 - val_loss: 6594.3623\n",
      "Epoch 6704/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7540.6091 - val_loss: 6593.8999\n",
      "Epoch 6705/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7540.1473 - val_loss: 6593.4360\n",
      "Epoch 6706/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7539.6838 - val_loss: 6592.9722\n",
      "Epoch 6707/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7539.2219 - val_loss: 6592.5122\n",
      "Epoch 6708/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7538.7577 - val_loss: 6592.0483\n",
      "Epoch 6709/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7538.2958 - val_loss: 6591.5850\n",
      "Epoch 6710/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7537.8340 - val_loss: 6591.1235\n",
      "Epoch 6711/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7537.3700 - val_loss: 6590.6587\n",
      "Epoch 6712/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7536.9091 - val_loss: 6590.1982\n",
      "Epoch 6713/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7536.4455 - val_loss: 6589.7344\n",
      "Epoch 6714/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7535.9822 - val_loss: 6589.2734\n",
      "Epoch 6715/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7535.5208 - val_loss: 6588.8086\n",
      "Epoch 6716/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7535.0572 - val_loss: 6588.3462\n",
      "Epoch 6717/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7534.5938 - val_loss: 6587.8857\n",
      "Epoch 6718/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7534.1315 - val_loss: 6587.4209\n",
      "Epoch 6719/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7533.6691 - val_loss: 6586.9595\n",
      "Epoch 6720/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7533.2067 - val_loss: 6586.4961\n",
      "Epoch 6721/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7532.7431 - val_loss: 6586.0332\n",
      "Epoch 6722/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7532.2807 - val_loss: 6585.5693\n",
      "Epoch 6723/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 7533.25 - 0s 120us/step - loss: 7531.8186 - val_loss: 6585.1079\n",
      "Epoch 6724/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7531.3554 - val_loss: 6584.6470\n",
      "Epoch 6725/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7530.8930 - val_loss: 6584.1816\n",
      "Epoch 6726/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7530.4294 - val_loss: 6583.7183\n",
      "Epoch 6727/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7529.9677 - val_loss: 6583.2573\n",
      "Epoch 6728/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7529.5052 - val_loss: 6582.7935\n",
      "Epoch 6729/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7529.0429 - val_loss: 6582.3315\n",
      "Epoch 6730/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7528.5800 - val_loss: 6581.8691\n",
      "Epoch 6731/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7528.1160 - val_loss: 6581.4072\n",
      "Epoch 6732/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7527.6544 - val_loss: 6580.9419\n",
      "Epoch 6733/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7527.1912 - val_loss: 6580.4805\n",
      "Epoch 6734/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7526.7277 - val_loss: 6580.0190\n",
      "Epoch 6735/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7526.2654 - val_loss: 6579.5542\n",
      "Epoch 6736/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7525.8031 - val_loss: 6579.0918\n",
      "Epoch 6737/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7525.3400 - val_loss: 6578.6294\n",
      "Epoch 6738/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7524.8770 - val_loss: 6578.1680\n",
      "Epoch 6739/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7524.4147 - val_loss: 6577.7031\n",
      "Epoch 6740/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7523.9522 - val_loss: 6577.2407\n",
      "Epoch 6741/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7523.4899 - val_loss: 6576.7803\n",
      "Epoch 6742/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7523.0275 - val_loss: 6576.3164\n",
      "Epoch 6743/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 7522.5648 - val_loss: 6575.8540\n",
      "Epoch 6744/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7522.1014 - val_loss: 6575.3921\n",
      "Epoch 6745/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7521.6389 - val_loss: 6574.9277\n",
      "Epoch 6746/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7521.1768 - val_loss: 6574.4648\n",
      "Epoch 6747/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7520.7133 - val_loss: 6574.0029\n",
      "Epoch 6748/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7520.2504 - val_loss: 6573.5415\n",
      "Epoch 6749/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7519.7879 - val_loss: 6573.0762\n",
      "Epoch 6750/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7519.3254 - val_loss: 6572.6138\n",
      "Epoch 6751/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7518.8627 - val_loss: 6572.1528\n",
      "Epoch 6752/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7518.3986 - val_loss: 6571.6899\n",
      "Epoch 6753/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7517.9372 - val_loss: 6571.2266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6754/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7517.4749 - val_loss: 6570.7637\n",
      "Epoch 6755/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7517.0116 - val_loss: 6570.3013\n",
      "Epoch 6756/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7516.5504 - val_loss: 6569.8379\n",
      "Epoch 6757/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7516.0872 - val_loss: 6569.3750\n",
      "Epoch 6758/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7515.6238 - val_loss: 6568.9155\n",
      "Epoch 6759/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7515.1620 - val_loss: 6568.4497\n",
      "Epoch 6760/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 7514.6990 - val_loss: 6567.9873\n",
      "Epoch 6761/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7514.2353 - val_loss: 6567.5254\n",
      "Epoch 6762/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7513.7723 - val_loss: 6567.0625\n",
      "Epoch 6763/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7513.3106 - val_loss: 6566.5996\n",
      "Epoch 6764/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7512.8483 - val_loss: 6566.1362\n",
      "Epoch 6765/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7512.3842 - val_loss: 6565.6753\n",
      "Epoch 6766/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7511.9218 - val_loss: 6565.2109\n",
      "Epoch 6767/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7511.4595 - val_loss: 6564.7485\n",
      "Epoch 6768/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7510.9969 - val_loss: 6564.2876\n",
      "Epoch 6769/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 7510.5337 - val_loss: 6563.8223\n",
      "Epoch 6770/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7510.0706 - val_loss: 6563.3608\n",
      "Epoch 6771/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7509.6095 - val_loss: 6562.8984\n",
      "Epoch 6772/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7509.1462 - val_loss: 6562.4360\n",
      "Epoch 6773/10000\n",
      "750/750 [==============================] - 0s 166us/step - loss: 7508.6838 - val_loss: 6561.9712\n",
      "Epoch 6774/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 7508.2207 - val_loss: 6561.5093\n",
      "Epoch 6775/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 7507.7574 - val_loss: 6561.0488\n",
      "Epoch 6776/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7507.2952 - val_loss: 6560.5845\n",
      "Epoch 6777/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7506.8328 - val_loss: 6560.1211\n",
      "Epoch 6778/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7506.3689 - val_loss: 6559.6587\n",
      "Epoch 6779/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7505.9061 - val_loss: 6559.1968\n",
      "Epoch 6780/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7505.4443 - val_loss: 6558.7334\n",
      "Epoch 6781/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7504.9816 - val_loss: 6558.2695\n",
      "Epoch 6782/10000\n",
      "750/750 [==============================] - 0s 172us/step - loss: 7504.5188 - val_loss: 6557.8096\n",
      "Epoch 6783/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7504.0559 - val_loss: 6557.3457\n",
      "Epoch 6784/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7503.5933 - val_loss: 6556.8823\n",
      "Epoch 6785/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7503.1311 - val_loss: 6556.4209\n",
      "Epoch 6786/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7502.6671 - val_loss: 6555.9570\n",
      "Epoch 6787/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7502.2060 - val_loss: 6555.4941\n",
      "Epoch 6788/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7501.7427 - val_loss: 6555.0322\n",
      "Epoch 6789/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7501.2796 - val_loss: 6554.5698\n",
      "Epoch 6790/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7500.8183 - val_loss: 6554.1060\n",
      "Epoch 6791/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7500.3550 - val_loss: 6553.6431\n",
      "Epoch 6792/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7499.8916 - val_loss: 6553.1821\n",
      "Epoch 6793/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7499.4287 - val_loss: 6552.7178\n",
      "Epoch 6794/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7498.9663 - val_loss: 6552.2559\n",
      "Epoch 6795/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7498.5036 - val_loss: 6551.7930\n",
      "Epoch 6796/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7498.0403 - val_loss: 6551.3306\n",
      "Epoch 6797/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7497.5780 - val_loss: 6550.8667\n",
      "Epoch 6798/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 7508.67 - 0s 121us/step - loss: 7497.1159 - val_loss: 6550.4043\n",
      "Epoch 6799/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7496.6523 - val_loss: 6549.9443\n",
      "Epoch 6800/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7496.1922 - val_loss: 6549.4795\n",
      "Epoch 6801/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7495.7282 - val_loss: 6549.0151\n",
      "Epoch 6802/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7495.2648 - val_loss: 6548.5542\n",
      "Epoch 6803/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7494.8025 - val_loss: 6548.0903\n",
      "Epoch 6804/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7494.3403 - val_loss: 6547.6289\n",
      "Epoch 6805/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 7493.8772 - val_loss: 6547.1665\n",
      "Epoch 6806/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 7493.4131 - val_loss: 6546.7056\n",
      "Epoch 6807/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7492.9515 - val_loss: 6546.2402\n",
      "Epoch 6808/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 7492.4893 - val_loss: 6545.7773\n",
      "Epoch 6809/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7492.0258 - val_loss: 6545.3164\n",
      "Epoch 6810/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7491.5624 - val_loss: 6544.8516\n",
      "Epoch 6811/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7491.1001 - val_loss: 6544.3892\n",
      "Epoch 6812/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7490.6371 - val_loss: 6543.9277\n",
      "Epoch 6813/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7490.1751 - val_loss: 6543.4648\n",
      "Epoch 6814/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7489.7117 - val_loss: 6543.0015\n",
      "Epoch 6815/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7489.2505 - val_loss: 6542.5386\n",
      "Epoch 6816/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7488.7873 - val_loss: 6542.0781\n",
      "Epoch 6817/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7488.3251 - val_loss: 6541.6128\n",
      "Epoch 6818/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7487.8620 - val_loss: 6541.1504\n",
      "Epoch 6819/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7487.3983 - val_loss: 6540.6899\n",
      "Epoch 6820/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7486.9360 - val_loss: 6540.2261\n",
      "Epoch 6821/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7486.4743 - val_loss: 6539.7612\n",
      "Epoch 6822/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7486.0107 - val_loss: 6539.3003\n",
      "Epoch 6823/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7485.5476 - val_loss: 6538.8389\n",
      "Epoch 6824/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7485.0849 - val_loss: 6538.3740\n",
      "Epoch 6825/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7484.6222 - val_loss: 6537.9102\n",
      "Epoch 6826/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7484.1605 - val_loss: 6537.4497\n",
      "Epoch 6827/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7483.6964 - val_loss: 6536.9863\n",
      "Epoch 6828/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7483.2346 - val_loss: 6536.5234\n",
      "Epoch 6829/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7482.7721 - val_loss: 6536.0615\n",
      "Epoch 6830/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7482.3085 - val_loss: 6535.5996\n",
      "Epoch 6831/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7481.8476 - val_loss: 6535.1348\n",
      "Epoch 6832/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7481.3841 - val_loss: 6534.6719\n",
      "Epoch 6833/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7480.9208 - val_loss: 6534.2124\n",
      "Epoch 6834/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7480.4592 - val_loss: 6533.7485\n",
      "Epoch 6835/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7479.9961 - val_loss: 6533.2847\n",
      "Epoch 6836/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7479.5327 - val_loss: 6532.8232\n",
      "Epoch 6837/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7479.0696 - val_loss: 6532.3594\n",
      "Epoch 6838/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7478.6076 - val_loss: 6531.8970\n",
      "Epoch 6839/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7478.1453 - val_loss: 6531.4336\n",
      "Epoch 6840/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7477.6812 - val_loss: 6530.9722\n",
      "Epoch 6841/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7477.2187 - val_loss: 6530.5073\n",
      "Epoch 6842/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7476.7568 - val_loss: 6530.0454\n",
      "Epoch 6843/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7476.2940 - val_loss: 6529.5845\n",
      "Epoch 6844/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7475.8315 - val_loss: 6529.1206\n",
      "Epoch 6845/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7475.3682 - val_loss: 6528.6587\n",
      "Epoch 6846/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7474.9065 - val_loss: 6528.1948\n",
      "Epoch 6847/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7474.4432 - val_loss: 6527.7334\n",
      "Epoch 6848/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7473.9812 - val_loss: 6527.2690\n",
      "Epoch 6849/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7473.5181 - val_loss: 6526.8066\n",
      "Epoch 6850/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7473.0544 - val_loss: 6526.3462\n",
      "Epoch 6851/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7472.5921 - val_loss: 6525.8818\n",
      "Epoch 6852/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7472.1300 - val_loss: 6525.4185\n",
      "Epoch 6853/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 7471.6667 - val_loss: 6524.9565\n",
      "Epoch 6854/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7471.2038 - val_loss: 6524.4941\n",
      "Epoch 6855/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7470.7413 - val_loss: 6524.0303\n",
      "Epoch 6856/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7470.2785 - val_loss: 6523.5684\n",
      "Epoch 6857/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7469.8159 - val_loss: 6523.1079\n",
      "Epoch 6858/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7469.3534 - val_loss: 6522.6431\n",
      "Epoch 6859/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7468.8905 - val_loss: 6522.1792\n",
      "Epoch 6860/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7468.4281 - val_loss: 6521.7183\n",
      "Epoch 6861/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7467.9661 - val_loss: 6521.2539\n",
      "Epoch 6862/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7467.5036 - val_loss: 6520.7920\n",
      "Epoch 6863/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7467.0403 - val_loss: 6520.3315\n",
      "Epoch 6864/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7466.5769 - val_loss: 6519.8667\n",
      "Epoch 6865/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7466.1151 - val_loss: 6519.4043\n",
      "Epoch 6866/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 7465.6521 - val_loss: 6518.9404\n",
      "Epoch 6867/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7465.1885 - val_loss: 6518.4795\n",
      "Epoch 6868/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7464.7257 - val_loss: 6518.0151\n",
      "Epoch 6869/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7464.2640 - val_loss: 6517.5532\n",
      "Epoch 6870/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7463.8016 - val_loss: 6517.0903\n",
      "Epoch 6871/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7463.3373 - val_loss: 6516.6279\n",
      "Epoch 6872/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7462.8756 - val_loss: 6516.1636\n",
      "Epoch 6873/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7462.4128 - val_loss: 6515.7017\n",
      "Epoch 6874/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7461.9495 - val_loss: 6515.2407\n",
      "Epoch 6875/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 7461.4891 - val_loss: 6514.7764\n",
      "Epoch 6876/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7461.0251 - val_loss: 6514.3140\n",
      "Epoch 6877/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7460.5622 - val_loss: 6513.8530\n",
      "Epoch 6878/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7460.0999 - val_loss: 6513.3892\n",
      "Epoch 6879/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7459.6374 - val_loss: 6512.9253\n",
      "Epoch 6880/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7459.1741 - val_loss: 6512.4639\n",
      "Epoch 6881/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7458.7107 - val_loss: 6512.0024\n",
      "Epoch 6882/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7458.2486 - val_loss: 6511.5386\n",
      "Epoch 6883/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7457.7863 - val_loss: 6511.0737\n",
      "Epoch 6884/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7457.3230 - val_loss: 6510.6128\n",
      "Epoch 6885/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7456.8602 - val_loss: 6510.1499\n",
      "Epoch 6886/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7456.3978 - val_loss: 6509.6865\n",
      "Epoch 6887/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7455.9357 - val_loss: 6509.2261\n",
      "Epoch 6888/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7455.4722 - val_loss: 6508.7612\n",
      "Epoch 6889/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7455.0087 - val_loss: 6508.2983\n",
      "Epoch 6890/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7454.5481 - val_loss: 6507.8359\n",
      "Epoch 6891/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7454.0843 - val_loss: 6507.3750\n",
      "Epoch 6892/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7453.6227 - val_loss: 6506.9097\n",
      "Epoch 6893/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7453.1597 - val_loss: 6506.4482\n",
      "Epoch 6894/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7452.6963 - val_loss: 6505.9873\n",
      "Epoch 6895/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7452.2333 - val_loss: 6505.5225\n",
      "Epoch 6896/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7451.7716 - val_loss: 6505.0596\n",
      "Epoch 6897/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 117us/step - loss: 7451.3077 - val_loss: 6504.5972\n",
      "Epoch 6898/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7450.8449 - val_loss: 6504.1362\n",
      "Epoch 6899/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7450.3822 - val_loss: 6503.6709\n",
      "Epoch 6900/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7449.9203 - val_loss: 6503.2095\n",
      "Epoch 6901/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7449.4576 - val_loss: 6502.7485\n",
      "Epoch 6902/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7448.9940 - val_loss: 6502.2837\n",
      "Epoch 6903/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7448.5322 - val_loss: 6501.8198\n",
      "Epoch 6904/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7448.0692 - val_loss: 6501.3584\n",
      "Epoch 6905/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7447.6057 - val_loss: 6500.8970\n",
      "Epoch 6906/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7447.1447 - val_loss: 6500.4321\n",
      "Epoch 6907/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7446.6813 - val_loss: 6499.9697\n",
      "Epoch 6908/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7446.2183 - val_loss: 6499.5098\n",
      "Epoch 6909/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7445.7561 - val_loss: 6499.0459\n",
      "Epoch 6910/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7445.2930 - val_loss: 6498.5815\n",
      "Epoch 6911/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7444.8304 - val_loss: 6498.1206\n",
      "Epoch 6912/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 7444.3669 - val_loss: 6497.6558\n",
      "Epoch 6913/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7443.9049 - val_loss: 6497.1943\n",
      "Epoch 6914/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7443.4422 - val_loss: 6496.7310\n",
      "Epoch 6915/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7442.9791 - val_loss: 6496.2705\n",
      "Epoch 6916/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7442.5164 - val_loss: 6495.8057\n",
      "Epoch 6917/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7442.0541 - val_loss: 6495.3428\n",
      "Epoch 6918/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7441.5913 - val_loss: 6494.8818\n",
      "Epoch 6919/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7441.1292 - val_loss: 6494.4170\n",
      "Epoch 6920/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7440.6669 - val_loss: 6493.9556\n",
      "Epoch 6921/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7440.2035 - val_loss: 6493.4932\n",
      "Epoch 6922/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7439.7403 - val_loss: 6493.0303\n",
      "Epoch 6923/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7439.2784 - val_loss: 6492.5664\n",
      "Epoch 6924/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7438.8159 - val_loss: 6492.1045\n",
      "Epoch 6925/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7438.3517 - val_loss: 6491.6440\n",
      "Epoch 6926/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7437.8896 - val_loss: 6491.1782\n",
      "Epoch 6927/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7437.4276 - val_loss: 6490.7153\n",
      "Epoch 6928/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7436.9637 - val_loss: 6490.2539\n",
      "Epoch 6929/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7436.5013 - val_loss: 6489.7915\n",
      "Epoch 6930/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7436.0383 - val_loss: 6489.3276\n",
      "Epoch 6931/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7435.5763 - val_loss: 6488.8657\n",
      "Epoch 6932/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7435.1139 - val_loss: 6488.4053\n",
      "Epoch 6933/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7434.6503 - val_loss: 6487.9404\n",
      "Epoch 6934/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7434.1890 - val_loss: 6487.4766\n",
      "Epoch 6935/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7433.7255 - val_loss: 6487.0151\n",
      "Epoch 6936/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7433.2631 - val_loss: 6486.5513\n",
      "Epoch 6937/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7432.8006 - val_loss: 6486.0889\n",
      "Epoch 6938/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7432.3374 - val_loss: 6485.6279\n",
      "Epoch 6939/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7431.8741 - val_loss: 6485.1655\n",
      "Epoch 6940/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7431.4123 - val_loss: 6484.7017\n",
      "Epoch 6941/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7430.9493 - val_loss: 6484.2378\n",
      "Epoch 6942/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7430.4864 - val_loss: 6483.7773\n",
      "Epoch 6943/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7430.0231 - val_loss: 6483.3125\n",
      "Epoch 6944/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7429.5608 - val_loss: 6482.8511\n",
      "Epoch 6945/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7429.0984 - val_loss: 6482.3872\n",
      "Epoch 6946/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7428.6352 - val_loss: 6481.9253\n",
      "Epoch 6947/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7428.1731 - val_loss: 6481.4609\n",
      "Epoch 6948/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7427.7100 - val_loss: 6480.9990\n",
      "Epoch 6949/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7427.2471 - val_loss: 6480.5386\n",
      "Epoch 6950/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7426.7860 - val_loss: 6480.0737\n",
      "Epoch 6951/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7426.3229 - val_loss: 6479.6108\n",
      "Epoch 6952/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7425.8593 - val_loss: 6479.1499\n",
      "Epoch 6953/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7425.3968 - val_loss: 6478.6865\n",
      "Epoch 6954/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7424.9345 - val_loss: 6478.2222\n",
      "Epoch 6955/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7424.4714 - val_loss: 6477.7607\n",
      "Epoch 6956/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7424.0075 - val_loss: 6477.2998\n",
      "Epoch 6957/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7423.5458 - val_loss: 6476.8350\n",
      "Epoch 6958/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7423.0839 - val_loss: 6476.3721\n",
      "Epoch 6959/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7422.6199 - val_loss: 6475.9102\n",
      "Epoch 6960/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7422.1571 - val_loss: 6475.4473\n",
      "Epoch 6961/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7421.6950 - val_loss: 6474.9834\n",
      "Epoch 6962/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7421.2328 - val_loss: 6474.5225\n",
      "Epoch 6963/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7420.7707 - val_loss: 6474.0596\n",
      "Epoch 6964/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7420.3071 - val_loss: 6473.5962\n",
      "Epoch 6965/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7419.8450 - val_loss: 6473.1323\n",
      "Epoch 6966/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7419.3811 - val_loss: 6472.6733\n",
      "Epoch 6967/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7418.9200 - val_loss: 6472.2080\n",
      "Epoch 6968/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7418.4568 - val_loss: 6471.7456\n",
      "Epoch 6969/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 137us/step - loss: 7417.9931 - val_loss: 6471.2837\n",
      "Epoch 6970/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7417.5303 - val_loss: 6470.8198\n",
      "Epoch 6971/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7417.0684 - val_loss: 6470.3560\n",
      "Epoch 6972/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7416.6053 - val_loss: 6469.8945\n",
      "Epoch 6973/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7416.1420 - val_loss: 6469.4331\n",
      "Epoch 6974/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7415.6792 - val_loss: 6468.9683\n",
      "Epoch 6975/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7415.2176 - val_loss: 6468.5068\n",
      "Epoch 6976/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7414.7548 - val_loss: 6468.0459\n",
      "Epoch 6977/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7414.2916 - val_loss: 6467.5815\n",
      "Epoch 6978/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7413.8304 - val_loss: 6467.1182\n",
      "Epoch 6979/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7413.3666 - val_loss: 6466.6558\n",
      "Epoch 6980/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7412.9039 - val_loss: 6466.1943\n",
      "Epoch 6981/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7412.4422 - val_loss: 6465.7295\n",
      "Epoch 6982/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7411.9790 - val_loss: 6465.2671\n",
      "Epoch 6983/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7411.5151 - val_loss: 6464.8071\n",
      "Epoch 6984/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7411.0532 - val_loss: 6464.3428\n",
      "Epoch 6985/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7410.5908 - val_loss: 6463.8789\n",
      "Epoch 6986/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7410.1273 - val_loss: 6463.4180\n",
      "Epoch 6987/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7409.6641 - val_loss: 6462.9546\n",
      "Epoch 6988/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7409.2021 - val_loss: 6462.4907\n",
      "Epoch 6989/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7408.7395 - val_loss: 6462.0303\n",
      "Epoch 6990/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7408.2762 - val_loss: 6461.5669\n",
      "Epoch 6991/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7407.8139 - val_loss: 6461.1040\n",
      "Epoch 6992/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7407.3510 - val_loss: 6460.6401\n",
      "Epoch 6993/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7406.8894 - val_loss: 6460.1782\n",
      "Epoch 6994/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7406.4269 - val_loss: 6459.7148\n",
      "Epoch 6995/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7405.9641 - val_loss: 6459.2524\n",
      "Epoch 6996/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7405.5008 - val_loss: 6458.7905\n",
      "Epoch 6997/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7405.0375 - val_loss: 6458.3276\n",
      "Epoch 6998/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7404.5761 - val_loss: 6457.8638\n",
      "Epoch 6999/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7404.1129 - val_loss: 6457.4014\n",
      "Epoch 7000/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7403.6487 - val_loss: 6456.9404\n",
      "Epoch 7001/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7403.1865 - val_loss: 6456.4761\n",
      "Epoch 7002/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7402.7249 - val_loss: 6456.0137\n",
      "Epoch 7003/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7402.2616 - val_loss: 6455.5503\n",
      "Epoch 7004/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7401.7988 - val_loss: 6455.0889\n",
      "Epoch 7005/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7401.3355 - val_loss: 6454.6240\n",
      "Epoch 7006/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7400.8737 - val_loss: 6454.1636\n",
      "Epoch 7007/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7400.4109 - val_loss: 6453.7017\n",
      "Epoch 7008/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7399.9473 - val_loss: 6453.2373\n",
      "Epoch 7009/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7399.4860 - val_loss: 6452.7749\n",
      "Epoch 7010/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7399.0229 - val_loss: 6452.3125\n",
      "Epoch 7011/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7398.5604 - val_loss: 6451.8477\n",
      "Epoch 7012/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7398.0981 - val_loss: 6451.3862\n",
      "Epoch 7013/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7397.6350 - val_loss: 6450.9248\n",
      "Epoch 7014/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7397.1715 - val_loss: 6450.4629\n",
      "Epoch 7015/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7396.7095 - val_loss: 6449.9985\n",
      "Epoch 7016/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7396.2464 - val_loss: 6449.5347\n",
      "Epoch 7017/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7395.7833 - val_loss: 6449.0737\n",
      "Epoch 7018/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7395.3202 - val_loss: 6448.6108\n",
      "Epoch 7019/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7394.8583 - val_loss: 6448.1475\n",
      "Epoch 7020/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7394.3964 - val_loss: 6447.6860\n",
      "Epoch 7021/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7393.9324 - val_loss: 6447.2222\n",
      "Epoch 7022/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7393.4720 - val_loss: 6446.7593\n",
      "Epoch 7023/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7393.0072 - val_loss: 6446.2969\n",
      "Epoch 7024/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7392.5449 - val_loss: 6445.8350\n",
      "Epoch 7025/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7392.0832 - val_loss: 6445.3711\n",
      "Epoch 7026/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7391.6203 - val_loss: 6444.9087\n",
      "Epoch 7027/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7391.1571 - val_loss: 6444.4482\n",
      "Epoch 7028/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7390.6939 - val_loss: 6443.9834\n",
      "Epoch 7029/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7390.2318 - val_loss: 6443.5195\n",
      "Epoch 7030/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7389.7692 - val_loss: 6443.0581\n",
      "Epoch 7031/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7389.3048 - val_loss: 6442.5962\n",
      "Epoch 7032/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7388.8429 - val_loss: 6442.1323\n",
      "Epoch 7033/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7388.3807 - val_loss: 6441.6685\n",
      "Epoch 7034/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7387.9170 - val_loss: 6441.2080\n",
      "Epoch 7035/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7387.4547 - val_loss: 6440.7446\n",
      "Epoch 7036/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7386.9921 - val_loss: 6440.2808\n",
      "Epoch 7037/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7386.5302 - val_loss: 6439.8193\n",
      "Epoch 7038/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7386.0675 - val_loss: 6439.3579\n",
      "Epoch 7039/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7385.6054 - val_loss: 6438.8940\n",
      "Epoch 7040/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7385.1419 - val_loss: 6438.4307\n",
      "Epoch 7041/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7384.6785 - val_loss: 6437.9707\n",
      "Epoch 7042/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7384.2169 - val_loss: 6437.5059\n",
      "Epoch 7043/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7383.7538 - val_loss: 6437.0420\n",
      "Epoch 7044/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7383.2905 - val_loss: 6436.5815\n",
      "Epoch 7045/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7382.8274 - val_loss: 6436.1167\n",
      "Epoch 7046/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7382.3655 - val_loss: 6435.6543\n",
      "Epoch 7047/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7381.9024 - val_loss: 6435.1919\n",
      "Epoch 7048/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7381.4395 - val_loss: 6434.7305\n",
      "Epoch 7049/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 7380.9763 - val_loss: 6434.2651\n",
      "Epoch 7050/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7380.5152 - val_loss: 6433.8032\n",
      "Epoch 7051/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7380.0517 - val_loss: 6433.3428\n",
      "Epoch 7052/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7379.5887 - val_loss: 6432.8779\n",
      "Epoch 7053/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7379.1273 - val_loss: 6432.4165\n",
      "Epoch 7054/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7378.6644 - val_loss: 6431.9531\n",
      "Epoch 7055/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7378.2008 - val_loss: 6431.4907\n",
      "Epoch 7056/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7377.7400 - val_loss: 6431.0273\n",
      "Epoch 7057/10000\n",
      "750/750 [==============================] - 0s 164us/step - loss: 7377.2759 - val_loss: 6430.5654\n",
      "Epoch 7058/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 7376.8123 - val_loss: 6430.1045\n",
      "Epoch 7059/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 7376.3501 - val_loss: 6429.6392\n",
      "Epoch 7060/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7375.8877 - val_loss: 6429.1763\n",
      "Epoch 7061/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7375.4251 - val_loss: 6428.7148\n",
      "Epoch 7062/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7374.9612 - val_loss: 6428.2524\n",
      "Epoch 7063/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7374.4991 - val_loss: 6427.7886\n",
      "Epoch 7064/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 7374.0373 - val_loss: 6427.3267\n",
      "Epoch 7065/10000\n",
      "750/750 [==============================] - 0s 164us/step - loss: 7373.5739 - val_loss: 6426.8657\n",
      "Epoch 7066/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7373.1112 - val_loss: 6426.4004\n",
      "Epoch 7067/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7372.6483 - val_loss: 6425.9365\n",
      "Epoch 7068/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7372.1864 - val_loss: 6425.4761\n",
      "Epoch 7069/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7371.7239 - val_loss: 6425.0112\n",
      "Epoch 7070/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7371.2610 - val_loss: 6424.5498\n",
      "Epoch 7071/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7370.7980 - val_loss: 6424.0889\n",
      "Epoch 7072/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7370.3347 - val_loss: 6423.6240\n",
      "Epoch 7073/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7369.8730 - val_loss: 6423.1602\n",
      "Epoch 7074/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7369.4099 - val_loss: 6422.6982\n",
      "Epoch 7075/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7368.9463 - val_loss: 6422.2378\n",
      "Epoch 7076/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7368.4839 - val_loss: 6421.7725\n",
      "Epoch 7077/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7368.0218 - val_loss: 6421.3110\n",
      "Epoch 7078/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7367.5587 - val_loss: 6420.8477\n",
      "Epoch 7079/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7367.0959 - val_loss: 6420.3857\n",
      "Epoch 7080/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7366.6332 - val_loss: 6419.9219\n",
      "Epoch 7081/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7366.1707 - val_loss: 6419.4600\n",
      "Epoch 7082/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7365.7086 - val_loss: 6418.9990\n",
      "Epoch 7083/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7365.2456 - val_loss: 6418.5337\n",
      "Epoch 7084/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7364.7836 - val_loss: 6418.0718\n",
      "Epoch 7085/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7364.3202 - val_loss: 6417.6108\n",
      "Epoch 7086/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7363.8577 - val_loss: 6417.1475\n",
      "Epoch 7087/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 7363.3952 - val_loss: 6416.6836\n",
      "Epoch 7088/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7362.9320 - val_loss: 6416.2212\n",
      "Epoch 7089/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7362.4685 - val_loss: 6415.7607\n",
      "Epoch 7090/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7362.0063 - val_loss: 6415.2959\n",
      "Epoch 7091/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7361.5442 - val_loss: 6414.8320\n",
      "Epoch 7092/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7361.0809 - val_loss: 6414.3711\n",
      "Epoch 7093/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7360.6173 - val_loss: 6413.9072\n",
      "Epoch 7094/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7360.1558 - val_loss: 6413.4448\n",
      "Epoch 7095/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7359.6934 - val_loss: 6412.9834\n",
      "Epoch 7096/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7359.2295 - val_loss: 6412.5205\n",
      "Epoch 7097/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7358.7692 - val_loss: 6412.0566\n",
      "Epoch 7098/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7358.3052 - val_loss: 6411.5933\n",
      "Epoch 7099/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7357.8419 - val_loss: 6411.1343\n",
      "Epoch 7100/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7357.3806 - val_loss: 6410.6680\n",
      "Epoch 7101/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7356.9175 - val_loss: 6410.2065\n",
      "Epoch 7102/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 7356.4540 - val_loss: 6409.7446\n",
      "Epoch 7103/10000\n",
      "750/750 [==============================] - 0s 107us/step - loss: 7355.9911 - val_loss: 6409.2808\n",
      "Epoch 7104/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7355.5294 - val_loss: 6408.8169\n",
      "Epoch 7105/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7355.0662 - val_loss: 6408.3555\n",
      "Epoch 7106/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7354.6029 - val_loss: 6407.8940\n",
      "Epoch 7107/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7354.1404 - val_loss: 6407.4292\n",
      "Epoch 7108/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7353.6786 - val_loss: 6406.9678\n",
      "Epoch 7109/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7353.2148 - val_loss: 6406.5059\n",
      "Epoch 7110/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7352.7523 - val_loss: 6406.0420\n",
      "Epoch 7111/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7352.2890 - val_loss: 6405.5796\n",
      "Epoch 7112/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7351.8282 - val_loss: 6405.1167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7113/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7351.3647 - val_loss: 6404.6553\n",
      "Epoch 7114/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7350.9028 - val_loss: 6404.1904\n",
      "Epoch 7115/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7350.4396 - val_loss: 6403.7290\n",
      "Epoch 7116/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7349.9760 - val_loss: 6403.2681\n",
      "Epoch 7117/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7349.5139 - val_loss: 6402.8032\n",
      "Epoch 7118/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7349.0516 - val_loss: 6402.3398\n",
      "Epoch 7119/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7348.5876 - val_loss: 6401.8779\n",
      "Epoch 7120/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7348.1249 - val_loss: 6401.4155\n",
      "Epoch 7121/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7347.6628 - val_loss: 6400.9517\n",
      "Epoch 7122/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7347.1996 - val_loss: 6400.4888\n",
      "Epoch 7123/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7346.7375 - val_loss: 6400.0278\n",
      "Epoch 7124/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7346.2739 - val_loss: 6399.5640\n",
      "Epoch 7125/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7345.8124 - val_loss: 6399.1011\n",
      "Epoch 7126/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7345.3496 - val_loss: 6398.6392\n",
      "Epoch 7127/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7344.8863 - val_loss: 6398.1753\n",
      "Epoch 7128/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7344.4248 - val_loss: 6397.7139\n",
      "Epoch 7129/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7343.9616 - val_loss: 6397.2500\n",
      "Epoch 7130/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7343.4980 - val_loss: 6396.7886\n",
      "Epoch 7131/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7343.0368 - val_loss: 6396.3237\n",
      "Epoch 7132/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7342.5731 - val_loss: 6395.8623\n",
      "Epoch 7133/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7342.1102 - val_loss: 6395.4014\n",
      "Epoch 7134/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7341.6474 - val_loss: 6394.9365\n",
      "Epoch 7135/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7341.1847 - val_loss: 6394.4727\n",
      "Epoch 7136/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7340.7221 - val_loss: 6394.0112\n",
      "Epoch 7137/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7340.2589 - val_loss: 6393.5498\n",
      "Epoch 7138/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 7339.7964 - val_loss: 6393.0850\n",
      "Epoch 7139/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7339.3347 - val_loss: 6392.6240\n",
      "Epoch 7140/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7338.8712 - val_loss: 6392.1636\n",
      "Epoch 7141/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7338.4094 - val_loss: 6391.6982\n",
      "Epoch 7142/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7337.9462 - val_loss: 6391.2344\n",
      "Epoch 7143/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7337.4834 - val_loss: 6390.7725\n",
      "Epoch 7144/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7337.0209 - val_loss: 6390.3096\n",
      "Epoch 7145/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7336.5584 - val_loss: 6389.8462\n",
      "Epoch 7146/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7336.0959 - val_loss: 6389.3857\n",
      "Epoch 7147/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7335.6318 - val_loss: 6388.9233\n",
      "Epoch 7148/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7335.1703 - val_loss: 6388.4595\n",
      "Epoch 7149/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7334.7078 - val_loss: 6387.9961\n",
      "Epoch 7150/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7334.2434 - val_loss: 6387.5347\n",
      "Epoch 7151/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7333.7812 - val_loss: 6387.0698\n",
      "Epoch 7152/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7333.3187 - val_loss: 6386.6084\n",
      "Epoch 7153/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7332.8561 - val_loss: 6386.1455\n",
      "Epoch 7154/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7332.3938 - val_loss: 6385.6831\n",
      "Epoch 7155/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7331.9305 - val_loss: 6385.2197\n",
      "Epoch 7156/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7331.4685 - val_loss: 6384.7573\n",
      "Epoch 7157/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7331.0055 - val_loss: 6384.2969\n",
      "Epoch 7158/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7330.5438 - val_loss: 6383.8315\n",
      "Epoch 7159/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7330.0807 - val_loss: 6383.3696\n",
      "Epoch 7160/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7329.6171 - val_loss: 6382.9072\n",
      "Epoch 7161/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7329.1546 - val_loss: 6382.4443\n",
      "Epoch 7162/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7328.6924 - val_loss: 6381.9805\n",
      "Epoch 7163/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7328.2290 - val_loss: 6381.5190\n",
      "Epoch 7164/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7327.7662 - val_loss: 6381.0571\n",
      "Epoch 7165/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7327.3037 - val_loss: 6380.5928\n",
      "Epoch 7166/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7326.8411 - val_loss: 6380.1294\n",
      "Epoch 7167/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7326.3782 - val_loss: 6379.6685\n",
      "Epoch 7168/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7325.9143 - val_loss: 6379.2056\n",
      "Epoch 7169/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7325.4529 - val_loss: 6378.7417\n",
      "Epoch 7170/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7324.9906 - val_loss: 6378.2803\n",
      "Epoch 7171/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7324.5272 - val_loss: 6377.8169\n",
      "Epoch 7172/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7324.0660 - val_loss: 6377.3545\n",
      "Epoch 7173/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7323.6028 - val_loss: 6376.8901\n",
      "Epoch 7174/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7323.1394 - val_loss: 6376.4307\n",
      "Epoch 7175/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7322.6779 - val_loss: 6375.9653\n",
      "Epoch 7176/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7322.2146 - val_loss: 6375.5029\n",
      "Epoch 7177/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7321.7511 - val_loss: 6375.0420\n",
      "Epoch 7178/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7321.2880 - val_loss: 6374.5776\n",
      "Epoch 7179/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7320.8263 - val_loss: 6374.1157\n",
      "Epoch 7180/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7320.3641 - val_loss: 6373.6523\n",
      "Epoch 7181/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7319.8997 - val_loss: 6373.1904\n",
      "Epoch 7182/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7319.4374 - val_loss: 6372.7261\n",
      "Epoch 7183/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7318.9756 - val_loss: 6372.2642\n",
      "Epoch 7184/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7318.5126 - val_loss: 6371.8032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7185/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7318.0501 - val_loss: 6371.3389\n",
      "Epoch 7186/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7317.5870 - val_loss: 6370.8765\n",
      "Epoch 7187/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7317.1251 - val_loss: 6370.4136\n",
      "Epoch 7188/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7316.6617 - val_loss: 6369.9517\n",
      "Epoch 7189/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7316.2002 - val_loss: 6369.4873\n",
      "Epoch 7190/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7315.7365 - val_loss: 6369.0264\n",
      "Epoch 7191/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7315.2731 - val_loss: 6368.5649\n",
      "Epoch 7192/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7314.8109 - val_loss: 6368.1011\n",
      "Epoch 7193/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7314.3486 - val_loss: 6367.6362\n",
      "Epoch 7194/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7313.8853 - val_loss: 6367.1753\n",
      "Epoch 7195/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7313.4220 - val_loss: 6366.7124\n",
      "Epoch 7196/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7312.9600 - val_loss: 6366.2490\n",
      "Epoch 7197/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7312.4974 - val_loss: 6365.7876\n",
      "Epoch 7198/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7312.0347 - val_loss: 6365.3247\n",
      "Epoch 7199/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7311.5709 - val_loss: 6364.8613\n",
      "Epoch 7200/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7311.1105 - val_loss: 6364.3975\n",
      "Epoch 7201/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7310.6469 - val_loss: 6363.9365\n",
      "Epoch 7202/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7310.1848 - val_loss: 6363.4722\n",
      "Epoch 7203/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7309.7222 - val_loss: 6363.0107\n",
      "Epoch 7204/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7309.2587 - val_loss: 6362.5483\n",
      "Epoch 7205/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7308.7956 - val_loss: 6362.0850\n",
      "Epoch 7206/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7308.3340 - val_loss: 6361.6221\n",
      "Epoch 7207/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7307.8706 - val_loss: 6361.1587\n",
      "Epoch 7208/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7307.4072 - val_loss: 6360.6982\n",
      "Epoch 7209/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7306.9446 - val_loss: 6360.2334\n",
      "Epoch 7210/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7306.4827 - val_loss: 6359.7720\n",
      "Epoch 7211/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7306.0196 - val_loss: 6359.3096\n",
      "Epoch 7212/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7305.5564 - val_loss: 6358.8462\n",
      "Epoch 7213/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7305.0941 - val_loss: 6358.3823\n",
      "Epoch 7214/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7304.6316 - val_loss: 6357.9209\n",
      "Epoch 7215/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7304.1684 - val_loss: 6357.4600\n",
      "Epoch 7216/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7303.7075 - val_loss: 6356.9956\n",
      "Epoch 7217/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7303.2441 - val_loss: 6356.5322\n",
      "Epoch 7218/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7302.7808 - val_loss: 6356.0698\n",
      "Epoch 7219/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7302.3183 - val_loss: 6355.6060\n",
      "Epoch 7220/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7301.8560 - val_loss: 6355.1440\n",
      "Epoch 7221/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7301.3929 - val_loss: 6354.6821\n",
      "Epoch 7222/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7300.9289 - val_loss: 6354.2197\n",
      "Epoch 7223/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7300.4673 - val_loss: 6353.7568\n",
      "Epoch 7224/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 7300.0050 - val_loss: 6353.2930\n",
      "Epoch 7225/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7299.5412 - val_loss: 6352.8320\n",
      "Epoch 7226/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7299.0791 - val_loss: 6352.3682\n",
      "Epoch 7227/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7298.6164 - val_loss: 6351.9053\n",
      "Epoch 7228/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7298.1538 - val_loss: 6351.4443\n",
      "Epoch 7229/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7297.6908 - val_loss: 6350.9795\n",
      "Epoch 7230/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7297.2273 - val_loss: 6350.5171\n",
      "Epoch 7231/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7296.7661 - val_loss: 6350.0542\n",
      "Epoch 7232/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7296.3031 - val_loss: 6349.5933\n",
      "Epoch 7233/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7295.8413 - val_loss: 6349.1289\n",
      "Epoch 7234/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7295.3779 - val_loss: 6348.6670\n",
      "Epoch 7235/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7294.9144 - val_loss: 6348.2065\n",
      "Epoch 7236/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7294.4518 - val_loss: 6347.7407\n",
      "Epoch 7237/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7293.9901 - val_loss: 6347.2778\n",
      "Epoch 7238/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7293.5264 - val_loss: 6346.8164\n",
      "Epoch 7239/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7293.0634 - val_loss: 6346.3545\n",
      "Epoch 7240/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7292.6008 - val_loss: 6345.8901\n",
      "Epoch 7241/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7292.1390 - val_loss: 6345.4263\n",
      "Epoch 7242/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7291.6762 - val_loss: 6344.9653\n",
      "Epoch 7243/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7291.2124 - val_loss: 6344.5029\n",
      "Epoch 7244/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7290.7509 - val_loss: 6344.0386\n",
      "Epoch 7245/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7290.2877 - val_loss: 6343.5767\n",
      "Epoch 7246/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7289.8248 - val_loss: 6343.1152\n",
      "Epoch 7247/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7289.3632 - val_loss: 6342.6514\n",
      "Epoch 7248/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7288.8999 - val_loss: 6342.1890\n",
      "Epoch 7249/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7288.4370 - val_loss: 6341.7290\n",
      "Epoch 7250/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7287.9747 - val_loss: 6341.2622\n",
      "Epoch 7251/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7287.5116 - val_loss: 6340.7998\n",
      "Epoch 7252/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7287.0489 - val_loss: 6340.3389\n",
      "Epoch 7253/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7286.5854 - val_loss: 6339.8740\n",
      "Epoch 7254/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7286.1234 - val_loss: 6339.4126\n",
      "Epoch 7255/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7285.6612 - val_loss: 6338.9497\n",
      "Epoch 7256/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7285.1968 - val_loss: 6338.4878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7257/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7284.7345 - val_loss: 6338.0249\n",
      "Epoch 7258/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7284.2728 - val_loss: 6337.5615\n",
      "Epoch 7259/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7283.8096 - val_loss: 6337.1011\n",
      "Epoch 7260/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7283.3483 - val_loss: 6336.6362\n",
      "Epoch 7261/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7282.8851 - val_loss: 6336.1748\n",
      "Epoch 7262/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7282.4221 - val_loss: 6335.7109\n",
      "Epoch 7263/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7281.9591 - val_loss: 6335.2490\n",
      "Epoch 7264/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7281.4971 - val_loss: 6334.7847\n",
      "Epoch 7265/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7281.0337 - val_loss: 6334.3232\n",
      "Epoch 7266/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7280.5700 - val_loss: 6333.8623\n",
      "Epoch 7267/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7280.1080 - val_loss: 6333.3975\n",
      "Epoch 7268/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 7279.6463 - val_loss: 6332.9336\n",
      "Epoch 7269/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7279.1824 - val_loss: 6332.4722\n",
      "Epoch 7270/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7278.7195 - val_loss: 6332.0107\n",
      "Epoch 7271/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7278.2571 - val_loss: 6331.5459\n",
      "Epoch 7272/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7277.7946 - val_loss: 6331.0850\n",
      "Epoch 7273/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7277.3316 - val_loss: 6330.6235\n",
      "Epoch 7274/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7276.8690 - val_loss: 6330.1587\n",
      "Epoch 7275/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7276.4073 - val_loss: 6329.6948\n",
      "Epoch 7276/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7275.9438 - val_loss: 6329.2334\n",
      "Epoch 7277/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7275.4820 - val_loss: 6328.7705\n",
      "Epoch 7278/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7275.0197 - val_loss: 6328.3071\n",
      "Epoch 7279/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7274.5559 - val_loss: 6327.8462\n",
      "Epoch 7280/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7274.0926 - val_loss: 6327.3823\n",
      "Epoch 7281/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7273.6309 - val_loss: 6326.9185\n",
      "Epoch 7282/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7273.1677 - val_loss: 6326.4565\n",
      "Epoch 7283/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7272.7041 - val_loss: 6325.9961\n",
      "Epoch 7284/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7272.2416 - val_loss: 6325.5308\n",
      "Epoch 7285/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7271.7796 - val_loss: 6325.0693\n",
      "Epoch 7286/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7271.3170 - val_loss: 6324.6079\n",
      "Epoch 7287/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7270.8541 - val_loss: 6324.1440\n",
      "Epoch 7288/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7270.3911 - val_loss: 6323.6792\n",
      "Epoch 7289/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7269.9289 - val_loss: 6323.2183\n",
      "Epoch 7290/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7269.4664 - val_loss: 6322.7568\n",
      "Epoch 7291/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7269.0047 - val_loss: 6322.2920\n",
      "Epoch 7292/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7268.5412 - val_loss: 6321.8296\n",
      "Epoch 7293/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7268.0779 - val_loss: 6321.3682\n",
      "Epoch 7294/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7267.6154 - val_loss: 6320.9053\n",
      "Epoch 7295/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7267.1532 - val_loss: 6320.4414\n",
      "Epoch 7296/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7266.6898 - val_loss: 6319.9795\n",
      "Epoch 7297/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7266.2262 - val_loss: 6319.5181\n",
      "Epoch 7298/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7265.7645 - val_loss: 6319.0532\n",
      "Epoch 7299/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7265.3020 - val_loss: 6318.5898\n",
      "Epoch 7300/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7264.8385 - val_loss: 6318.1294\n",
      "Epoch 7301/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7264.3759 - val_loss: 6317.6665\n",
      "Epoch 7302/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7263.9137 - val_loss: 6317.2017\n",
      "Epoch 7303/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7263.4508 - val_loss: 6316.7407\n",
      "Epoch 7304/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7262.9887 - val_loss: 6316.2773\n",
      "Epoch 7305/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7262.5259 - val_loss: 6315.8154\n",
      "Epoch 7306/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7262.0631 - val_loss: 6315.3511\n",
      "Epoch 7307/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7261.5999 - val_loss: 6314.8906\n",
      "Epoch 7308/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7261.1385 - val_loss: 6314.4263\n",
      "Epoch 7309/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7260.6752 - val_loss: 6313.9639\n",
      "Epoch 7310/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7260.2119 - val_loss: 6313.5029\n",
      "Epoch 7311/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7259.7490 - val_loss: 6313.0386\n",
      "Epoch 7312/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7259.2871 - val_loss: 6312.5767\n",
      "Epoch 7313/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7258.8243 - val_loss: 6312.1128\n",
      "Epoch 7314/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7258.3607 - val_loss: 6311.6514\n",
      "Epoch 7315/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 7257.8980 - val_loss: 6311.1865\n",
      "Epoch 7316/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 7257.4361 - val_loss: 6310.7256\n",
      "Epoch 7317/10000\n",
      "750/750 [==============================] - 0s 176us/step - loss: 7256.9732 - val_loss: 6310.2642\n",
      "Epoch 7318/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 7256.5103 - val_loss: 6309.7998\n",
      "Epoch 7319/10000\n",
      "750/750 [==============================] - 0s 168us/step - loss: 7256.0483 - val_loss: 6309.3374\n",
      "Epoch 7320/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 7255.5850 - val_loss: 6308.8740\n",
      "Epoch 7321/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7255.1226 - val_loss: 6308.4126\n",
      "Epoch 7322/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7254.6606 - val_loss: 6307.9482\n",
      "Epoch 7323/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7254.1975 - val_loss: 6307.4863\n",
      "Epoch 7324/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7253.7338 - val_loss: 6307.0259\n",
      "Epoch 7325/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7253.2719 - val_loss: 6306.5615\n",
      "Epoch 7326/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 7252.8089 - val_loss: 6306.0977\n",
      "Epoch 7327/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7252.3460 - val_loss: 6305.6362\n",
      "Epoch 7328/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7251.8827 - val_loss: 6305.1733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7329/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7251.4206 - val_loss: 6304.7100\n",
      "Epoch 7330/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7250.9587 - val_loss: 6304.2480\n",
      "Epoch 7331/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7250.4950 - val_loss: 6303.7852\n",
      "Epoch 7332/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7250.0325 - val_loss: 6303.3223\n",
      "Epoch 7333/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7249.5699 - val_loss: 6302.8584\n",
      "Epoch 7334/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7249.1070 - val_loss: 6302.3975\n",
      "Epoch 7335/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7248.6453 - val_loss: 6301.9336\n",
      "Epoch 7336/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 7248.1828 - val_loss: 6301.4717\n",
      "Epoch 7337/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 7247.7193 - val_loss: 6301.0093\n",
      "Epoch 7338/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7247.2564 - val_loss: 6300.5459\n",
      "Epoch 7339/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7246.7941 - val_loss: 6300.0815\n",
      "Epoch 7340/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7246.3315 - val_loss: 6299.6201\n",
      "Epoch 7341/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7245.8674 - val_loss: 6299.1592\n",
      "Epoch 7342/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7245.4053 - val_loss: 6298.6943\n",
      "Epoch 7343/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 7244.9432 - val_loss: 6298.2310\n",
      "Epoch 7344/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7244.4797 - val_loss: 6297.7700\n",
      "Epoch 7345/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7244.0173 - val_loss: 6297.3076\n",
      "Epoch 7346/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7243.5544 - val_loss: 6296.8428\n",
      "Epoch 7347/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7243.0925 - val_loss: 6296.3818\n",
      "Epoch 7348/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7242.6301 - val_loss: 6295.9204\n",
      "Epoch 7349/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7242.1668 - val_loss: 6295.4561\n",
      "Epoch 7350/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7241.7046 - val_loss: 6294.9937\n",
      "Epoch 7351/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7241.2414 - val_loss: 6294.5308\n",
      "Epoch 7352/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7240.7789 - val_loss: 6294.0684\n",
      "Epoch 7353/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7240.3170 - val_loss: 6293.6045\n",
      "Epoch 7354/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 7239.8532 - val_loss: 6293.1436\n",
      "Epoch 7355/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7239.3897 - val_loss: 6292.6812\n",
      "Epoch 7356/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7238.9283 - val_loss: 6292.2168\n",
      "Epoch 7357/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7238.4651 - val_loss: 6291.7539\n",
      "Epoch 7358/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7238.0019 - val_loss: 6291.2930\n",
      "Epoch 7359/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7237.5386 - val_loss: 6290.8296\n",
      "Epoch 7360/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7237.0767 - val_loss: 6290.3662\n",
      "Epoch 7361/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7236.6149 - val_loss: 6289.9053\n",
      "Epoch 7362/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7236.1513 - val_loss: 6289.4409\n",
      "Epoch 7363/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7235.6892 - val_loss: 6288.9780\n",
      "Epoch 7364/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7235.2264 - val_loss: 6288.5151\n",
      "Epoch 7365/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7234.7634 - val_loss: 6288.0542\n",
      "Epoch 7366/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7234.3017 - val_loss: 6287.5894\n",
      "Epoch 7367/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7233.8383 - val_loss: 6287.1279\n",
      "Epoch 7368/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7233.3756 - val_loss: 6286.6655\n",
      "Epoch 7369/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7232.9124 - val_loss: 6286.2021\n",
      "Epoch 7370/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7232.4502 - val_loss: 6285.7378\n",
      "Epoch 7371/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7231.9872 - val_loss: 6285.2769\n",
      "Epoch 7372/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7231.5236 - val_loss: 6284.8149\n",
      "Epoch 7373/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7231.0615 - val_loss: 6284.3506\n",
      "Epoch 7374/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7230.5993 - val_loss: 6283.8872\n",
      "Epoch 7375/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7230.1357 - val_loss: 6283.4263\n",
      "Epoch 7376/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7229.6732 - val_loss: 6282.9639\n",
      "Epoch 7377/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7229.2109 - val_loss: 6282.4990\n",
      "Epoch 7378/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7228.7484 - val_loss: 6282.0381\n",
      "Epoch 7379/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7228.2863 - val_loss: 6281.5747\n",
      "Epoch 7380/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7227.8234 - val_loss: 6281.1123\n",
      "Epoch 7381/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 7227.3605 - val_loss: 6280.6499\n",
      "Epoch 7382/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 7226.8972 - val_loss: 6280.1890\n",
      "Epoch 7383/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7226.4356 - val_loss: 6279.7231\n",
      "Epoch 7384/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7225.9723 - val_loss: 6279.2612\n",
      "Epoch 7385/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7225.5090 - val_loss: 6278.7998\n",
      "Epoch 7386/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7225.0463 - val_loss: 6278.3354\n",
      "Epoch 7387/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7224.5842 - val_loss: 6277.8735\n",
      "Epoch 7388/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 7224.1213 - val_loss: 6277.4106\n",
      "Epoch 7389/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7223.6581 - val_loss: 6276.9487\n",
      "Epoch 7390/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 7223.1951 - val_loss: 6276.4839\n",
      "Epoch 7391/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 7222.7335 - val_loss: 6276.0225\n",
      "Epoch 7392/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 7222.2702 - val_loss: 6275.5615\n",
      "Epoch 7393/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7221.8076 - val_loss: 6275.0972\n",
      "Epoch 7394/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7221.3459 - val_loss: 6274.6343\n",
      "Epoch 7395/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7220.8824 - val_loss: 6274.1719\n",
      "Epoch 7396/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7220.4196 - val_loss: 6273.7100\n",
      "Epoch 7397/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7219.9580 - val_loss: 6273.2451\n",
      "Epoch 7398/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7219.4943 - val_loss: 6272.7842\n",
      "Epoch 7399/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7219.0309 - val_loss: 6272.3228\n",
      "Epoch 7400/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7218.5688 - val_loss: 6271.8584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7401/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7218.1065 - val_loss: 6271.3940\n",
      "Epoch 7402/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7217.6440 - val_loss: 6270.9336\n",
      "Epoch 7403/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7217.1798 - val_loss: 6270.4702\n",
      "Epoch 7404/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7216.7177 - val_loss: 6270.0068\n",
      "Epoch 7405/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7216.2560 - val_loss: 6269.5459\n",
      "Epoch 7406/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7215.7926 - val_loss: 6269.0825\n",
      "Epoch 7407/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7215.3301 - val_loss: 6268.6196\n",
      "Epoch 7408/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 7214.8673 - val_loss: 6268.1558\n",
      "Epoch 7409/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7214.4050 - val_loss: 6267.6948\n",
      "Epoch 7410/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7213.9426 - val_loss: 6267.2310\n",
      "Epoch 7411/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7213.4802 - val_loss: 6266.7686\n",
      "Epoch 7412/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7213.0165 - val_loss: 6266.3062\n",
      "Epoch 7413/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 7212.5535 - val_loss: 6265.8433\n",
      "Epoch 7414/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 7212.0918 - val_loss: 6265.3794\n",
      "Epoch 7415/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7211.6285 - val_loss: 6264.9180\n",
      "Epoch 7416/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 7211.1651 - val_loss: 6264.4565\n",
      "Epoch 7417/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 7210.7024 - val_loss: 6263.9917\n",
      "Epoch 7418/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 7210.2405 - val_loss: 6263.5293\n",
      "Epoch 7419/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 7209.7773 - val_loss: 6263.0669\n",
      "Epoch 7420/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 7209.3146 - val_loss: 6262.6045\n",
      "Epoch 7421/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 7208.8513 - val_loss: 6262.1406\n",
      "Epoch 7422/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 7208.3897 - val_loss: 6261.6787\n",
      "Epoch 7423/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 7207.9272 - val_loss: 6261.2178\n",
      "Epoch 7424/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7207.4645 - val_loss: 6260.7529\n",
      "Epoch 7425/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7207.0019 - val_loss: 6260.2905\n",
      "Epoch 7426/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7206.5385 - val_loss: 6259.8296\n",
      "Epoch 7427/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7206.0764 - val_loss: 6259.3657\n",
      "Epoch 7428/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7205.6141 - val_loss: 6258.9019\n",
      "Epoch 7429/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7205.1502 - val_loss: 6258.4404\n",
      "Epoch 7430/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7204.6872 - val_loss: 6257.9780\n",
      "Epoch 7431/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7204.2251 - val_loss: 6257.5146\n",
      "Epoch 7432/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7203.7628 - val_loss: 6257.0503\n",
      "Epoch 7433/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 7203.2994 - val_loss: 6256.5898\n",
      "Epoch 7434/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7202.8364 - val_loss: 6256.1265\n",
      "Epoch 7435/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7202.3744 - val_loss: 6255.6631\n",
      "Epoch 7436/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7201.9121 - val_loss: 6255.2021\n",
      "Epoch 7437/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7201.4485 - val_loss: 6254.7378\n",
      "Epoch 7438/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 7200.9868 - val_loss: 6254.2749\n",
      "Epoch 7439/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7200.5240 - val_loss: 6253.8140\n",
      "Epoch 7440/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7200.0608 - val_loss: 6253.3511\n",
      "Epoch 7441/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7199.5991 - val_loss: 6252.8867\n",
      "Epoch 7442/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7199.1357 - val_loss: 6252.4248\n",
      "Epoch 7443/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7198.6725 - val_loss: 6251.9639\n",
      "Epoch 7444/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7198.2096 - val_loss: 6251.4990\n",
      "Epoch 7445/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7197.7477 - val_loss: 6251.0356\n",
      "Epoch 7446/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7197.2846 - val_loss: 6250.5742\n",
      "Epoch 7447/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7196.8207 - val_loss: 6250.1123\n",
      "Epoch 7448/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7196.3592 - val_loss: 6249.6479\n",
      "Epoch 7449/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7195.8969 - val_loss: 6249.1855\n",
      "Epoch 7450/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7195.4335 - val_loss: 6248.7246\n",
      "Epoch 7451/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7194.9716 - val_loss: 6248.2607\n",
      "Epoch 7452/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7194.5081 - val_loss: 6247.7964\n",
      "Epoch 7453/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7194.0461 - val_loss: 6247.3354\n",
      "Epoch 7454/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7193.5836 - val_loss: 6246.8716\n",
      "Epoch 7455/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7193.1210 - val_loss: 6246.4092\n",
      "Epoch 7456/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7192.6580 - val_loss: 6245.9482\n",
      "Epoch 7457/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7192.1944 - val_loss: 6245.4863\n",
      "Epoch 7458/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7191.7326 - val_loss: 6245.0220\n",
      "Epoch 7459/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7191.2702 - val_loss: 6244.5581\n",
      "Epoch 7460/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7190.8062 - val_loss: 6244.0967\n",
      "Epoch 7461/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7190.3434 - val_loss: 6243.6323\n",
      "Epoch 7462/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7189.8814 - val_loss: 6243.1709\n",
      "Epoch 7463/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 7193.00 - 0s 120us/step - loss: 7189.4185 - val_loss: 6242.7075\n",
      "Epoch 7464/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7188.9553 - val_loss: 6242.2456\n",
      "Epoch 7465/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7188.4925 - val_loss: 6241.7827\n",
      "Epoch 7466/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7188.0309 - val_loss: 6241.3193\n",
      "Epoch 7467/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7187.5681 - val_loss: 6240.8584\n",
      "Epoch 7468/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7187.1053 - val_loss: 6240.3940\n",
      "Epoch 7469/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7186.6430 - val_loss: 6239.9316\n",
      "Epoch 7470/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7186.1795 - val_loss: 6239.4683\n",
      "Epoch 7471/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7185.7168 - val_loss: 6239.0068\n",
      "Epoch 7472/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7185.2553 - val_loss: 6238.5430\n",
      "Epoch 7473/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7184.7918 - val_loss: 6238.0811\n",
      "Epoch 7474/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7184.3286 - val_loss: 6237.6201\n",
      "Epoch 7475/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7183.8661 - val_loss: 6237.1553\n",
      "Epoch 7476/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7183.4035 - val_loss: 6236.6914\n",
      "Epoch 7477/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7182.9407 - val_loss: 6236.2305\n",
      "Epoch 7478/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7182.4769 - val_loss: 6235.7686\n",
      "Epoch 7479/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7182.0149 - val_loss: 6235.3042\n",
      "Epoch 7480/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7181.5534 - val_loss: 6234.8428\n",
      "Epoch 7481/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7181.0896 - val_loss: 6234.3809\n",
      "Epoch 7482/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7180.6280 - val_loss: 6233.9170\n",
      "Epoch 7483/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7180.1653 - val_loss: 6233.4526\n",
      "Epoch 7484/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 7179.7018 - val_loss: 6232.9922\n",
      "Epoch 7485/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7179.2399 - val_loss: 6232.5278\n",
      "Epoch 7486/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7178.7771 - val_loss: 6232.0654\n",
      "Epoch 7487/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7178.3137 - val_loss: 6231.6045\n",
      "Epoch 7488/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7177.8506 - val_loss: 6231.1401\n",
      "Epoch 7489/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7177.3888 - val_loss: 6230.6763\n",
      "Epoch 7490/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7176.9255 - val_loss: 6230.2148\n",
      "Epoch 7491/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7176.4623 - val_loss: 6229.7534\n",
      "Epoch 7492/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7175.9999 - val_loss: 6229.2886\n",
      "Epoch 7493/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7175.5376 - val_loss: 6228.8271\n",
      "Epoch 7494/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7175.0746 - val_loss: 6228.3657\n",
      "Epoch 7495/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7174.6119 - val_loss: 6227.9014\n",
      "Epoch 7496/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7174.1494 - val_loss: 6227.4375\n",
      "Epoch 7497/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7173.6871 - val_loss: 6226.9761\n",
      "Epoch 7498/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7173.2243 - val_loss: 6226.5146\n",
      "Epoch 7499/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7172.7620 - val_loss: 6226.0498\n",
      "Epoch 7500/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7172.2993 - val_loss: 6225.5879\n",
      "Epoch 7501/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7171.8356 - val_loss: 6225.1274\n",
      "Epoch 7502/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7171.3732 - val_loss: 6224.6631\n",
      "Epoch 7503/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7170.9111 - val_loss: 6224.1992\n",
      "Epoch 7504/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7170.4476 - val_loss: 6223.7373\n",
      "Epoch 7505/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7169.9842 - val_loss: 6223.2759\n",
      "Epoch 7506/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7169.5224 - val_loss: 6222.8115\n",
      "Epoch 7507/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7169.0598 - val_loss: 6222.3481\n",
      "Epoch 7508/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7168.5968 - val_loss: 6221.8872\n",
      "Epoch 7509/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7168.1336 - val_loss: 6221.4238\n",
      "Epoch 7510/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7167.6714 - val_loss: 6220.9604\n",
      "Epoch 7511/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7167.2091 - val_loss: 6220.4990\n",
      "Epoch 7512/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7166.7468 - val_loss: 6220.0352\n",
      "Epoch 7513/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7166.2847 - val_loss: 6219.5732\n",
      "Epoch 7514/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7165.8210 - val_loss: 6219.1108\n",
      "Epoch 7515/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7165.3580 - val_loss: 6218.6484\n",
      "Epoch 7516/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7164.8962 - val_loss: 6218.1836\n",
      "Epoch 7517/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7164.4332 - val_loss: 6217.7217\n",
      "Epoch 7518/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7163.9696 - val_loss: 6217.2607\n",
      "Epoch 7519/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7163.5068 - val_loss: 6216.7964\n",
      "Epoch 7520/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7163.0451 - val_loss: 6216.3345\n",
      "Epoch 7521/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7162.5821 - val_loss: 6215.8711\n",
      "Epoch 7522/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7162.1183 - val_loss: 6215.4092\n",
      "Epoch 7523/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7161.6561 - val_loss: 6214.9448\n",
      "Epoch 7524/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7161.1940 - val_loss: 6214.4834\n",
      "Epoch 7525/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7160.7307 - val_loss: 6214.0225\n",
      "Epoch 7526/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7160.2690 - val_loss: 6213.5576\n",
      "Epoch 7527/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7159.8057 - val_loss: 6213.0933\n",
      "Epoch 7528/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7159.3430 - val_loss: 6212.6323\n",
      "Epoch 7529/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7158.8808 - val_loss: 6212.1704\n",
      "Epoch 7530/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7158.4187 - val_loss: 6211.7061\n",
      "Epoch 7531/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7157.9553 - val_loss: 6211.2451\n",
      "Epoch 7532/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7157.4916 - val_loss: 6210.7837\n",
      "Epoch 7533/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7157.0297 - val_loss: 6210.3193\n",
      "Epoch 7534/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7156.5672 - val_loss: 6209.8555\n",
      "Epoch 7535/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7156.1040 - val_loss: 6209.3940\n",
      "Epoch 7536/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7155.6405 - val_loss: 6208.9312\n",
      "Epoch 7537/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7155.1787 - val_loss: 6208.4678\n",
      "Epoch 7538/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7154.7161 - val_loss: 6208.0068\n",
      "Epoch 7539/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7154.2532 - val_loss: 6207.5430\n",
      "Epoch 7540/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7153.7904 - val_loss: 6207.0796\n",
      "Epoch 7541/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7153.3289 - val_loss: 6206.6162\n",
      "Epoch 7542/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7152.8652 - val_loss: 6206.1553\n",
      "Epoch 7543/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7152.4025 - val_loss: 6205.6914\n",
      "Epoch 7544/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 126us/step - loss: 7151.9407 - val_loss: 6205.2295\n",
      "Epoch 7545/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7151.4774 - val_loss: 6204.7671\n",
      "Epoch 7546/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7151.0143 - val_loss: 6204.3037\n",
      "Epoch 7547/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7150.5525 - val_loss: 6203.8398\n",
      "Epoch 7548/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7150.0888 - val_loss: 6203.3779\n",
      "Epoch 7549/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7149.6258 - val_loss: 6202.9170\n",
      "Epoch 7550/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7149.1631 - val_loss: 6202.4521\n",
      "Epoch 7551/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7148.7009 - val_loss: 6201.9888\n",
      "Epoch 7552/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7148.2381 - val_loss: 6201.5278\n",
      "Epoch 7553/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7147.7750 - val_loss: 6201.0654\n",
      "Epoch 7554/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7147.3123 - val_loss: 6200.6011\n",
      "Epoch 7555/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7146.8503 - val_loss: 6200.1396\n",
      "Epoch 7556/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7146.3870 - val_loss: 6199.6782\n",
      "Epoch 7557/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7145.9254 - val_loss: 6199.2139\n",
      "Epoch 7558/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7145.4626 - val_loss: 6198.7500\n",
      "Epoch 7559/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7144.9991 - val_loss: 6198.2886\n",
      "Epoch 7560/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7144.5370 - val_loss: 6197.8271\n",
      "Epoch 7561/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7144.0744 - val_loss: 6197.3623\n",
      "Epoch 7562/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7143.6113 - val_loss: 6196.9014\n",
      "Epoch 7563/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7143.1479 - val_loss: 6196.4375\n",
      "Epoch 7564/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7142.6858 - val_loss: 6195.9746\n",
      "Epoch 7565/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7142.2233 - val_loss: 6195.5122\n",
      "Epoch 7566/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7141.7596 - val_loss: 6195.0503\n",
      "Epoch 7567/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 7141.2966 - val_loss: 6194.5874\n",
      "Epoch 7568/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7140.8351 - val_loss: 6194.1240\n",
      "Epoch 7569/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7140.3724 - val_loss: 6193.6631\n",
      "Epoch 7570/10000\n",
      "750/750 [==============================] - 0s 167us/step - loss: 7139.9100 - val_loss: 6193.1982\n",
      "Epoch 7571/10000\n",
      "750/750 [==============================] - 0s 169us/step - loss: 7139.4466 - val_loss: 6192.7358\n",
      "Epoch 7572/10000\n",
      "750/750 [==============================] - 0s 185us/step - loss: 7138.9844 - val_loss: 6192.2734\n",
      "Epoch 7573/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7138.5212 - val_loss: 6191.8115\n",
      "Epoch 7574/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 7138.0594 - val_loss: 6191.3477\n",
      "Epoch 7575/10000\n",
      "750/750 [==============================] - 0s 162us/step - loss: 7137.5970 - val_loss: 6190.8857\n",
      "Epoch 7576/10000\n",
      "750/750 [==============================] - 0s 165us/step - loss: 7137.1330 - val_loss: 6190.4248\n",
      "Epoch 7577/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7136.6702 - val_loss: 6189.9600\n",
      "Epoch 7578/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 7136.2088 - val_loss: 6189.4961\n",
      "Epoch 7579/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 7135.7447 - val_loss: 6189.0352\n",
      "Epoch 7580/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7135.2818 - val_loss: 6188.5728\n",
      "Epoch 7581/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7134.8193 - val_loss: 6188.1084\n",
      "Epoch 7582/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7134.3570 - val_loss: 6187.6470\n",
      "Epoch 7583/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7133.8940 - val_loss: 6187.1855\n",
      "Epoch 7584/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7133.4313 - val_loss: 6186.7217\n",
      "Epoch 7585/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7132.9694 - val_loss: 6186.2573\n",
      "Epoch 7586/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7132.5062 - val_loss: 6185.7964\n",
      "Epoch 7587/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7132.0442 - val_loss: 6185.3325\n",
      "Epoch 7588/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7131.5815 - val_loss: 6184.8701\n",
      "Epoch 7589/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 7131.1182 - val_loss: 6184.4087\n",
      "Epoch 7590/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7130.6553 - val_loss: 6183.9468\n",
      "Epoch 7591/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7130.1934 - val_loss: 6183.4810\n",
      "Epoch 7592/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7129.7304 - val_loss: 6183.0190\n",
      "Epoch 7593/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7129.2672 - val_loss: 6182.5576\n",
      "Epoch 7594/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7128.8038 - val_loss: 6182.0933\n",
      "Epoch 7595/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7128.3421 - val_loss: 6181.6318\n",
      "Epoch 7596/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7127.8795 - val_loss: 6181.1685\n",
      "Epoch 7597/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7127.4157 - val_loss: 6180.7061\n",
      "Epoch 7598/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7126.9534 - val_loss: 6180.2417\n",
      "Epoch 7599/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7126.4913 - val_loss: 6179.7803\n",
      "Epoch 7600/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7126.0281 - val_loss: 6179.3193\n",
      "Epoch 7601/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7125.5664 - val_loss: 6178.8550\n",
      "Epoch 7602/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7125.1029 - val_loss: 6178.3921\n",
      "Epoch 7603/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7124.6400 - val_loss: 6177.9297\n",
      "Epoch 7604/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7124.1778 - val_loss: 6177.4678\n",
      "Epoch 7605/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7123.7156 - val_loss: 6177.0034\n",
      "Epoch 7606/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7123.2522 - val_loss: 6176.5420\n",
      "Epoch 7607/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7122.7887 - val_loss: 6176.0806\n",
      "Epoch 7608/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7122.3267 - val_loss: 6175.6162\n",
      "Epoch 7609/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7121.8649 - val_loss: 6175.1528\n",
      "Epoch 7610/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7121.4012 - val_loss: 6174.6914\n",
      "Epoch 7611/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7120.9381 - val_loss: 6174.2285\n",
      "Epoch 7612/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7120.4758 - val_loss: 6173.7646\n",
      "Epoch 7613/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7120.0132 - val_loss: 6173.3037\n",
      "Epoch 7614/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7119.5502 - val_loss: 6172.8403\n",
      "Epoch 7615/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7119.0878 - val_loss: 6172.3774\n",
      "Epoch 7616/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 124us/step - loss: 7118.6260 - val_loss: 6171.9136\n",
      "Epoch 7617/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7118.1623 - val_loss: 6171.4531\n",
      "Epoch 7618/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7117.7005 - val_loss: 6170.9888\n",
      "Epoch 7619/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7117.2377 - val_loss: 6170.5264\n",
      "Epoch 7620/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7116.7743 - val_loss: 6170.0640\n",
      "Epoch 7621/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7116.3114 - val_loss: 6169.6011\n",
      "Epoch 7622/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7115.8497 - val_loss: 6169.1372\n",
      "Epoch 7623/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7115.3864 - val_loss: 6168.6753\n",
      "Epoch 7624/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7114.9234 - val_loss: 6168.2144\n",
      "Epoch 7625/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7114.4605 - val_loss: 6167.7490\n",
      "Epoch 7626/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7113.9983 - val_loss: 6167.2856\n",
      "Epoch 7627/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7113.5355 - val_loss: 6166.8247\n",
      "Epoch 7628/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7113.0722 - val_loss: 6166.3623\n",
      "Epoch 7629/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7112.6100 - val_loss: 6165.8984\n",
      "Epoch 7630/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7112.1476 - val_loss: 6165.4365\n",
      "Epoch 7631/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 7111.6846 - val_loss: 6164.9756\n",
      "Epoch 7632/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7111.2226 - val_loss: 6164.5112\n",
      "Epoch 7633/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7110.7597 - val_loss: 6164.0483\n",
      "Epoch 7634/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 7110.2964 - val_loss: 6163.5874\n",
      "Epoch 7635/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7109.8341 - val_loss: 6163.1240\n",
      "Epoch 7636/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7109.3714 - val_loss: 6162.6597\n",
      "Epoch 7637/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7108.9084 - val_loss: 6162.1982\n",
      "Epoch 7638/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7108.4450 - val_loss: 6161.7358\n",
      "Epoch 7639/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 7107.9831 - val_loss: 6161.2725\n",
      "Epoch 7640/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7107.5207 - val_loss: 6160.8091\n",
      "Epoch 7641/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7107.0569 - val_loss: 6160.3481\n",
      "Epoch 7642/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7106.5947 - val_loss: 6159.8843\n",
      "Epoch 7643/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7106.1321 - val_loss: 6159.4209\n",
      "Epoch 7644/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7105.6695 - val_loss: 6158.9600\n",
      "Epoch 7645/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7105.2074 - val_loss: 6158.4961\n",
      "Epoch 7646/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7104.7444 - val_loss: 6158.0337\n",
      "Epoch 7647/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 7104.2819 - val_loss: 6157.5718\n",
      "Epoch 7648/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7103.8187 - val_loss: 6157.1089\n",
      "Epoch 7649/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7103.3569 - val_loss: 6156.6440\n",
      "Epoch 7650/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7102.8938 - val_loss: 6156.1826\n",
      "Epoch 7651/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7102.4301 - val_loss: 6155.7217\n",
      "Epoch 7652/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7101.9677 - val_loss: 6155.2573\n",
      "Epoch 7653/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7101.5060 - val_loss: 6154.7935\n",
      "Epoch 7654/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7101.0425 - val_loss: 6154.3315\n",
      "Epoch 7655/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7100.5793 - val_loss: 6153.8701\n",
      "Epoch 7656/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7100.1169 - val_loss: 6153.4053\n",
      "Epoch 7657/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7099.6546 - val_loss: 6152.9443\n",
      "Epoch 7658/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7099.1919 - val_loss: 6152.4829\n",
      "Epoch 7659/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7098.7287 - val_loss: 6152.0186\n",
      "Epoch 7660/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7098.2667 - val_loss: 6151.5547\n",
      "Epoch 7661/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7097.8038 - val_loss: 6151.0933\n",
      "Epoch 7662/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7097.3413 - val_loss: 6150.6294\n",
      "Epoch 7663/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7096.8791 - val_loss: 6150.1670\n",
      "Epoch 7664/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7096.4158 - val_loss: 6149.7061\n",
      "Epoch 7665/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7095.9525 - val_loss: 6149.2437\n",
      "Epoch 7666/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7095.4905 - val_loss: 6148.7798\n",
      "Epoch 7667/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7095.0279 - val_loss: 6148.3159\n",
      "Epoch 7668/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7094.5647 - val_loss: 6147.8550\n",
      "Epoch 7669/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7094.1011 - val_loss: 6147.3906\n",
      "Epoch 7670/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 7093.6391 - val_loss: 6146.9287\n",
      "Epoch 7671/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7093.1767 - val_loss: 6146.4653\n",
      "Epoch 7672/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7092.7129 - val_loss: 6146.0034\n",
      "Epoch 7673/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7092.2514 - val_loss: 6145.5405\n",
      "Epoch 7674/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7091.7887 - val_loss: 6145.0771\n",
      "Epoch 7675/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7091.3254 - val_loss: 6144.6162\n",
      "Epoch 7676/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7090.8639 - val_loss: 6144.1519\n",
      "Epoch 7677/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7090.4009 - val_loss: 6143.6899\n",
      "Epoch 7678/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7089.9377 - val_loss: 6143.2280\n",
      "Epoch 7679/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7089.4752 - val_loss: 6142.7646\n",
      "Epoch 7680/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7089.0129 - val_loss: 6142.3003\n",
      "Epoch 7681/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7088.5499 - val_loss: 6141.8398\n",
      "Epoch 7682/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7088.0862 - val_loss: 6141.3779\n",
      "Epoch 7683/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7087.6239 - val_loss: 6140.9131\n",
      "Epoch 7684/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7087.1618 - val_loss: 6140.4497\n",
      "Epoch 7685/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7086.6984 - val_loss: 6139.9878\n",
      "Epoch 7686/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7086.2352 - val_loss: 6139.5264\n",
      "Epoch 7687/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7085.7728 - val_loss: 6139.0615\n",
      "Epoch 7688/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 120us/step - loss: 7085.3111 - val_loss: 6138.6006\n",
      "Epoch 7689/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7084.8480 - val_loss: 6138.1372\n",
      "Epoch 7690/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7084.3853 - val_loss: 6137.6748\n",
      "Epoch 7691/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7083.9230 - val_loss: 6137.2109\n",
      "Epoch 7692/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7083.4597 - val_loss: 6136.7500\n",
      "Epoch 7693/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7082.9977 - val_loss: 6136.2871\n",
      "Epoch 7694/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7082.5352 - val_loss: 6135.8232\n",
      "Epoch 7695/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7082.0718 - val_loss: 6135.3623\n",
      "Epoch 7696/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 7081.6088 - val_loss: 6134.8979\n",
      "Epoch 7697/10000\n",
      "750/750 [==============================] - 0s 169us/step - loss: 7081.1468 - val_loss: 6134.4365\n",
      "Epoch 7698/10000\n",
      "750/750 [==============================] - 0s 162us/step - loss: 7080.6837 - val_loss: 6133.9727\n",
      "Epoch 7699/10000\n",
      "750/750 [==============================] - 0s 155us/step - loss: 7080.2207 - val_loss: 6133.5112\n",
      "Epoch 7700/10000\n",
      "750/750 [==============================] - 0s 180us/step - loss: 7079.7574 - val_loss: 6133.0469\n",
      "Epoch 7701/10000\n",
      "750/750 [==============================] - 0s 173us/step - loss: 7079.2953 - val_loss: 6132.5850\n",
      "Epoch 7702/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7078.8331 - val_loss: 6132.1240\n",
      "Epoch 7703/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 7078.3697 - val_loss: 6131.6592\n",
      "Epoch 7704/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7077.9072 - val_loss: 6131.1968\n",
      "Epoch 7705/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7077.4447 - val_loss: 6130.7339\n",
      "Epoch 7706/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7076.9821 - val_loss: 6130.2725\n",
      "Epoch 7707/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7076.5197 - val_loss: 6129.8081\n",
      "Epoch 7708/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7076.0572 - val_loss: 6129.3462\n",
      "Epoch 7709/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 7075.5934 - val_loss: 6128.8848\n",
      "Epoch 7710/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 7075.1314 - val_loss: 6128.4209\n",
      "Epoch 7711/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 7074.6691 - val_loss: 6127.9565\n",
      "Epoch 7712/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7074.2054 - val_loss: 6127.4951\n",
      "Epoch 7713/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 7073.7425 - val_loss: 6127.0327\n",
      "Epoch 7714/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7073.2803 - val_loss: 6126.5693\n",
      "Epoch 7715/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7072.8181 - val_loss: 6126.1060\n",
      "Epoch 7716/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7072.3543 - val_loss: 6125.6470\n",
      "Epoch 7717/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7071.8920 - val_loss: 6125.1826\n",
      "Epoch 7718/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7071.4294 - val_loss: 6124.7178\n",
      "Epoch 7719/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7070.9674 - val_loss: 6124.2568\n",
      "Epoch 7720/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7070.5049 - val_loss: 6123.7930\n",
      "Epoch 7721/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7070.0415 - val_loss: 6123.3311\n",
      "Epoch 7722/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7069.5789 - val_loss: 6122.8687\n",
      "Epoch 7723/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7069.1158 - val_loss: 6122.4058\n",
      "Epoch 7724/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7068.6541 - val_loss: 6121.9419\n",
      "Epoch 7725/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7068.1910 - val_loss: 6121.4795\n",
      "Epoch 7726/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7067.7271 - val_loss: 6121.0186\n",
      "Epoch 7727/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7067.2646 - val_loss: 6120.5542\n",
      "Epoch 7728/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7066.8030 - val_loss: 6120.0928\n",
      "Epoch 7729/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7066.3399 - val_loss: 6119.6289\n",
      "Epoch 7730/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7065.8763 - val_loss: 6119.1670\n",
      "Epoch 7731/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7065.4141 - val_loss: 6118.7026\n",
      "Epoch 7732/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7064.9519 - val_loss: 6118.2412\n",
      "Epoch 7733/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7064.4896 - val_loss: 6117.7803\n",
      "Epoch 7734/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7064.0269 - val_loss: 6117.3154\n",
      "Epoch 7735/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7063.5640 - val_loss: 6116.8530\n",
      "Epoch 7736/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7063.1010 - val_loss: 6116.3921\n",
      "Epoch 7737/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7062.6388 - val_loss: 6115.9282\n",
      "Epoch 7738/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7062.1765 - val_loss: 6115.4639\n",
      "Epoch 7739/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 7061.7131 - val_loss: 6115.0029\n",
      "Epoch 7740/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7061.2496 - val_loss: 6114.5410\n",
      "Epoch 7741/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7060.7875 - val_loss: 6114.0771\n",
      "Epoch 7742/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7060.3253 - val_loss: 6113.6128\n",
      "Epoch 7743/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7059.8618 - val_loss: 6113.1523\n",
      "Epoch 7744/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7059.3983 - val_loss: 6112.6890\n",
      "Epoch 7745/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7058.9367 - val_loss: 6112.2256\n",
      "Epoch 7746/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7058.4744 - val_loss: 6111.7646\n",
      "Epoch 7747/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7058.0108 - val_loss: 6111.3003\n",
      "Epoch 7748/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7057.5487 - val_loss: 6110.8379\n",
      "Epoch 7749/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7057.0858 - val_loss: 6110.3740\n",
      "Epoch 7750/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7056.6227 - val_loss: 6109.9136\n",
      "Epoch 7751/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7056.1616 - val_loss: 6109.4497\n",
      "Epoch 7752/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7055.6985 - val_loss: 6108.9873\n",
      "Epoch 7753/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7055.2349 - val_loss: 6108.5254\n",
      "Epoch 7754/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7054.7723 - val_loss: 6108.0615\n",
      "Epoch 7755/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7054.3099 - val_loss: 6107.5977\n",
      "Epoch 7756/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7053.8472 - val_loss: 6107.1367\n",
      "Epoch 7757/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7053.3836 - val_loss: 6106.6748\n",
      "Epoch 7758/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7052.9209 - val_loss: 6106.2104\n",
      "Epoch 7759/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7052.4593 - val_loss: 6105.7490\n",
      "Epoch 7760/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 132us/step - loss: 7051.9961 - val_loss: 6105.2856\n",
      "Epoch 7761/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7051.5334 - val_loss: 6104.8232\n",
      "Epoch 7762/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7051.0703 - val_loss: 6104.3589\n",
      "Epoch 7763/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7050.6082 - val_loss: 6103.8975\n",
      "Epoch 7764/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7050.1456 - val_loss: 6103.4360\n",
      "Epoch 7765/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 7049.6828 - val_loss: 6102.9717\n",
      "Epoch 7766/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7049.2204 - val_loss: 6102.5093\n",
      "Epoch 7767/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7048.7569 - val_loss: 6102.0469\n",
      "Epoch 7768/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7048.2948 - val_loss: 6101.5850\n",
      "Epoch 7769/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7047.8324 - val_loss: 6101.1201\n",
      "Epoch 7770/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 7047.3687 - val_loss: 6100.6592\n",
      "Epoch 7771/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7046.9059 - val_loss: 6100.1948\n",
      "Epoch 7772/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 7046.4436 - val_loss: 6099.7334\n",
      "Epoch 7773/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7045.9816 - val_loss: 6099.2690\n",
      "Epoch 7774/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7045.5181 - val_loss: 6098.8086\n",
      "Epoch 7775/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 7045.0552 - val_loss: 6098.3452\n",
      "Epoch 7776/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7044.5926 - val_loss: 6097.8818\n",
      "Epoch 7777/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7044.1305 - val_loss: 6097.4209\n",
      "Epoch 7778/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7043.6671 - val_loss: 6096.9565\n",
      "Epoch 7779/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7043.2054 - val_loss: 6096.4937\n",
      "Epoch 7780/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7042.7426 - val_loss: 6096.0308\n",
      "Epoch 7781/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7042.2791 - val_loss: 6095.5693\n",
      "Epoch 7782/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 7041.8173 - val_loss: 6095.1055\n",
      "Epoch 7783/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7041.3545 - val_loss: 6094.6436\n",
      "Epoch 7784/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7040.8913 - val_loss: 6094.1826\n",
      "Epoch 7785/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7040.4284 - val_loss: 6093.7178\n",
      "Epoch 7786/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 7039.9661 - val_loss: 6093.2539\n",
      "Epoch 7787/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7039.5031 - val_loss: 6092.7930\n",
      "Epoch 7788/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7039.0402 - val_loss: 6092.3301\n",
      "Epoch 7789/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7038.5772 - val_loss: 6091.8662\n",
      "Epoch 7790/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7038.1153 - val_loss: 6091.4053\n",
      "Epoch 7791/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7037.6522 - val_loss: 6090.9438\n",
      "Epoch 7792/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7037.1899 - val_loss: 6090.4795\n",
      "Epoch 7793/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7036.7268 - val_loss: 6090.0151\n",
      "Epoch 7794/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7036.2642 - val_loss: 6089.5542\n",
      "Epoch 7795/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7035.8021 - val_loss: 6089.0903\n",
      "Epoch 7796/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7035.3392 - val_loss: 6088.6279\n",
      "Epoch 7797/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7034.8759 - val_loss: 6088.1660\n",
      "Epoch 7798/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7034.4131 - val_loss: 6087.7046\n",
      "Epoch 7799/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7033.9513 - val_loss: 6087.2402\n",
      "Epoch 7800/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7033.4887 - val_loss: 6086.7773\n",
      "Epoch 7801/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7033.0246 - val_loss: 6086.3154\n",
      "Epoch 7802/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7032.5622 - val_loss: 6085.8511\n",
      "Epoch 7803/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7032.1000 - val_loss: 6085.3896\n",
      "Epoch 7804/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7031.6370 - val_loss: 6084.9263\n",
      "Epoch 7805/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7031.1739 - val_loss: 6084.4639\n",
      "Epoch 7806/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7030.7114 - val_loss: 6084.0000\n",
      "Epoch 7807/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7030.2489 - val_loss: 6083.5381\n",
      "Epoch 7808/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7029.7863 - val_loss: 6083.0771\n",
      "Epoch 7809/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7029.3245 - val_loss: 6082.6128\n",
      "Epoch 7810/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7028.8616 - val_loss: 6082.1504\n",
      "Epoch 7811/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7028.3979 - val_loss: 6081.6890\n",
      "Epoch 7812/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 7027.9359 - val_loss: 6081.2256\n",
      "Epoch 7813/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7027.4734 - val_loss: 6080.7617\n",
      "Epoch 7814/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7027.0102 - val_loss: 6080.2998\n",
      "Epoch 7815/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7026.5471 - val_loss: 6079.8384\n",
      "Epoch 7816/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7026.0846 - val_loss: 6079.3740\n",
      "Epoch 7817/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7025.6223 - val_loss: 6078.9106\n",
      "Epoch 7818/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7025.1595 - val_loss: 6078.4492\n",
      "Epoch 7819/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7024.6955 - val_loss: 6077.9873\n",
      "Epoch 7820/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 7024.2339 - val_loss: 6077.5225\n",
      "Epoch 7821/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 7023.7714 - val_loss: 6077.0615\n",
      "Epoch 7822/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 7023.3082 - val_loss: 6076.5977\n",
      "Epoch 7823/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7022.8464 - val_loss: 6076.1357\n",
      "Epoch 7824/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7022.3829 - val_loss: 6075.6719\n",
      "Epoch 7825/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7021.9204 - val_loss: 6075.2104\n",
      "Epoch 7826/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7021.4587 - val_loss: 6074.7466\n",
      "Epoch 7827/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7020.9956 - val_loss: 6074.2842\n",
      "Epoch 7828/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 7020.5321 - val_loss: 6073.8232\n",
      "Epoch 7829/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7020.0693 - val_loss: 6073.3589\n",
      "Epoch 7830/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 7019.6074 - val_loss: 6072.8950\n",
      "Epoch 7831/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7019.1446 - val_loss: 6072.4336\n",
      "Epoch 7832/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 140us/step - loss: 7018.6812 - val_loss: 6071.9722\n",
      "Epoch 7833/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 7018.2183 - val_loss: 6071.5073\n",
      "Epoch 7834/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7017.7563 - val_loss: 6071.0459\n",
      "Epoch 7835/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7017.2932 - val_loss: 6070.5845\n",
      "Epoch 7836/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 7016.8304 - val_loss: 6070.1201\n",
      "Epoch 7837/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7016.3677 - val_loss: 6069.6577\n",
      "Epoch 7838/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7015.9056 - val_loss: 6069.1948\n",
      "Epoch 7839/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7015.4430 - val_loss: 6068.7334\n",
      "Epoch 7840/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7014.9801 - val_loss: 6068.2686\n",
      "Epoch 7841/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7014.5175 - val_loss: 6067.8066\n",
      "Epoch 7842/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7014.0541 - val_loss: 6067.3452\n",
      "Epoch 7843/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7013.5920 - val_loss: 6066.8818\n",
      "Epoch 7844/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7013.1296 - val_loss: 6066.4180\n",
      "Epoch 7845/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7012.6664 - val_loss: 6065.9561\n",
      "Epoch 7846/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7012.2031 - val_loss: 6065.4937\n",
      "Epoch 7847/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 7011.7409 - val_loss: 6065.0303\n",
      "Epoch 7848/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 7011.2783 - val_loss: 6064.5669\n",
      "Epoch 7849/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 7010.8156 - val_loss: 6064.1060\n",
      "Epoch 7850/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 7010.3521 - val_loss: 6063.6426\n",
      "Epoch 7851/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7009.8902 - val_loss: 6063.1792\n",
      "Epoch 7852/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 7009.4278 - val_loss: 6062.7178\n",
      "Epoch 7853/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7008.9646 - val_loss: 6062.2539\n",
      "Epoch 7854/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7008.5024 - val_loss: 6061.7920\n",
      "Epoch 7855/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 7008.0396 - val_loss: 6061.3296\n",
      "Epoch 7856/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7007.5768 - val_loss: 6060.8667\n",
      "Epoch 7857/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7007.1148 - val_loss: 6060.4028\n",
      "Epoch 7858/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 7006.6514 - val_loss: 6059.9404\n",
      "Epoch 7859/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 7006.1885 - val_loss: 6059.4795\n",
      "Epoch 7860/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7005.7254 - val_loss: 6059.0151\n",
      "Epoch 7861/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7005.2639 - val_loss: 6058.5513\n",
      "Epoch 7862/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7004.8006 - val_loss: 6058.0898\n",
      "Epoch 7863/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 7004.3372 - val_loss: 6057.6279\n",
      "Epoch 7864/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 7003.8749 - val_loss: 6057.1636\n",
      "Epoch 7865/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 7003.4126 - val_loss: 6056.7021\n",
      "Epoch 7866/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 7002.9494 - val_loss: 6056.2407\n",
      "Epoch 7867/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 7002.4871 - val_loss: 6055.7764\n",
      "Epoch 7868/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 7002.0241 - val_loss: 6055.3125\n",
      "Epoch 7869/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 7001.5616 - val_loss: 6054.8511\n",
      "Epoch 7870/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 7001.0994 - val_loss: 6054.3872\n",
      "Epoch 7871/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 7000.6370 - val_loss: 6053.9248\n",
      "Epoch 7872/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 7000.1738 - val_loss: 6053.4639\n",
      "Epoch 7873/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6999.7102 - val_loss: 6053.0015\n",
      "Epoch 7874/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6999.2482 - val_loss: 6052.5381\n",
      "Epoch 7875/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6998.7856 - val_loss: 6052.0742\n",
      "Epoch 7876/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6998.3223 - val_loss: 6051.6128\n",
      "Epoch 7877/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6997.8591 - val_loss: 6051.1499\n",
      "Epoch 7878/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6997.3975 - val_loss: 6050.6865\n",
      "Epoch 7879/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 6996.9348 - val_loss: 6050.2231\n",
      "Epoch 7880/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6996.4717 - val_loss: 6049.7612\n",
      "Epoch 7881/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 6996.0088 - val_loss: 6049.2983\n",
      "Epoch 7882/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 6995.5470 - val_loss: 6048.8354\n",
      "Epoch 7883/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6995.0839 - val_loss: 6048.3740\n",
      "Epoch 7884/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6994.6217 - val_loss: 6047.9102\n",
      "Epoch 7885/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6994.1588 - val_loss: 6047.4482\n",
      "Epoch 7886/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6993.6955 - val_loss: 6046.9863\n",
      "Epoch 7887/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6993.2330 - val_loss: 6046.5225\n",
      "Epoch 7888/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 6992.7710 - val_loss: 6046.0586\n",
      "Epoch 7889/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 6992.3074 - val_loss: 6045.5977\n",
      "Epoch 7890/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 6991.8445 - val_loss: 6045.1357\n",
      "Epoch 7891/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6991.3817 - val_loss: 6044.6709\n",
      "Epoch 7892/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6990.9193 - val_loss: 6044.2095\n",
      "Epoch 7893/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 6990.4568 - val_loss: 6043.7466\n",
      "Epoch 7894/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 6989.9930 - val_loss: 6043.2842\n",
      "Epoch 7895/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 6989.5313 - val_loss: 6042.8198\n",
      "Epoch 7896/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 6989.0690 - val_loss: 6042.3584\n",
      "Epoch 7897/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 6988.6058 - val_loss: 6041.8950\n",
      "Epoch 7898/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6988.1440 - val_loss: 6041.4326\n",
      "Epoch 7899/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6987.6805 - val_loss: 6040.9683\n",
      "Epoch 7900/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6987.2174 - val_loss: 6040.5073\n",
      "Epoch 7901/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 6986.7556 - val_loss: 6040.0454\n",
      "Epoch 7902/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6986.2930 - val_loss: 6039.5811\n",
      "Epoch 7903/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 6985.8293 - val_loss: 6039.1201\n",
      "Epoch 7904/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 130us/step - loss: 6985.3666 - val_loss: 6038.6558\n",
      "Epoch 7905/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6984.9044 - val_loss: 6038.1934\n",
      "Epoch 7906/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6984.4419 - val_loss: 6037.7310\n",
      "Epoch 7907/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6983.9784 - val_loss: 6037.2690\n",
      "Epoch 7908/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6983.5154 - val_loss: 6036.8062\n",
      "Epoch 7909/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6983.0537 - val_loss: 6036.3428\n",
      "Epoch 7910/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6982.5912 - val_loss: 6035.8818\n",
      "Epoch 7911/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6982.1285 - val_loss: 6035.4170\n",
      "Epoch 7912/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6981.6651 - val_loss: 6034.9546\n",
      "Epoch 7913/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6981.2030 - val_loss: 6034.4922\n",
      "Epoch 7914/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6980.7400 - val_loss: 6034.0303\n",
      "Epoch 7915/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6980.2778 - val_loss: 6033.5659\n",
      "Epoch 7916/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6979.8150 - val_loss: 6033.1045\n",
      "Epoch 7917/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6979.3512 - val_loss: 6032.6421\n",
      "Epoch 7918/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6978.8892 - val_loss: 6032.1787\n",
      "Epoch 7919/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6978.4273 - val_loss: 6031.7148\n",
      "Epoch 7920/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6977.9636 - val_loss: 6031.2529\n",
      "Epoch 7921/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6977.5008 - val_loss: 6030.7910\n",
      "Epoch 7922/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6977.0380 - val_loss: 6030.3271\n",
      "Epoch 7923/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6976.5755 - val_loss: 6029.8638\n",
      "Epoch 7924/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6976.1127 - val_loss: 6029.4028\n",
      "Epoch 7925/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6975.6494 - val_loss: 6028.9404\n",
      "Epoch 7926/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6975.1872 - val_loss: 6028.4761\n",
      "Epoch 7927/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6974.7251 - val_loss: 6028.0146\n",
      "Epoch 7928/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6974.2624 - val_loss: 6027.5513\n",
      "Epoch 7929/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6973.7997 - val_loss: 6027.0889\n",
      "Epoch 7930/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6973.3368 - val_loss: 6026.6265\n",
      "Epoch 7931/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6972.8737 - val_loss: 6026.1636\n",
      "Epoch 7932/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6972.4120 - val_loss: 6025.6997\n",
      "Epoch 7933/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6971.9488 - val_loss: 6025.2373\n",
      "Epoch 7934/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6971.4853 - val_loss: 6024.7764\n",
      "Epoch 7935/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6971.0228 - val_loss: 6024.3115\n",
      "Epoch 7936/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6970.5608 - val_loss: 6023.8506\n",
      "Epoch 7937/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6970.0981 - val_loss: 6023.3872\n",
      "Epoch 7938/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6969.6345 - val_loss: 6022.9248\n",
      "Epoch 7939/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6969.1720 - val_loss: 6022.4604\n",
      "Epoch 7940/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6968.7097 - val_loss: 6021.9990\n",
      "Epoch 7941/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6968.2471 - val_loss: 6021.5381\n",
      "Epoch 7942/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6967.7852 - val_loss: 6021.0737\n",
      "Epoch 7943/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6967.3215 - val_loss: 6020.6108\n",
      "Epoch 7944/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6966.8588 - val_loss: 6020.1484\n",
      "Epoch 7945/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6966.3963 - val_loss: 6019.6865\n",
      "Epoch 7946/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6965.9342 - val_loss: 6019.2222\n",
      "Epoch 7947/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6965.4709 - val_loss: 6018.7607\n",
      "Epoch 7948/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6965.0073 - val_loss: 6018.2983\n",
      "Epoch 7949/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6964.5453 - val_loss: 6017.8350\n",
      "Epoch 7950/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6964.0831 - val_loss: 6017.3711\n",
      "Epoch 7951/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6963.6198 - val_loss: 6016.9102\n",
      "Epoch 7952/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6963.1565 - val_loss: 6016.4468\n",
      "Epoch 7953/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6962.6945 - val_loss: 6015.9834\n",
      "Epoch 7954/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6962.2318 - val_loss: 6015.5225\n",
      "Epoch 7955/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 6961.7689 - val_loss: 6015.0586\n",
      "Epoch 7956/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 6961.3066 - val_loss: 6014.5962\n",
      "Epoch 7957/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6960.8440 - val_loss: 6014.1323\n",
      "Epoch 7958/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 6960.3811 - val_loss: 6013.6714\n",
      "Epoch 7959/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6959.9194 - val_loss: 6013.2075\n",
      "Epoch 7960/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6959.4562 - val_loss: 6012.7451\n",
      "Epoch 7961/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6958.9925 - val_loss: 6012.2842\n",
      "Epoch 7962/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6958.5302 - val_loss: 6011.8193\n",
      "Epoch 7963/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6958.0680 - val_loss: 6011.3560\n",
      "Epoch 7964/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6957.6054 - val_loss: 6010.8940\n",
      "Epoch 7965/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 6957.1417 - val_loss: 6010.4326\n",
      "Epoch 7966/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6956.6791 - val_loss: 6009.9683\n",
      "Epoch 7967/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6956.2173 - val_loss: 6009.5063\n",
      "Epoch 7968/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6955.7544 - val_loss: 6009.0435\n",
      "Epoch 7969/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6955.2908 - val_loss: 6008.5811\n",
      "Epoch 7970/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6954.8287 - val_loss: 6008.1167\n",
      "Epoch 7971/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6954.3662 - val_loss: 6007.6553\n",
      "Epoch 7972/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6953.9027 - val_loss: 6007.1934\n",
      "Epoch 7973/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6953.4413 - val_loss: 6006.7295\n",
      "Epoch 7974/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6952.9782 - val_loss: 6006.2671\n",
      "Epoch 7975/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6952.5149 - val_loss: 6005.8047\n",
      "Epoch 7976/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 121us/step - loss: 6952.0529 - val_loss: 6005.3428\n",
      "Epoch 7977/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6951.5898 - val_loss: 6004.8779\n",
      "Epoch 7978/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6951.1269 - val_loss: 6004.4170\n",
      "Epoch 7979/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6950.6637 - val_loss: 6003.9526\n",
      "Epoch 7980/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6950.2015 - val_loss: 6003.4907\n",
      "Epoch 7981/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6949.7391 - val_loss: 6003.0278\n",
      "Epoch 7982/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6949.2760 - val_loss: 6002.5664\n",
      "Epoch 7983/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6948.8127 - val_loss: 6002.1030\n",
      "Epoch 7984/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6948.3508 - val_loss: 6001.6396\n",
      "Epoch 7985/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6947.8882 - val_loss: 6001.1787\n",
      "Epoch 7986/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6947.4257 - val_loss: 6000.7148\n",
      "Epoch 7987/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6946.9632 - val_loss: 6000.2524\n",
      "Epoch 7988/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6946.5002 - val_loss: 5999.7905\n",
      "Epoch 7989/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6946.0371 - val_loss: 5999.3271\n",
      "Epoch 7990/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6945.5752 - val_loss: 5998.8628\n",
      "Epoch 7991/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6945.1125 - val_loss: 5998.4014\n",
      "Epoch 7992/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6944.6488 - val_loss: 5997.9404\n",
      "Epoch 7993/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6944.1863 - val_loss: 5997.4761\n",
      "Epoch 7994/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6943.7245 - val_loss: 5997.0122\n",
      "Epoch 7995/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6943.2606 - val_loss: 5996.5503\n",
      "Epoch 7996/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6942.7982 - val_loss: 5996.0884\n",
      "Epoch 7997/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6942.3351 - val_loss: 5995.6240\n",
      "Epoch 7998/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6941.8732 - val_loss: 5995.1631\n",
      "Epoch 7999/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6941.4106 - val_loss: 5994.7017\n",
      "Epoch 8000/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6940.9475 - val_loss: 5994.2373\n",
      "Epoch 8001/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6940.4851 - val_loss: 5993.7734\n",
      "Epoch 8002/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6940.0221 - val_loss: 5993.3115\n",
      "Epoch 8003/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6939.5599 - val_loss: 5992.8481\n",
      "Epoch 8004/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6939.0974 - val_loss: 5992.3857\n",
      "Epoch 8005/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6938.6341 - val_loss: 5991.9248\n",
      "Epoch 8006/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6938.1712 - val_loss: 5991.4624\n",
      "Epoch 8007/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6937.7092 - val_loss: 5990.9985\n",
      "Epoch 8008/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6937.2462 - val_loss: 5990.5352\n",
      "Epoch 8009/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6936.7831 - val_loss: 5990.0732\n",
      "Epoch 8010/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6936.3200 - val_loss: 5989.6094\n",
      "Epoch 8011/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6935.8578 - val_loss: 5989.1475\n",
      "Epoch 8012/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6935.3954 - val_loss: 5988.6841\n",
      "Epoch 8013/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6934.9321 - val_loss: 5988.2217\n",
      "Epoch 8014/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6934.4693 - val_loss: 5987.7573\n",
      "Epoch 8015/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6934.0068 - val_loss: 5987.2959\n",
      "Epoch 8016/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6933.5443 - val_loss: 5986.8350\n",
      "Epoch 8017/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6933.0827 - val_loss: 5986.3711\n",
      "Epoch 8018/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6932.6193 - val_loss: 5985.9077\n",
      "Epoch 8019/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6932.1562 - val_loss: 5985.4468\n",
      "Epoch 8020/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6931.6938 - val_loss: 5984.9834\n",
      "Epoch 8021/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6931.2313 - val_loss: 5984.5190\n",
      "Epoch 8022/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6930.7687 - val_loss: 5984.0576\n",
      "Epoch 8023/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 6930.3045 - val_loss: 5983.5962\n",
      "Epoch 8024/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6929.8427 - val_loss: 5983.1318\n",
      "Epoch 8025/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6929.3804 - val_loss: 5982.6685\n",
      "Epoch 8026/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6928.9171 - val_loss: 5982.2065\n",
      "Epoch 8027/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6928.4541 - val_loss: 5981.7451\n",
      "Epoch 8028/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6927.9918 - val_loss: 5981.2803\n",
      "Epoch 8029/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6927.5295 - val_loss: 5980.8193\n",
      "Epoch 8030/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6927.0665 - val_loss: 5980.3555\n",
      "Epoch 8031/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6926.6040 - val_loss: 5979.8936\n",
      "Epoch 8032/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6926.1412 - val_loss: 5979.4297\n",
      "Epoch 8033/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6925.6782 - val_loss: 5978.9683\n",
      "Epoch 8034/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6925.2167 - val_loss: 5978.5044\n",
      "Epoch 8035/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6924.7535 - val_loss: 5978.0420\n",
      "Epoch 8036/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6924.2902 - val_loss: 5977.5811\n",
      "Epoch 8037/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6923.8273 - val_loss: 5977.1167\n",
      "Epoch 8038/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6923.3654 - val_loss: 5976.6543\n",
      "Epoch 8039/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6922.9025 - val_loss: 5976.1914\n",
      "Epoch 8040/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6922.4393 - val_loss: 5975.7295\n",
      "Epoch 8041/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6921.9761 - val_loss: 5975.2651\n",
      "Epoch 8042/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6921.5144 - val_loss: 5974.8032\n",
      "Epoch 8043/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6921.0514 - val_loss: 5974.3428\n",
      "Epoch 8044/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6920.5885 - val_loss: 5973.8779\n",
      "Epoch 8045/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6920.1265 - val_loss: 5973.4155\n",
      "Epoch 8046/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6919.6633 - val_loss: 5972.9526\n",
      "Epoch 8047/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6919.2009 - val_loss: 5972.4907\n",
      "Epoch 8048/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 128us/step - loss: 6918.7385 - val_loss: 5972.0269\n",
      "Epoch 8049/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6918.2757 - val_loss: 5971.5649\n",
      "Epoch 8050/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6917.8121 - val_loss: 5971.1030\n",
      "Epoch 8051/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6917.3501 - val_loss: 5970.6396\n",
      "Epoch 8052/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6916.8871 - val_loss: 5970.1753\n",
      "Epoch 8053/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6916.4246 - val_loss: 5969.7139\n",
      "Epoch 8054/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6915.9612 - val_loss: 5969.2515\n",
      "Epoch 8055/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6915.4987 - val_loss: 5968.7881\n",
      "Epoch 8056/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6915.0362 - val_loss: 5968.3271\n",
      "Epoch 8057/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6914.5732 - val_loss: 5967.8638\n",
      "Epoch 8058/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6914.1103 - val_loss: 5967.4004\n",
      "Epoch 8059/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6913.6481 - val_loss: 5966.9365\n",
      "Epoch 8060/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6913.1859 - val_loss: 5966.4756\n",
      "Epoch 8061/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6912.7232 - val_loss: 5966.0117\n",
      "Epoch 8062/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6912.2603 - val_loss: 5965.5498\n",
      "Epoch 8063/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6911.7974 - val_loss: 5965.0874\n",
      "Epoch 8064/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6911.3346 - val_loss: 5964.6240\n",
      "Epoch 8065/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6910.8729 - val_loss: 5964.1606\n",
      "Epoch 8066/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6910.4096 - val_loss: 5963.6982\n",
      "Epoch 8067/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6909.9459 - val_loss: 5963.2373\n",
      "Epoch 8068/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6909.4833 - val_loss: 5962.7729\n",
      "Epoch 8069/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6909.0217 - val_loss: 5962.3091\n",
      "Epoch 8070/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6908.5583 - val_loss: 5961.8477\n",
      "Epoch 8071/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6908.0952 - val_loss: 5961.3857\n",
      "Epoch 8072/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6907.6326 - val_loss: 5960.9214\n",
      "Epoch 8073/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6907.1704 - val_loss: 5960.4600\n",
      "Epoch 8074/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6906.7078 - val_loss: 5959.9985\n",
      "Epoch 8075/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6906.2443 - val_loss: 5959.5342\n",
      "Epoch 8076/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6905.7826 - val_loss: 5959.0718\n",
      "Epoch 8077/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6905.3199 - val_loss: 5958.6089\n",
      "Epoch 8078/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6904.8573 - val_loss: 5958.1470\n",
      "Epoch 8079/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 6904.3950 - val_loss: 5957.6826\n",
      "Epoch 8080/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6903.9313 - val_loss: 5957.2217\n",
      "Epoch 8081/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 6903.4682 - val_loss: 5956.7593\n",
      "Epoch 8082/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6903.0061 - val_loss: 5956.2959\n",
      "Epoch 8083/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6902.5438 - val_loss: 5955.8315\n",
      "Epoch 8084/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6902.0804 - val_loss: 5955.3711\n",
      "Epoch 8085/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6901.6171 - val_loss: 5954.9077\n",
      "Epoch 8086/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6901.1553 - val_loss: 5954.4443\n",
      "Epoch 8087/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6900.6929 - val_loss: 5953.9829\n",
      "Epoch 8088/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6900.2294 - val_loss: 5953.5190\n",
      "Epoch 8089/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6899.7673 - val_loss: 5953.0562\n",
      "Epoch 8090/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6899.3044 - val_loss: 5952.5928\n",
      "Epoch 8091/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6898.8416 - val_loss: 5952.1323\n",
      "Epoch 8092/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6898.3797 - val_loss: 5951.6680\n",
      "Epoch 8093/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6897.9166 - val_loss: 5951.2061\n",
      "Epoch 8094/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6897.4532 - val_loss: 5950.7441\n",
      "Epoch 8095/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6896.9908 - val_loss: 5950.2803\n",
      "Epoch 8096/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6896.5285 - val_loss: 5949.8169\n",
      "Epoch 8097/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6896.0657 - val_loss: 5949.3550\n",
      "Epoch 8098/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 6896.08 - 0s 128us/step - loss: 6895.6024 - val_loss: 5948.8936\n",
      "Epoch 8099/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6895.1396 - val_loss: 5948.4292\n",
      "Epoch 8100/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6894.6777 - val_loss: 5947.9653\n",
      "Epoch 8101/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6894.2145 - val_loss: 5947.5044\n",
      "Epoch 8102/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6893.7513 - val_loss: 5947.0420\n",
      "Epoch 8103/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6893.2889 - val_loss: 5946.5776\n",
      "Epoch 8104/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6892.8268 - val_loss: 5946.1162\n",
      "Epoch 8105/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6892.3643 - val_loss: 5945.6528\n",
      "Epoch 8106/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6891.9016 - val_loss: 5945.1904\n",
      "Epoch 8107/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6891.4388 - val_loss: 5944.7280\n",
      "Epoch 8108/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6890.9755 - val_loss: 5944.2651\n",
      "Epoch 8109/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6890.5135 - val_loss: 5943.8032\n",
      "Epoch 8110/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6890.0507 - val_loss: 5943.3389\n",
      "Epoch 8111/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6889.5873 - val_loss: 5942.8779\n",
      "Epoch 8112/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6889.1246 - val_loss: 5942.4136\n",
      "Epoch 8113/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6888.6623 - val_loss: 5941.9521\n",
      "Epoch 8114/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6888.1995 - val_loss: 5941.4888\n",
      "Epoch 8115/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6887.7365 - val_loss: 5941.0269\n",
      "Epoch 8116/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6887.2732 - val_loss: 5940.5640\n",
      "Epoch 8117/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6886.8117 - val_loss: 5940.1006\n",
      "Epoch 8118/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6886.3491 - val_loss: 5939.6396\n",
      "Epoch 8119/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6885.8856 - val_loss: 5939.1753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8120/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6885.4236 - val_loss: 5938.7124\n",
      "Epoch 8121/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6884.9607 - val_loss: 5938.2490\n",
      "Epoch 8122/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6884.4977 - val_loss: 5937.7881\n",
      "Epoch 8123/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6884.0358 - val_loss: 5937.3237\n",
      "Epoch 8124/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6883.5728 - val_loss: 5936.8623\n",
      "Epoch 8125/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6883.1092 - val_loss: 5936.4014\n",
      "Epoch 8126/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6882.6470 - val_loss: 5935.9365\n",
      "Epoch 8127/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6882.1847 - val_loss: 5935.4727\n",
      "Epoch 8128/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6881.7219 - val_loss: 5935.0117\n",
      "Epoch 8129/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6881.2585 - val_loss: 5934.5488\n",
      "Epoch 8130/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 6889.70 - 0s 122us/step - loss: 6880.7961 - val_loss: 5934.0850\n",
      "Epoch 8131/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6880.3340 - val_loss: 5933.6240\n",
      "Epoch 8132/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6879.8701 - val_loss: 5933.1606\n",
      "Epoch 8133/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6879.4079 - val_loss: 5932.6982\n",
      "Epoch 8134/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6878.9453 - val_loss: 5932.2339\n",
      "Epoch 8135/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6878.4830 - val_loss: 5931.7725\n",
      "Epoch 8136/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6878.0206 - val_loss: 5931.3091\n",
      "Epoch 8137/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6877.5577 - val_loss: 5930.8467\n",
      "Epoch 8138/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6877.0948 - val_loss: 5930.3848\n",
      "Epoch 8139/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6876.6315 - val_loss: 5929.9214\n",
      "Epoch 8140/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6876.1699 - val_loss: 5929.4575\n",
      "Epoch 8141/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6875.7072 - val_loss: 5928.9951\n",
      "Epoch 8142/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6875.2433 - val_loss: 5928.5342\n",
      "Epoch 8143/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6874.7809 - val_loss: 5928.0698\n",
      "Epoch 8144/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6874.3185 - val_loss: 5927.6079\n",
      "Epoch 8145/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6873.8556 - val_loss: 5927.1450\n",
      "Epoch 8146/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6873.3927 - val_loss: 5926.6826\n",
      "Epoch 8147/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6872.9298 - val_loss: 5926.2183\n",
      "Epoch 8148/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 6872.4677 - val_loss: 5925.7568\n",
      "Epoch 8149/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6872.0053 - val_loss: 5925.2959\n",
      "Epoch 8150/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6871.5426 - val_loss: 5924.8315\n",
      "Epoch 8151/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6871.0796 - val_loss: 5924.3687\n",
      "Epoch 8152/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6870.6166 - val_loss: 5923.9077\n",
      "Epoch 8153/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6870.1543 - val_loss: 5923.4443\n",
      "Epoch 8154/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6869.6922 - val_loss: 5922.9800\n",
      "Epoch 8155/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6869.2287 - val_loss: 5922.5186\n",
      "Epoch 8156/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6868.7653 - val_loss: 5922.0566\n",
      "Epoch 8157/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6868.3033 - val_loss: 5921.5928\n",
      "Epoch 8158/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6867.8410 - val_loss: 5921.1289\n",
      "Epoch 8159/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6867.3780 - val_loss: 5920.6680\n",
      "Epoch 8160/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6866.9144 - val_loss: 5920.2061\n",
      "Epoch 8161/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6866.4525 - val_loss: 5919.7412\n",
      "Epoch 8162/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6865.9902 - val_loss: 5919.2803\n",
      "Epoch 8163/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6865.5264 - val_loss: 5918.8164\n",
      "Epoch 8164/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6865.0652 - val_loss: 5918.3545\n",
      "Epoch 8165/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6864.6015 - val_loss: 5917.8906\n",
      "Epoch 8166/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6864.1388 - val_loss: 5917.4292\n",
      "Epoch 8167/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6863.6772 - val_loss: 5916.9653\n",
      "Epoch 8168/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6863.2144 - val_loss: 5916.5029\n",
      "Epoch 8169/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6862.7508 - val_loss: 5916.0420\n",
      "Epoch 8170/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6862.2880 - val_loss: 5915.5771\n",
      "Epoch 8171/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6861.8257 - val_loss: 5915.1138\n",
      "Epoch 8172/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6861.3631 - val_loss: 5914.6523\n",
      "Epoch 8173/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 6860.8996 - val_loss: 5914.1904\n",
      "Epoch 8174/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6860.4368 - val_loss: 5913.7261\n",
      "Epoch 8175/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 6859.9751 - val_loss: 5913.2646\n",
      "Epoch 8176/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6859.5118 - val_loss: 5912.8013\n",
      "Epoch 8177/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6859.0490 - val_loss: 5912.3389\n",
      "Epoch 8178/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6858.5862 - val_loss: 5911.8740\n",
      "Epoch 8179/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6858.1240 - val_loss: 5911.4131\n",
      "Epoch 8180/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6857.6612 - val_loss: 5910.9517\n",
      "Epoch 8181/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6857.1991 - val_loss: 5910.4873\n",
      "Epoch 8182/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6856.7361 - val_loss: 5910.0249\n",
      "Epoch 8183/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6856.2727 - val_loss: 5909.5640\n",
      "Epoch 8184/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6855.8109 - val_loss: 5909.1006\n",
      "Epoch 8185/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 6855.3482 - val_loss: 5908.6367\n",
      "Epoch 8186/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6854.8850 - val_loss: 5908.1748\n",
      "Epoch 8187/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6854.4218 - val_loss: 5907.7109\n",
      "Epoch 8188/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6853.9594 - val_loss: 5907.2490\n",
      "Epoch 8189/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6853.4970 - val_loss: 5906.7871\n",
      "Epoch 8190/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6853.0340 - val_loss: 5906.3242\n",
      "Epoch 8191/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6852.5708 - val_loss: 5905.8608\n",
      "Epoch 8192/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6852.1089 - val_loss: 5905.3975\n",
      "Epoch 8193/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 6851.6464 - val_loss: 5904.9365\n",
      "Epoch 8194/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 6851.1829 - val_loss: 5904.4727\n",
      "Epoch 8195/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 6850.7212 - val_loss: 5904.0098\n",
      "Epoch 8196/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6850.2575 - val_loss: 5903.5483\n",
      "Epoch 8197/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6849.7953 - val_loss: 5903.0850\n",
      "Epoch 8198/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6849.3332 - val_loss: 5902.6211\n",
      "Epoch 8199/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6848.8701 - val_loss: 5902.1592\n",
      "Epoch 8200/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6848.4069 - val_loss: 5901.6982\n",
      "Epoch 8201/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6847.9442 - val_loss: 5901.2339\n",
      "Epoch 8202/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6847.4822 - val_loss: 5900.7700\n",
      "Epoch 8203/10000\n",
      "750/750 [==============================] - 0s 168us/step - loss: 6847.0191 - val_loss: 5900.3086\n",
      "Epoch 8204/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6846.5560 - val_loss: 5899.8467\n",
      "Epoch 8205/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6846.0931 - val_loss: 5899.3823\n",
      "Epoch 8206/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 6845.6312 - val_loss: 5898.9209\n",
      "Epoch 8207/10000\n",
      "750/750 [==============================] - 0s 174us/step - loss: 6845.1682 - val_loss: 5898.4595\n",
      "Epoch 8208/10000\n",
      "750/750 [==============================] - 0s 160us/step - loss: 6844.7059 - val_loss: 5897.9951\n",
      "Epoch 8209/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6844.2431 - val_loss: 5897.5308\n",
      "Epoch 8210/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6843.7800 - val_loss: 5897.0698\n",
      "Epoch 8211/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6843.3178 - val_loss: 5896.6060\n",
      "Epoch 8212/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6842.8552 - val_loss: 5896.1436\n",
      "Epoch 8213/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6842.3918 - val_loss: 5895.6826\n",
      "Epoch 8214/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6841.9289 - val_loss: 5895.2183\n",
      "Epoch 8215/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6841.4667 - val_loss: 5894.7563\n",
      "Epoch 8216/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 6841.0044 - val_loss: 5894.2930\n",
      "Epoch 8217/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6840.5405 - val_loss: 5893.8311\n",
      "Epoch 8218/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6840.0778 - val_loss: 5893.3687\n",
      "Epoch 8219/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6839.6159 - val_loss: 5892.9053\n",
      "Epoch 8220/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6839.1527 - val_loss: 5892.4419\n",
      "Epoch 8221/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6838.6905 - val_loss: 5891.9795\n",
      "Epoch 8222/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6838.2272 - val_loss: 5891.5171\n",
      "Epoch 8223/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6837.7649 - val_loss: 5891.0547\n",
      "Epoch 8224/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6837.3026 - val_loss: 5890.5928\n",
      "Epoch 8225/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6836.8401 - val_loss: 5890.1289\n",
      "Epoch 8226/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6836.3775 - val_loss: 5889.6670\n",
      "Epoch 8227/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6835.9140 - val_loss: 5889.2046\n",
      "Epoch 8228/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6835.4517 - val_loss: 5888.7412\n",
      "Epoch 8229/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6834.9896 - val_loss: 5888.2773\n",
      "Epoch 8230/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6834.5261 - val_loss: 5887.8159\n",
      "Epoch 8231/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6834.0631 - val_loss: 5887.3540\n",
      "Epoch 8232/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6833.6005 - val_loss: 5886.8896\n",
      "Epoch 8233/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6833.1380 - val_loss: 5886.4263\n",
      "Epoch 8234/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6832.6751 - val_loss: 5885.9648\n",
      "Epoch 8235/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6832.2118 - val_loss: 5885.5029\n",
      "Epoch 8236/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6831.7495 - val_loss: 5885.0386\n",
      "Epoch 8237/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 6831.2874 - val_loss: 5884.5771\n",
      "Epoch 8238/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6830.8244 - val_loss: 5884.1128\n",
      "Epoch 8239/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6830.3625 - val_loss: 5883.6514\n",
      "Epoch 8240/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6829.8991 - val_loss: 5883.1875\n",
      "Epoch 8241/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6829.4362 - val_loss: 5882.7261\n",
      "Epoch 8242/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6828.9743 - val_loss: 5882.2622\n",
      "Epoch 8243/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6828.5112 - val_loss: 5881.7998\n",
      "Epoch 8244/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6828.0485 - val_loss: 5881.3389\n",
      "Epoch 8245/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6827.5851 - val_loss: 5880.8740\n",
      "Epoch 8246/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6827.1232 - val_loss: 5880.4121\n",
      "Epoch 8247/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6826.6607 - val_loss: 5879.9492\n",
      "Epoch 8248/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6826.1968 - val_loss: 5879.4878\n",
      "Epoch 8249/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6825.7343 - val_loss: 5879.0229\n",
      "Epoch 8250/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6825.2723 - val_loss: 5878.5615\n",
      "Epoch 8251/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6824.8087 - val_loss: 5878.1006\n",
      "Epoch 8252/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6824.3469 - val_loss: 5877.6357\n",
      "Epoch 8253/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6823.8839 - val_loss: 5877.1733\n",
      "Epoch 8254/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6823.4213 - val_loss: 5876.7104\n",
      "Epoch 8255/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6822.9587 - val_loss: 5876.2490\n",
      "Epoch 8256/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6822.4963 - val_loss: 5875.7842\n",
      "Epoch 8257/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6822.0334 - val_loss: 5875.3232\n",
      "Epoch 8258/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6821.5698 - val_loss: 5874.8608\n",
      "Epoch 8259/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 6821.1077 - val_loss: 5874.3975\n",
      "Epoch 8260/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6820.6454 - val_loss: 5873.9336\n",
      "Epoch 8261/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6820.1821 - val_loss: 5873.4722\n",
      "Epoch 8262/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6819.7189 - val_loss: 5873.0093\n",
      "Epoch 8263/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 138us/step - loss: 6819.2568 - val_loss: 5872.5459\n",
      "Epoch 8264/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6818.7944 - val_loss: 5872.0850\n",
      "Epoch 8265/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6818.3314 - val_loss: 5871.6211\n",
      "Epoch 8266/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6817.8680 - val_loss: 5871.1587\n",
      "Epoch 8267/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6817.4066 - val_loss: 5870.6948\n",
      "Epoch 8268/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 6816.9437 - val_loss: 5870.2334\n",
      "Epoch 8269/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 6816.4811 - val_loss: 5869.7700\n",
      "Epoch 8270/10000\n",
      "750/750 [==============================] - 0s 168us/step - loss: 6816.0187 - val_loss: 5869.3076\n",
      "Epoch 8271/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 6815.5556 - val_loss: 5868.8452\n",
      "Epoch 8272/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 6815.0924 - val_loss: 5868.3823\n",
      "Epoch 8273/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6814.6307 - val_loss: 5867.9185\n",
      "Epoch 8274/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 6814.1670 - val_loss: 5867.4561\n",
      "Epoch 8275/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6813.7041 - val_loss: 5866.9951\n",
      "Epoch 8276/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6813.2413 - val_loss: 5866.5308\n",
      "Epoch 8277/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6812.7794 - val_loss: 5866.0669\n",
      "Epoch 8278/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6812.3161 - val_loss: 5865.6060\n",
      "Epoch 8279/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6811.8533 - val_loss: 5865.1436\n",
      "Epoch 8280/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6811.3906 - val_loss: 5864.6792\n",
      "Epoch 8281/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6810.9282 - val_loss: 5864.2178\n",
      "Epoch 8282/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6810.4653 - val_loss: 5863.7563\n",
      "Epoch 8283/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6810.0029 - val_loss: 5863.2920\n",
      "Epoch 8284/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6809.5405 - val_loss: 5862.8296\n",
      "Epoch 8285/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6809.0777 - val_loss: 5862.3667\n",
      "Epoch 8286/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6808.6152 - val_loss: 5861.9053\n",
      "Epoch 8287/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6808.1523 - val_loss: 5861.4409\n",
      "Epoch 8288/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6807.6896 - val_loss: 5860.9795\n",
      "Epoch 8289/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6807.2262 - val_loss: 5860.5171\n",
      "Epoch 8290/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6806.7641 - val_loss: 5860.0532\n",
      "Epoch 8291/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6806.3018 - val_loss: 5859.5898\n",
      "Epoch 8292/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6805.8382 - val_loss: 5859.1289\n",
      "Epoch 8293/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6805.3752 - val_loss: 5858.6660\n",
      "Epoch 8294/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6804.9132 - val_loss: 5858.2021\n",
      "Epoch 8295/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6804.4504 - val_loss: 5857.7402\n",
      "Epoch 8296/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6803.9877 - val_loss: 5857.2773\n",
      "Epoch 8297/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6803.5244 - val_loss: 5856.8140\n",
      "Epoch 8298/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6803.0622 - val_loss: 5856.3511\n",
      "Epoch 8299/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6802.5999 - val_loss: 5855.8901\n",
      "Epoch 8300/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6802.1381 - val_loss: 5855.4253\n",
      "Epoch 8301/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6801.6746 - val_loss: 5854.9639\n",
      "Epoch 8302/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6801.2113 - val_loss: 5854.5024\n",
      "Epoch 8303/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6800.7487 - val_loss: 5854.0386\n",
      "Epoch 8304/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6800.2869 - val_loss: 5853.5747\n",
      "Epoch 8305/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6799.8239 - val_loss: 5853.1128\n",
      "Epoch 8306/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6799.3604 - val_loss: 5852.6514\n",
      "Epoch 8307/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6798.8974 - val_loss: 5852.1865\n",
      "Epoch 8308/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6798.4356 - val_loss: 5851.7246\n",
      "Epoch 8309/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6797.9726 - val_loss: 5851.2622\n",
      "Epoch 8310/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6797.5094 - val_loss: 5850.7998\n",
      "Epoch 8311/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6797.0475 - val_loss: 5850.3354\n",
      "Epoch 8312/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6796.5847 - val_loss: 5849.8740\n",
      "Epoch 8313/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6796.1213 - val_loss: 5849.4106\n",
      "Epoch 8314/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6795.6597 - val_loss: 5848.9482\n",
      "Epoch 8315/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6795.1963 - val_loss: 5848.4858\n",
      "Epoch 8316/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6794.7334 - val_loss: 5848.0249\n",
      "Epoch 8317/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6794.2715 - val_loss: 5847.5610\n",
      "Epoch 8318/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6793.8087 - val_loss: 5847.0967\n",
      "Epoch 8319/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6793.3456 - val_loss: 5846.6357\n",
      "Epoch 8320/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6792.8820 - val_loss: 5846.1733\n",
      "Epoch 8321/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6792.4203 - val_loss: 5845.7100\n",
      "Epoch 8322/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6791.9580 - val_loss: 5845.2466\n",
      "Epoch 8323/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6791.4939 - val_loss: 5844.7847\n",
      "Epoch 8324/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6791.0314 - val_loss: 5844.3218\n",
      "Epoch 8325/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6790.5693 - val_loss: 5843.8584\n",
      "Epoch 8326/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6790.1065 - val_loss: 5843.3975\n",
      "Epoch 8327/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6789.6445 - val_loss: 5842.9336\n",
      "Epoch 8328/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6789.1814 - val_loss: 5842.4712\n",
      "Epoch 8329/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6788.7189 - val_loss: 5842.0073\n",
      "Epoch 8330/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6788.2561 - val_loss: 5841.5459\n",
      "Epoch 8331/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6787.7937 - val_loss: 5841.0815\n",
      "Epoch 8332/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6787.3305 - val_loss: 5840.6201\n",
      "Epoch 8333/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 6786.8672 - val_loss: 5840.1592\n",
      "Epoch 8334/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6786.4048 - val_loss: 5839.6943\n",
      "Epoch 8335/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 136us/step - loss: 6785.9429 - val_loss: 5839.2310\n",
      "Epoch 8336/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 6785.4794 - val_loss: 5838.7690\n",
      "Epoch 8337/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6785.0166 - val_loss: 5838.3076\n",
      "Epoch 8338/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6784.5540 - val_loss: 5837.8428\n",
      "Epoch 8339/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6784.0915 - val_loss: 5837.3818\n",
      "Epoch 8340/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6783.6285 - val_loss: 5836.9185\n",
      "Epoch 8341/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6783.1656 - val_loss: 5836.4561\n",
      "Epoch 8342/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6782.7037 - val_loss: 5835.9917\n",
      "Epoch 8343/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6782.2406 - val_loss: 5835.5303\n",
      "Epoch 8344/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6781.7786 - val_loss: 5835.0669\n",
      "Epoch 8345/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6781.3156 - val_loss: 5834.6045\n",
      "Epoch 8346/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6780.8528 - val_loss: 5834.1426\n",
      "Epoch 8347/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6780.3898 - val_loss: 5833.6792\n",
      "Epoch 8348/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6779.9278 - val_loss: 5833.2153\n",
      "Epoch 8349/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6779.4646 - val_loss: 5832.7529\n",
      "Epoch 8350/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6779.0016 - val_loss: 5832.2920\n",
      "Epoch 8351/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6778.5385 - val_loss: 5831.8296\n",
      "Epoch 8352/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6778.0766 - val_loss: 5831.3657\n",
      "Epoch 8353/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6777.6139 - val_loss: 5830.9028\n",
      "Epoch 8354/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6777.1502 - val_loss: 5830.4404\n",
      "Epoch 8355/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6776.6877 - val_loss: 5829.9761\n",
      "Epoch 8356/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6776.2256 - val_loss: 5829.5151\n",
      "Epoch 8357/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6775.7633 - val_loss: 5829.0532\n",
      "Epoch 8358/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6775.3007 - val_loss: 5828.5894\n",
      "Epoch 8359/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6774.8375 - val_loss: 5828.1265\n",
      "Epoch 8360/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6774.3745 - val_loss: 5827.6636\n",
      "Epoch 8361/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6773.9119 - val_loss: 5827.2021\n",
      "Epoch 8362/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6773.4501 - val_loss: 5826.7378\n",
      "Epoch 8363/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6772.9871 - val_loss: 5826.2764\n",
      "Epoch 8364/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6772.5234 - val_loss: 5825.8140\n",
      "Epoch 8365/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6772.0614 - val_loss: 5825.3506\n",
      "Epoch 8366/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6771.5992 - val_loss: 5824.8867\n",
      "Epoch 8367/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6771.1354 - val_loss: 5824.4253\n",
      "Epoch 8368/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6770.6728 - val_loss: 5823.9639\n",
      "Epoch 8369/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6770.2103 - val_loss: 5823.4990\n",
      "Epoch 8370/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6769.7476 - val_loss: 5823.0381\n",
      "Epoch 8371/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6769.2853 - val_loss: 5822.5742\n",
      "Epoch 8372/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6768.8224 - val_loss: 5822.1123\n",
      "Epoch 8373/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6768.3598 - val_loss: 5821.6484\n",
      "Epoch 8374/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6767.8969 - val_loss: 5821.1865\n",
      "Epoch 8375/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6767.4349 - val_loss: 5820.7231\n",
      "Epoch 8376/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6766.9718 - val_loss: 5820.2607\n",
      "Epoch 8377/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6766.5088 - val_loss: 5819.7998\n",
      "Epoch 8378/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6766.0461 - val_loss: 5819.3354\n",
      "Epoch 8379/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6765.5838 - val_loss: 5818.8716\n",
      "Epoch 8380/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6765.1208 - val_loss: 5818.4102\n",
      "Epoch 8381/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6764.6573 - val_loss: 5817.9482\n",
      "Epoch 8382/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6764.1948 - val_loss: 5817.4839\n",
      "Epoch 8383/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6763.7331 - val_loss: 5817.0225\n",
      "Epoch 8384/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6763.2701 - val_loss: 5816.5591\n",
      "Epoch 8385/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6762.8064 - val_loss: 5816.0967\n",
      "Epoch 8386/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6762.3444 - val_loss: 5815.6323\n",
      "Epoch 8387/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6761.8817 - val_loss: 5815.1709\n",
      "Epoch 8388/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6761.4188 - val_loss: 5814.7095\n",
      "Epoch 8389/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6760.9572 - val_loss: 5814.2451\n",
      "Epoch 8390/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6760.4941 - val_loss: 5813.7837\n",
      "Epoch 8391/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6760.0307 - val_loss: 5813.3218\n",
      "Epoch 8392/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6759.5686 - val_loss: 5812.8584\n",
      "Epoch 8393/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6759.1061 - val_loss: 5812.3940\n",
      "Epoch 8394/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6758.6429 - val_loss: 5811.9326\n",
      "Epoch 8395/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6758.1795 - val_loss: 5811.4702\n",
      "Epoch 8396/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6757.7174 - val_loss: 5811.0068\n",
      "Epoch 8397/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6757.2549 - val_loss: 5810.5454\n",
      "Epoch 8398/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6756.7914 - val_loss: 5810.0815\n",
      "Epoch 8399/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6756.3291 - val_loss: 5809.6196\n",
      "Epoch 8400/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6755.8669 - val_loss: 5809.1553\n",
      "Epoch 8401/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6755.4039 - val_loss: 5808.6943\n",
      "Epoch 8402/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6754.9415 - val_loss: 5808.2305\n",
      "Epoch 8403/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6754.4790 - val_loss: 5807.7686\n",
      "Epoch 8404/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6754.0157 - val_loss: 5807.3062\n",
      "Epoch 8405/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6753.5533 - val_loss: 5806.8428\n",
      "Epoch 8406/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6753.0908 - val_loss: 5806.3789\n",
      "Epoch 8407/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 121us/step - loss: 6752.6282 - val_loss: 5805.9170\n",
      "Epoch 8408/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6752.1650 - val_loss: 5805.4561\n",
      "Epoch 8409/10000\n",
      "750/750 [==============================] - 0s 143us/step - loss: 6751.7022 - val_loss: 5804.9917\n",
      "Epoch 8410/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6751.2401 - val_loss: 5804.5278\n",
      "Epoch 8411/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6750.7764 - val_loss: 5804.0669\n",
      "Epoch 8412/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6750.3146 - val_loss: 5803.6045\n",
      "Epoch 8413/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6749.8511 - val_loss: 5803.1401\n",
      "Epoch 8414/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6749.3890 - val_loss: 5802.6787\n",
      "Epoch 8415/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6748.9268 - val_loss: 5802.2168\n",
      "Epoch 8416/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6748.4635 - val_loss: 5801.7529\n",
      "Epoch 8417/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6748.0012 - val_loss: 5801.2905\n",
      "Epoch 8418/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6747.5379 - val_loss: 5800.8276\n",
      "Epoch 8419/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6747.0756 - val_loss: 5800.3657\n",
      "Epoch 8420/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6746.6134 - val_loss: 5799.9014\n",
      "Epoch 8421/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6746.1499 - val_loss: 5799.4404\n",
      "Epoch 8422/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6745.6868 - val_loss: 5798.9761\n",
      "Epoch 8423/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6745.2249 - val_loss: 5798.5146\n",
      "Epoch 8424/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6744.7624 - val_loss: 5798.0503\n",
      "Epoch 8425/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6744.2991 - val_loss: 5797.5894\n",
      "Epoch 8426/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6743.8359 - val_loss: 5797.1265\n",
      "Epoch 8427/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6743.3736 - val_loss: 5796.6631\n",
      "Epoch 8428/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6742.9110 - val_loss: 5796.2017\n",
      "Epoch 8429/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6742.4481 - val_loss: 5795.7378\n",
      "Epoch 8430/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 6741.9853 - val_loss: 5795.2749\n",
      "Epoch 8431/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6741.5229 - val_loss: 5794.8125\n",
      "Epoch 8432/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6741.0601 - val_loss: 5794.3506\n",
      "Epoch 8433/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6740.5983 - val_loss: 5793.8867\n",
      "Epoch 8434/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6740.1351 - val_loss: 5793.4248\n",
      "Epoch 8435/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 6739.6717 - val_loss: 5792.9624\n",
      "Epoch 8436/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 6739.2095 - val_loss: 5792.4990\n",
      "Epoch 8437/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 6738.7472 - val_loss: 5792.0352\n",
      "Epoch 8438/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 6738.2844 - val_loss: 5791.5737\n",
      "Epoch 8439/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 6737.8204 - val_loss: 5791.1113\n",
      "Epoch 8440/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6737.3585 - val_loss: 5790.6475\n",
      "Epoch 8441/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6736.8965 - val_loss: 5790.1841\n",
      "Epoch 8442/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6736.4327 - val_loss: 5789.7227\n",
      "Epoch 8443/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6735.9700 - val_loss: 5789.2607\n",
      "Epoch 8444/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6735.5075 - val_loss: 5788.7964\n",
      "Epoch 8445/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6735.0454 - val_loss: 5788.3350\n",
      "Epoch 8446/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6734.5824 - val_loss: 5787.8716\n",
      "Epoch 8447/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6734.1198 - val_loss: 5787.4092\n",
      "Epoch 8448/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 6733.6569 - val_loss: 5786.9468\n",
      "Epoch 8449/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6733.1942 - val_loss: 5786.4844\n",
      "Epoch 8450/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6732.7322 - val_loss: 5786.0220\n",
      "Epoch 8451/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6732.2697 - val_loss: 5785.5576\n",
      "Epoch 8452/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 6717.50 - 0s 122us/step - loss: 6731.8061 - val_loss: 5785.0967\n",
      "Epoch 8453/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6731.3433 - val_loss: 5784.6323\n",
      "Epoch 8454/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 6730.8809 - val_loss: 5784.1704\n",
      "Epoch 8455/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 6730.4184 - val_loss: 5783.7075\n",
      "Epoch 8456/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 6729.9551 - val_loss: 5783.2451\n",
      "Epoch 8457/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 6729.4918 - val_loss: 5782.7827\n",
      "Epoch 8458/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 6729.0306 - val_loss: 5782.3193\n",
      "Epoch 8459/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6728.5670 - val_loss: 5781.8579\n",
      "Epoch 8460/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6728.1044 - val_loss: 5781.3940\n",
      "Epoch 8461/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 6727.6421 - val_loss: 5780.9312\n",
      "Epoch 8462/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6727.1791 - val_loss: 5780.4683\n",
      "Epoch 8463/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6726.7164 - val_loss: 5780.0063\n",
      "Epoch 8464/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6726.2542 - val_loss: 5779.5430\n",
      "Epoch 8465/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6725.7913 - val_loss: 5779.0811\n",
      "Epoch 8466/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6725.3284 - val_loss: 5778.6196\n",
      "Epoch 8467/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 6724.8657 - val_loss: 5778.1553\n",
      "Epoch 8468/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6724.4032 - val_loss: 5777.6914\n",
      "Epoch 8469/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 6723.9405 - val_loss: 5777.2300\n",
      "Epoch 8470/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 6723.4767 - val_loss: 5776.7681\n",
      "Epoch 8471/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6723.0147 - val_loss: 5776.3037\n",
      "Epoch 8472/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6722.5526 - val_loss: 5775.8423\n",
      "Epoch 8473/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6722.0889 - val_loss: 5775.3789\n",
      "Epoch 8474/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6721.6263 - val_loss: 5774.9165\n",
      "Epoch 8475/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6721.1641 - val_loss: 5774.4526\n",
      "Epoch 8476/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6720.7015 - val_loss: 5773.9912\n",
      "Epoch 8477/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6720.2393 - val_loss: 5773.5278\n",
      "Epoch 8478/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6719.7766 - val_loss: 5773.0649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8479/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6719.3132 - val_loss: 5772.6035\n",
      "Epoch 8480/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6718.8503 - val_loss: 5772.1396\n",
      "Epoch 8481/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6718.3879 - val_loss: 5771.6763\n",
      "Epoch 8482/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6717.9257 - val_loss: 5771.2139\n",
      "Epoch 8483/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6717.4622 - val_loss: 5770.7529\n",
      "Epoch 8484/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6716.9994 - val_loss: 5770.2886\n",
      "Epoch 8485/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6716.5373 - val_loss: 5769.8267\n",
      "Epoch 8486/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6716.0742 - val_loss: 5769.3638\n",
      "Epoch 8487/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 6715.6115 - val_loss: 5768.9014\n",
      "Epoch 8488/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6715.1483 - val_loss: 5768.4375\n",
      "Epoch 8489/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 6714.6866 - val_loss: 5767.9756\n",
      "Epoch 8490/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6714.2238 - val_loss: 5767.5142\n",
      "Epoch 8491/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6713.7608 - val_loss: 5767.0498\n",
      "Epoch 8492/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6713.2986 - val_loss: 5766.5874\n",
      "Epoch 8493/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6712.8353 - val_loss: 5766.1250\n",
      "Epoch 8494/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6712.3728 - val_loss: 5765.6626\n",
      "Epoch 8495/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6711.9107 - val_loss: 5765.1992\n",
      "Epoch 8496/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6711.4472 - val_loss: 5764.7373\n",
      "Epoch 8497/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6710.9840 - val_loss: 5764.2749\n",
      "Epoch 8498/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 6710.5220 - val_loss: 5763.8110\n",
      "Epoch 8499/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6710.0595 - val_loss: 5763.3477\n",
      "Epoch 8500/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6709.5960 - val_loss: 5762.8867\n",
      "Epoch 8501/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 6709.1332 - val_loss: 5762.4238\n",
      "Epoch 8502/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6708.6711 - val_loss: 5761.9600\n",
      "Epoch 8503/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6708.2089 - val_loss: 5761.4985\n",
      "Epoch 8504/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6707.7453 - val_loss: 5761.0352\n",
      "Epoch 8505/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 6707.2829 - val_loss: 5760.5718\n",
      "Epoch 8506/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6706.8203 - val_loss: 5760.1094\n",
      "Epoch 8507/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6706.3576 - val_loss: 5759.6475\n",
      "Epoch 8508/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6705.8956 - val_loss: 5759.1841\n",
      "Epoch 8509/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 6705.4323 - val_loss: 5758.7212\n",
      "Epoch 8510/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 6704.9694 - val_loss: 5758.2593\n",
      "Epoch 8511/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 6704.5063 - val_loss: 5757.7959\n",
      "Epoch 8512/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6704.0443 - val_loss: 5757.3325\n",
      "Epoch 8513/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6703.5814 - val_loss: 5756.8711\n",
      "Epoch 8514/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6703.1183 - val_loss: 5756.4087\n",
      "Epoch 8515/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6702.6559 - val_loss: 5755.9448\n",
      "Epoch 8516/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6702.1938 - val_loss: 5755.4810\n",
      "Epoch 8517/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6701.7303 - val_loss: 5755.0200\n",
      "Epoch 8518/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6701.2677 - val_loss: 5754.5576\n",
      "Epoch 8519/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6700.8049 - val_loss: 5754.0933\n",
      "Epoch 8520/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6700.3426 - val_loss: 5753.6318\n",
      "Epoch 8521/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6699.8801 - val_loss: 5753.1685\n",
      "Epoch 8522/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6699.4174 - val_loss: 5752.7061\n",
      "Epoch 8523/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 6698.9546 - val_loss: 5752.2437\n",
      "Epoch 8524/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6698.4915 - val_loss: 5751.7827\n",
      "Epoch 8525/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 6698.0292 - val_loss: 5751.3188\n",
      "Epoch 8526/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 6697.5667 - val_loss: 5750.8550\n",
      "Epoch 8527/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6697.1036 - val_loss: 5750.3936\n",
      "Epoch 8528/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6696.6404 - val_loss: 5749.9312\n",
      "Epoch 8529/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6696.1782 - val_loss: 5749.4673\n",
      "Epoch 8530/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6695.7158 - val_loss: 5749.0059\n",
      "Epoch 8531/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6695.2526 - val_loss: 5748.5425\n",
      "Epoch 8532/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 6694.7892 - val_loss: 5748.0796\n",
      "Epoch 8533/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6694.3277 - val_loss: 5747.6162\n",
      "Epoch 8534/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6693.8645 - val_loss: 5747.1553\n",
      "Epoch 8535/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6693.4020 - val_loss: 5746.6914\n",
      "Epoch 8536/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6692.9399 - val_loss: 5746.2285\n",
      "Epoch 8537/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6692.4762 - val_loss: 5745.7656\n",
      "Epoch 8538/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6692.0139 - val_loss: 5745.3037\n",
      "Epoch 8539/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6691.5516 - val_loss: 5744.8398\n",
      "Epoch 8540/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 6691.0886 - val_loss: 5744.3779\n",
      "Epoch 8541/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6690.6254 - val_loss: 5743.9165\n",
      "Epoch 8542/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 6731.92 - 0s 100us/step - loss: 6690.1628 - val_loss: 5743.4521\n",
      "Epoch 8543/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 6689.7005 - val_loss: 5742.9878\n",
      "Epoch 8544/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 6689.2380 - val_loss: 5742.5273\n",
      "Epoch 8545/10000\n",
      "750/750 [==============================] - 0s 99us/step - loss: 6688.7741 - val_loss: 5742.0649\n",
      "Epoch 8546/10000\n",
      "750/750 [==============================] - 0s 103us/step - loss: 6688.3117 - val_loss: 5741.6011\n",
      "Epoch 8547/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6687.8499 - val_loss: 5741.1392\n",
      "Epoch 8548/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6687.3864 - val_loss: 5740.6763\n",
      "Epoch 8549/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6686.9244 - val_loss: 5740.2139\n",
      "Epoch 8550/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6686.4612 - val_loss: 5739.7500\n",
      "Epoch 8551/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6685.9985 - val_loss: 5739.2881\n",
      "Epoch 8552/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6685.5362 - val_loss: 5738.8267\n",
      "Epoch 8553/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6685.0739 - val_loss: 5738.3623\n",
      "Epoch 8554/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6684.6110 - val_loss: 5737.9009\n",
      "Epoch 8555/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6684.1477 - val_loss: 5737.4375\n",
      "Epoch 8556/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6683.6855 - val_loss: 5736.9731\n",
      "Epoch 8557/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6683.2229 - val_loss: 5736.5117\n",
      "Epoch 8558/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6682.7594 - val_loss: 5736.0498\n",
      "Epoch 8559/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6682.2965 - val_loss: 5735.5874\n",
      "Epoch 8560/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6681.8346 - val_loss: 5735.1235\n",
      "Epoch 8561/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6681.3714 - val_loss: 5734.6626\n",
      "Epoch 8562/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6680.9087 - val_loss: 5734.1982\n",
      "Epoch 8563/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6680.4457 - val_loss: 5733.7344\n",
      "Epoch 8564/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6679.9835 - val_loss: 5733.2734\n",
      "Epoch 8565/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6679.5212 - val_loss: 5732.8110\n",
      "Epoch 8566/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6679.0582 - val_loss: 5732.3477\n",
      "Epoch 8567/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6678.5959 - val_loss: 5731.8843\n",
      "Epoch 8568/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6678.1324 - val_loss: 5731.4233\n",
      "Epoch 8569/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6677.6700 - val_loss: 5730.9600\n",
      "Epoch 8570/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6677.2083 - val_loss: 5730.4961\n",
      "Epoch 8571/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6676.7446 - val_loss: 5730.0342\n",
      "Epoch 8572/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6676.2817 - val_loss: 5729.5718\n",
      "Epoch 8573/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6675.8191 - val_loss: 5729.1084\n",
      "Epoch 8574/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6675.3569 - val_loss: 5728.6440\n",
      "Epoch 8575/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6674.8937 - val_loss: 5728.1836\n",
      "Epoch 8576/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6674.4302 - val_loss: 5727.7212\n",
      "Epoch 8577/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6673.9684 - val_loss: 5727.2568\n",
      "Epoch 8578/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6673.5057 - val_loss: 5726.7959\n",
      "Epoch 8579/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6673.0435 - val_loss: 5726.3315\n",
      "Epoch 8580/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6672.5808 - val_loss: 5725.8696\n",
      "Epoch 8581/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6672.1178 - val_loss: 5725.4077\n",
      "Epoch 8582/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6671.6551 - val_loss: 5724.9448\n",
      "Epoch 8583/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6671.1929 - val_loss: 5724.4810\n",
      "Epoch 8584/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6670.7300 - val_loss: 5724.0186\n",
      "Epoch 8585/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6670.2671 - val_loss: 5723.5571\n",
      "Epoch 8586/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6669.8038 - val_loss: 5723.0928\n",
      "Epoch 8587/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6669.3417 - val_loss: 5722.6294\n",
      "Epoch 8588/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6668.8787 - val_loss: 5722.1680\n",
      "Epoch 8589/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6668.4152 - val_loss: 5721.7061\n",
      "Epoch 8590/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6667.9530 - val_loss: 5721.2422\n",
      "Epoch 8591/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6667.4908 - val_loss: 5720.7803\n",
      "Epoch 8592/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6667.0275 - val_loss: 5720.3184\n",
      "Epoch 8593/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6666.5653 - val_loss: 5719.8550\n",
      "Epoch 8594/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6666.1023 - val_loss: 5719.3906\n",
      "Epoch 8595/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 6665.6398 - val_loss: 5718.9292\n",
      "Epoch 8596/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6665.1774 - val_loss: 5718.4673\n",
      "Epoch 8597/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6664.7149 - val_loss: 5718.0029\n",
      "Epoch 8598/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6664.2519 - val_loss: 5717.5420\n",
      "Epoch 8599/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6663.7884 - val_loss: 5717.0796\n",
      "Epoch 8600/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6663.3264 - val_loss: 5716.6162\n",
      "Epoch 8601/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6662.8640 - val_loss: 5716.1523\n",
      "Epoch 8602/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 6662.4008 - val_loss: 5715.6904\n",
      "Epoch 8603/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6661.9374 - val_loss: 5715.2280\n",
      "Epoch 8604/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6661.4754 - val_loss: 5714.7646\n",
      "Epoch 8605/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6661.0127 - val_loss: 5714.3032\n",
      "Epoch 8606/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6660.5503 - val_loss: 5713.8398\n",
      "Epoch 8607/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6660.0865 - val_loss: 5713.3774\n",
      "Epoch 8608/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6659.6249 - val_loss: 5712.9131\n",
      "Epoch 8609/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6659.1620 - val_loss: 5712.4521\n",
      "Epoch 8610/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6658.6991 - val_loss: 5711.9878\n",
      "Epoch 8611/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6658.2372 - val_loss: 5711.5254\n",
      "Epoch 8612/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6657.7734 - val_loss: 5711.0640\n",
      "Epoch 8613/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6657.3113 - val_loss: 5710.6006\n",
      "Epoch 8614/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6656.8491 - val_loss: 5710.1372\n",
      "Epoch 8615/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6656.3862 - val_loss: 5709.6748\n",
      "Epoch 8616/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 6653.65 - 0s 117us/step - loss: 6655.9226 - val_loss: 5709.2139\n",
      "Epoch 8617/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6655.4601 - val_loss: 5708.7500\n",
      "Epoch 8618/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6654.9981 - val_loss: 5708.2856\n",
      "Epoch 8619/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6654.5348 - val_loss: 5707.8242\n",
      "Epoch 8620/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6654.0718 - val_loss: 5707.3623\n",
      "Epoch 8621/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6653.6091 - val_loss: 5706.8984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8622/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6653.1473 - val_loss: 5706.4365\n",
      "Epoch 8623/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6652.6839 - val_loss: 5705.9746\n",
      "Epoch 8624/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6652.2220 - val_loss: 5705.5107\n",
      "Epoch 8625/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 6651.7588 - val_loss: 5705.0469\n",
      "Epoch 8626/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6651.2958 - val_loss: 5704.5859\n",
      "Epoch 8627/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6650.8337 - val_loss: 5704.1235\n",
      "Epoch 8628/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6650.3709 - val_loss: 5703.6592\n",
      "Epoch 8629/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6649.9080 - val_loss: 5703.1978\n",
      "Epoch 8630/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6649.4447 - val_loss: 5702.7344\n",
      "Epoch 8631/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6648.9825 - val_loss: 5702.2720\n",
      "Epoch 8632/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6648.5203 - val_loss: 5701.8086\n",
      "Epoch 8633/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6648.0569 - val_loss: 5701.3472\n",
      "Epoch 8634/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6647.5937 - val_loss: 5700.8843\n",
      "Epoch 8635/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6647.1319 - val_loss: 5700.4204\n",
      "Epoch 8636/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6646.6689 - val_loss: 5699.9595\n",
      "Epoch 8637/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6646.2058 - val_loss: 5699.4961\n",
      "Epoch 8638/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6645.7429 - val_loss: 5699.0327\n",
      "Epoch 8639/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6645.2813 - val_loss: 5698.5698\n",
      "Epoch 8640/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6644.8181 - val_loss: 5698.1084\n",
      "Epoch 8641/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6644.3560 - val_loss: 5697.6440\n",
      "Epoch 8642/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 6643.8930 - val_loss: 5697.1821\n",
      "Epoch 8643/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6643.4296 - val_loss: 5696.7212\n",
      "Epoch 8644/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6642.9675 - val_loss: 5696.2568\n",
      "Epoch 8645/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6642.5053 - val_loss: 5695.7930\n",
      "Epoch 8646/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6642.0423 - val_loss: 5695.3315\n",
      "Epoch 8647/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6641.5790 - val_loss: 5694.8696\n",
      "Epoch 8648/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6641.1163 - val_loss: 5694.4053\n",
      "Epoch 8649/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6640.6539 - val_loss: 5693.9419\n",
      "Epoch 8650/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6640.1909 - val_loss: 5693.4810\n",
      "Epoch 8651/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6639.7283 - val_loss: 5693.0186\n",
      "Epoch 8652/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6639.2659 - val_loss: 5692.5547\n",
      "Epoch 8653/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6638.8030 - val_loss: 5692.0928\n",
      "Epoch 8654/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6638.3405 - val_loss: 5691.6294\n",
      "Epoch 8655/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6637.8779 - val_loss: 5691.1665\n",
      "Epoch 8656/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6637.4151 - val_loss: 5690.7046\n",
      "Epoch 8657/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6636.9522 - val_loss: 5690.2422\n",
      "Epoch 8658/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6636.4901 - val_loss: 5689.7798\n",
      "Epoch 8659/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6636.0273 - val_loss: 5689.3154\n",
      "Epoch 8660/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6635.5640 - val_loss: 5688.8545\n",
      "Epoch 8661/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6635.1008 - val_loss: 5688.3906\n",
      "Epoch 8662/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6634.6389 - val_loss: 5687.9282\n",
      "Epoch 8663/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6634.1765 - val_loss: 5687.4653\n",
      "Epoch 8664/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6633.7127 - val_loss: 5687.0029\n",
      "Epoch 8665/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6633.2500 - val_loss: 5686.5386\n",
      "Epoch 8666/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 6632.7879 - val_loss: 5686.0771\n",
      "Epoch 8667/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6632.3244 - val_loss: 5685.6157\n",
      "Epoch 8668/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6631.8632 - val_loss: 5685.1523\n",
      "Epoch 8669/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6631.3996 - val_loss: 5684.6890\n",
      "Epoch 8670/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6630.9371 - val_loss: 5684.2261\n",
      "Epoch 8671/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6630.4747 - val_loss: 5683.7642\n",
      "Epoch 8672/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6630.0121 - val_loss: 5683.3003\n",
      "Epoch 8673/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6629.5493 - val_loss: 5682.8389\n",
      "Epoch 8674/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6629.0857 - val_loss: 5682.3774\n",
      "Epoch 8675/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6628.6234 - val_loss: 5681.9131\n",
      "Epoch 8676/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6628.1615 - val_loss: 5681.4492\n",
      "Epoch 8677/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6627.6982 - val_loss: 5680.9878\n",
      "Epoch 8678/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6627.2351 - val_loss: 5680.5259\n",
      "Epoch 8679/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6626.7725 - val_loss: 5680.0615\n",
      "Epoch 8680/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6626.3103 - val_loss: 5679.6006\n",
      "Epoch 8681/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6625.8471 - val_loss: 5679.1367\n",
      "Epoch 8682/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6625.3846 - val_loss: 5678.6743\n",
      "Epoch 8683/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6624.9223 - val_loss: 5678.2104\n",
      "Epoch 8684/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6624.4594 - val_loss: 5677.7490\n",
      "Epoch 8685/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6623.9967 - val_loss: 5677.2856\n",
      "Epoch 8686/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6623.5344 - val_loss: 5676.8228\n",
      "Epoch 8687/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6623.0713 - val_loss: 5676.3613\n",
      "Epoch 8688/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6622.6083 - val_loss: 5675.8984\n",
      "Epoch 8689/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6622.1464 - val_loss: 5675.4341\n",
      "Epoch 8690/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6621.6832 - val_loss: 5674.9727\n",
      "Epoch 8691/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6621.2203 - val_loss: 5674.5107\n",
      "Epoch 8692/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6620.7571 - val_loss: 5674.0469\n",
      "Epoch 8693/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6620.2952 - val_loss: 5673.5825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8694/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6619.8324 - val_loss: 5673.1216\n",
      "Epoch 8695/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6619.3690 - val_loss: 5672.6592\n",
      "Epoch 8696/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6618.9064 - val_loss: 5672.1948\n",
      "Epoch 8697/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6618.4440 - val_loss: 5671.7334\n",
      "Epoch 8698/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6617.9809 - val_loss: 5671.2720\n",
      "Epoch 8699/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6617.5192 - val_loss: 5670.8081\n",
      "Epoch 8700/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6617.0568 - val_loss: 5670.3452\n",
      "Epoch 8701/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6616.5932 - val_loss: 5669.8843\n",
      "Epoch 8702/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 6616.1310 - val_loss: 5669.4209\n",
      "Epoch 8703/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6615.6681 - val_loss: 5668.9565\n",
      "Epoch 8704/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6615.2053 - val_loss: 5668.4951\n",
      "Epoch 8705/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6614.7419 - val_loss: 5668.0327\n",
      "Epoch 8706/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6614.2797 - val_loss: 5667.5688\n",
      "Epoch 8707/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6613.8174 - val_loss: 5667.1055\n",
      "Epoch 8708/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6613.3539 - val_loss: 5666.6440\n",
      "Epoch 8709/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6612.8916 - val_loss: 5666.1816\n",
      "Epoch 8710/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6612.4289 - val_loss: 5665.7178\n",
      "Epoch 8711/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6611.9663 - val_loss: 5665.2568\n",
      "Epoch 8712/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6611.5037 - val_loss: 5664.7930\n",
      "Epoch 8713/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6611.0411 - val_loss: 5664.3296\n",
      "Epoch 8714/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6610.5784 - val_loss: 5663.8672\n",
      "Epoch 8715/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6610.1154 - val_loss: 5663.4053\n",
      "Epoch 8716/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6609.6531 - val_loss: 5662.9414\n",
      "Epoch 8717/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6609.1907 - val_loss: 5662.4790\n",
      "Epoch 8718/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6608.7270 - val_loss: 5662.0181\n",
      "Epoch 8719/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6608.2645 - val_loss: 5661.5547\n",
      "Epoch 8720/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6607.8028 - val_loss: 5661.0903\n",
      "Epoch 8721/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6607.3395 - val_loss: 5660.6289\n",
      "Epoch 8722/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6606.8761 - val_loss: 5660.1665\n",
      "Epoch 8723/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6606.4135 - val_loss: 5659.7026\n",
      "Epoch 8724/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6605.9517 - val_loss: 5659.2402\n",
      "Epoch 8725/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6605.4888 - val_loss: 5658.7778\n",
      "Epoch 8726/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6605.0253 - val_loss: 5658.3154\n",
      "Epoch 8727/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6604.5631 - val_loss: 5657.8511\n",
      "Epoch 8728/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6604.1004 - val_loss: 5657.3906\n",
      "Epoch 8729/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6603.6381 - val_loss: 5656.9263\n",
      "Epoch 8730/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6603.1757 - val_loss: 5656.4639\n",
      "Epoch 8731/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6602.7123 - val_loss: 5656.0029\n",
      "Epoch 8732/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6602.2494 - val_loss: 5655.5386\n",
      "Epoch 8733/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6601.7870 - val_loss: 5655.0767\n",
      "Epoch 8734/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6601.3245 - val_loss: 5654.6128\n",
      "Epoch 8735/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6600.8613 - val_loss: 5654.1514\n",
      "Epoch 8736/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6600.3982 - val_loss: 5653.6890\n",
      "Epoch 8737/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6599.9361 - val_loss: 5653.2251\n",
      "Epoch 8738/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6599.4734 - val_loss: 5652.7642\n",
      "Epoch 8739/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6599.0104 - val_loss: 5652.2998\n",
      "Epoch 8740/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6598.5473 - val_loss: 5651.8374\n",
      "Epoch 8741/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6598.0855 - val_loss: 5651.3740\n",
      "Epoch 8742/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6597.6220 - val_loss: 5650.9131\n",
      "Epoch 8743/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6597.1601 - val_loss: 5650.4492\n",
      "Epoch 8744/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6596.6971 - val_loss: 5649.9858\n",
      "Epoch 8745/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6596.2339 - val_loss: 5649.5249\n",
      "Epoch 8746/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6595.7719 - val_loss: 5649.0615\n",
      "Epoch 8747/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6595.3093 - val_loss: 5648.5977\n",
      "Epoch 8748/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6594.8468 - val_loss: 5648.1357\n",
      "Epoch 8749/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6594.3832 - val_loss: 5647.6743\n",
      "Epoch 8750/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6593.9209 - val_loss: 5647.2104\n",
      "Epoch 8751/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6593.4590 - val_loss: 5646.7466\n",
      "Epoch 8752/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6592.9953 - val_loss: 5646.2852\n",
      "Epoch 8753/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6592.5324 - val_loss: 5645.8232\n",
      "Epoch 8754/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6592.0697 - val_loss: 5645.3589\n",
      "Epoch 8755/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6591.6081 - val_loss: 5644.8975\n",
      "Epoch 8756/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6591.1444 - val_loss: 5644.4341\n",
      "Epoch 8757/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6590.6815 - val_loss: 5643.9717\n",
      "Epoch 8758/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6590.2200 - val_loss: 5643.5073\n",
      "Epoch 8759/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6589.7565 - val_loss: 5643.0464\n",
      "Epoch 8760/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6589.2945 - val_loss: 5642.5845\n",
      "Epoch 8761/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6588.8316 - val_loss: 5642.1201\n",
      "Epoch 8762/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6588.3686 - val_loss: 5641.6587\n",
      "Epoch 8763/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6587.9056 - val_loss: 5641.1948\n",
      "Epoch 8764/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6587.4433 - val_loss: 5640.7329\n",
      "Epoch 8765/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6586.9806 - val_loss: 5640.2690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8766/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 6586.5174 - val_loss: 5639.8081\n",
      "Epoch 8767/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6586.0543 - val_loss: 5639.3452\n",
      "Epoch 8768/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6585.5923 - val_loss: 5638.8813\n",
      "Epoch 8769/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6585.1297 - val_loss: 5638.4204\n",
      "Epoch 8770/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6584.6668 - val_loss: 5637.9565\n",
      "Epoch 8771/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6584.2039 - val_loss: 5637.4937\n",
      "Epoch 8772/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6583.7414 - val_loss: 5637.0308\n",
      "Epoch 8773/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6583.2789 - val_loss: 5636.5688\n",
      "Epoch 8774/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6582.8165 - val_loss: 5636.1050\n",
      "Epoch 8775/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6582.3538 - val_loss: 5635.6421\n",
      "Epoch 8776/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6581.8909 - val_loss: 5635.1812\n",
      "Epoch 8777/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6581.4281 - val_loss: 5634.7178\n",
      "Epoch 8778/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6580.9658 - val_loss: 5634.2539\n",
      "Epoch 8779/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6580.5028 - val_loss: 5633.7920\n",
      "Epoch 8780/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6580.0391 - val_loss: 5633.3296\n",
      "Epoch 8781/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 6579.04 - 0s 117us/step - loss: 6579.5771 - val_loss: 5632.8662\n",
      "Epoch 8782/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6579.1150 - val_loss: 5632.4028\n",
      "Epoch 8783/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6578.6512 - val_loss: 5631.9414\n",
      "Epoch 8784/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6578.1887 - val_loss: 5631.4790\n",
      "Epoch 8785/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6577.7261 - val_loss: 5631.0151\n",
      "Epoch 8786/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6577.2639 - val_loss: 5630.5537\n",
      "Epoch 8787/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6576.8019 - val_loss: 5630.0898\n",
      "Epoch 8788/10000\n",
      "750/750 [==============================] - 0s 208us/step - loss: 6576.3384 - val_loss: 5629.6274\n",
      "Epoch 8789/10000\n",
      "750/750 [==============================] - 0s 166us/step - loss: 6575.8758 - val_loss: 5629.1655\n",
      "Epoch 8790/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 6575.4126 - val_loss: 5628.7031\n",
      "Epoch 8791/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 6574.9507 - val_loss: 5628.2388\n",
      "Epoch 8792/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 6574.4878 - val_loss: 5627.7764\n",
      "Epoch 8793/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 6574.0247 - val_loss: 5627.3154\n",
      "Epoch 8794/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6573.5617 - val_loss: 5626.8511\n",
      "Epoch 8795/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 6573.0997 - val_loss: 5626.3892\n",
      "Epoch 8796/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 6572.6367 - val_loss: 5625.9253\n",
      "Epoch 8797/10000\n",
      "750/750 [==============================] - 0s 167us/step - loss: 6572.1732 - val_loss: 5625.4639\n",
      "Epoch 8798/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 6571.7109 - val_loss: 5625.0000\n",
      "Epoch 8799/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6571.2489 - val_loss: 5624.5381\n",
      "Epoch 8800/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 6570.7858 - val_loss: 5624.0767\n",
      "Epoch 8801/10000\n",
      "750/750 [==============================] - 0s 165us/step - loss: 6570.3231 - val_loss: 5623.6128\n",
      "Epoch 8802/10000\n",
      "750/750 [==============================] - 0s 155us/step - loss: 6569.8607 - val_loss: 5623.1499\n",
      "Epoch 8803/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 6569.3978 - val_loss: 5622.6875\n",
      "Epoch 8804/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6568.9351 - val_loss: 5622.2251\n",
      "Epoch 8805/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6568.4731 - val_loss: 5621.7607\n",
      "Epoch 8806/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6568.0100 - val_loss: 5621.2998\n",
      "Epoch 8807/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6567.5471 - val_loss: 5620.8374\n",
      "Epoch 8808/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6567.0842 - val_loss: 5620.3740\n",
      "Epoch 8809/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6566.6219 - val_loss: 5619.9102\n",
      "Epoch 8810/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6566.1585 - val_loss: 5619.4487\n",
      "Epoch 8811/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6565.6954 - val_loss: 5618.9863\n",
      "Epoch 8812/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6565.2336 - val_loss: 5618.5225\n",
      "Epoch 8813/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6564.7711 - val_loss: 5618.0605\n",
      "Epoch 8814/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6564.3075 - val_loss: 5617.5977\n",
      "Epoch 8815/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6563.8454 - val_loss: 5617.1348\n",
      "Epoch 8816/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6563.3827 - val_loss: 5616.6714\n",
      "Epoch 8817/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6562.9200 - val_loss: 5616.2100\n",
      "Epoch 8818/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6562.4580 - val_loss: 5615.7466\n",
      "Epoch 8819/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6561.9951 - val_loss: 5615.2837\n",
      "Epoch 8820/10000\n",
      "750/750 [==============================] - 0s 111us/step - loss: 6561.5317 - val_loss: 5614.8218\n",
      "Epoch 8821/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 6561.0689 - val_loss: 5614.3584\n",
      "Epoch 8822/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 6560.6067 - val_loss: 5613.8940\n",
      "Epoch 8823/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 6560.1438 - val_loss: 5613.4336\n",
      "Epoch 8824/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6559.6805 - val_loss: 5612.9717\n",
      "Epoch 8825/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6559.2179 - val_loss: 5612.5073\n",
      "Epoch 8826/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6558.7559 - val_loss: 5612.0454\n",
      "Epoch 8827/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6558.2929 - val_loss: 5611.5815\n",
      "Epoch 8828/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6557.8297 - val_loss: 5611.1201\n",
      "Epoch 8829/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6557.3670 - val_loss: 5610.6558\n",
      "Epoch 8830/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6556.9052 - val_loss: 5610.1943\n",
      "Epoch 8831/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6556.4424 - val_loss: 5609.7329\n",
      "Epoch 8832/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6555.9794 - val_loss: 5609.2686\n",
      "Epoch 8833/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6555.5173 - val_loss: 5608.8047\n",
      "Epoch 8834/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6555.0537 - val_loss: 5608.3433\n",
      "Epoch 8835/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6554.5913 - val_loss: 5607.8813\n",
      "Epoch 8836/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6554.1293 - val_loss: 5607.4170\n",
      "Epoch 8837/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6553.6660 - val_loss: 5606.9561\n",
      "Epoch 8838/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6553.2028 - val_loss: 5606.4922\n",
      "Epoch 8839/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6552.7406 - val_loss: 5606.0298\n",
      "Epoch 8840/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6552.2782 - val_loss: 5605.5664\n",
      "Epoch 8841/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6551.8147 - val_loss: 5605.1055\n",
      "Epoch 8842/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6551.3515 - val_loss: 5604.6421\n",
      "Epoch 8843/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6550.8898 - val_loss: 5604.1782\n",
      "Epoch 8844/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6550.4271 - val_loss: 5603.7173\n",
      "Epoch 8845/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6549.9636 - val_loss: 5603.2539\n",
      "Epoch 8846/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6549.5018 - val_loss: 5602.7910\n",
      "Epoch 8847/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6549.0388 - val_loss: 5602.3281\n",
      "Epoch 8848/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6548.5763 - val_loss: 5601.8662\n",
      "Epoch 8849/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6548.1139 - val_loss: 5601.4023\n",
      "Epoch 8850/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6547.6508 - val_loss: 5600.9399\n",
      "Epoch 8851/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6547.1881 - val_loss: 5600.4785\n",
      "Epoch 8852/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6546.7252 - val_loss: 5600.0151\n",
      "Epoch 8853/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6546.2633 - val_loss: 5599.5503\n",
      "Epoch 8854/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6545.8002 - val_loss: 5599.0898\n",
      "Epoch 8855/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6545.3370 - val_loss: 5598.6274\n",
      "Epoch 8856/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6544.8743 - val_loss: 5598.1631\n",
      "Epoch 8857/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6544.4122 - val_loss: 5597.7017\n",
      "Epoch 8858/10000\n",
      "750/750 [==============================] - 0s 168us/step - loss: 6543.9490 - val_loss: 5597.2388\n",
      "Epoch 8859/10000\n",
      "750/750 [==============================] - 0s 173us/step - loss: 6543.4857 - val_loss: 5596.7764\n",
      "Epoch 8860/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 6543.0234 - val_loss: 5596.3125\n",
      "Epoch 8861/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 6542.5610 - val_loss: 5595.8511\n",
      "Epoch 8862/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6542.0989 - val_loss: 5595.3872\n",
      "Epoch 8863/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 6541.6358 - val_loss: 5594.9248\n",
      "Epoch 8864/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 6541.1729 - val_loss: 5594.4629\n",
      "Epoch 8865/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6540.7100 - val_loss: 5594.0000\n",
      "Epoch 8866/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6540.2479 - val_loss: 5593.5371\n",
      "Epoch 8867/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 6539.7855 - val_loss: 5593.0732\n",
      "Epoch 8868/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 6539.3223 - val_loss: 5592.6123\n",
      "Epoch 8869/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 6538.8588 - val_loss: 5592.1484\n",
      "Epoch 8870/10000\n",
      "750/750 [==============================] - 0s 158us/step - loss: 6538.3968 - val_loss: 5591.6860\n",
      "Epoch 8871/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6537.9340 - val_loss: 5591.2227\n",
      "Epoch 8872/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6537.4709 - val_loss: 5590.7607\n",
      "Epoch 8873/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6537.0080 - val_loss: 5590.2969\n",
      "Epoch 8874/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6536.5457 - val_loss: 5589.8350\n",
      "Epoch 8875/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6536.0828 - val_loss: 5589.3735\n",
      "Epoch 8876/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6535.6207 - val_loss: 5588.9102\n",
      "Epoch 8877/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6535.1581 - val_loss: 5588.4473\n",
      "Epoch 8878/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6534.6950 - val_loss: 5587.9844\n",
      "Epoch 8879/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6534.2326 - val_loss: 5587.5225\n",
      "Epoch 8880/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6533.7701 - val_loss: 5587.0586\n",
      "Epoch 8881/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6533.3072 - val_loss: 5586.5967\n",
      "Epoch 8882/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6532.8444 - val_loss: 5586.1348\n",
      "Epoch 8883/10000\n",
      "750/750 [==============================] - 0s 152us/step - loss: 6532.3813 - val_loss: 5585.6709\n",
      "Epoch 8884/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6531.9192 - val_loss: 5585.2065\n",
      "Epoch 8885/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6531.4563 - val_loss: 5584.7461\n",
      "Epoch 8886/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6530.9924 - val_loss: 5584.2837\n",
      "Epoch 8887/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6530.5306 - val_loss: 5583.8198\n",
      "Epoch 8888/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6530.0684 - val_loss: 5583.3579\n",
      "Epoch 8889/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6529.6047 - val_loss: 5582.8940\n",
      "Epoch 8890/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6529.1428 - val_loss: 5582.4321\n",
      "Epoch 8891/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6528.6798 - val_loss: 5581.9678\n",
      "Epoch 8892/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6528.2172 - val_loss: 5581.5073\n",
      "Epoch 8893/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6527.7553 - val_loss: 5581.0454\n",
      "Epoch 8894/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6527.2925 - val_loss: 5580.5811\n",
      "Epoch 8895/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6526.8292 - val_loss: 5580.1191\n",
      "Epoch 8896/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6526.3662 - val_loss: 5579.6553\n",
      "Epoch 8897/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6525.9041 - val_loss: 5579.1919\n",
      "Epoch 8898/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6525.4415 - val_loss: 5578.7305\n",
      "Epoch 8899/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6524.9777 - val_loss: 5578.2686\n",
      "Epoch 8900/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6524.5151 - val_loss: 5577.8047\n",
      "Epoch 8901/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6524.0530 - val_loss: 5577.3423\n",
      "Epoch 8902/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6523.5899 - val_loss: 5576.8794\n",
      "Epoch 8903/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6523.1272 - val_loss: 5576.4175\n",
      "Epoch 8904/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6522.6642 - val_loss: 5575.9531\n",
      "Epoch 8905/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6522.2023 - val_loss: 5575.4917\n",
      "Epoch 8906/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6521.7398 - val_loss: 5575.0298\n",
      "Epoch 8907/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6521.2771 - val_loss: 5574.5659\n",
      "Epoch 8908/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6520.8145 - val_loss: 5574.1030\n",
      "Epoch 8909/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 121us/step - loss: 6520.3510 - val_loss: 5573.6406\n",
      "Epoch 8910/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6519.8886 - val_loss: 5573.1787\n",
      "Epoch 8911/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6519.4269 - val_loss: 5572.7148\n",
      "Epoch 8912/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6518.9632 - val_loss: 5572.2529\n",
      "Epoch 8913/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6518.4997 - val_loss: 5571.7905\n",
      "Epoch 8914/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6518.0379 - val_loss: 5571.3271\n",
      "Epoch 8915/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6517.5752 - val_loss: 5570.8638\n",
      "Epoch 8916/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6517.1119 - val_loss: 5570.4023\n",
      "Epoch 8917/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6516.6490 - val_loss: 5569.9399\n",
      "Epoch 8918/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6516.1869 - val_loss: 5569.4756\n",
      "Epoch 8919/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6515.7240 - val_loss: 5569.0146\n",
      "Epoch 8920/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6515.2610 - val_loss: 5568.5503\n",
      "Epoch 8921/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6514.7993 - val_loss: 5568.0884\n",
      "Epoch 8922/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6514.3365 - val_loss: 5567.6250\n",
      "Epoch 8923/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6513.8734 - val_loss: 5567.1636\n",
      "Epoch 8924/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6513.4113 - val_loss: 5566.6997\n",
      "Epoch 8925/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6512.9484 - val_loss: 5566.2373\n",
      "Epoch 8926/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6512.4852 - val_loss: 5565.7764\n",
      "Epoch 8927/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6512.0223 - val_loss: 5565.3125\n",
      "Epoch 8928/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6511.5603 - val_loss: 5564.8481\n",
      "Epoch 8929/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6511.0974 - val_loss: 5564.3867\n",
      "Epoch 8930/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6510.6340 - val_loss: 5563.9243\n",
      "Epoch 8931/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6510.1712 - val_loss: 5563.4604\n",
      "Epoch 8932/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6509.7095 - val_loss: 5562.9985\n",
      "Epoch 8933/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6509.2461 - val_loss: 5562.5376\n",
      "Epoch 8934/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6508.7836 - val_loss: 5562.0732\n",
      "Epoch 8935/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6508.3206 - val_loss: 5561.6094\n",
      "Epoch 8936/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6507.8584 - val_loss: 5561.1484\n",
      "Epoch 8937/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6507.3960 - val_loss: 5560.6855\n",
      "Epoch 8938/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 6506.9332 - val_loss: 5560.2217\n",
      "Epoch 8939/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6506.4705 - val_loss: 5559.7603\n",
      "Epoch 8940/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6506.0071 - val_loss: 5559.2983\n",
      "Epoch 8941/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6505.5450 - val_loss: 5558.8345\n",
      "Epoch 8942/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6505.0828 - val_loss: 5558.3706\n",
      "Epoch 8943/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6504.6193 - val_loss: 5557.9097\n",
      "Epoch 8944/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6504.1562 - val_loss: 5557.4473\n",
      "Epoch 8945/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6503.6940 - val_loss: 5556.9834\n",
      "Epoch 8946/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6503.2314 - val_loss: 5556.5200\n",
      "Epoch 8947/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6502.7683 - val_loss: 5556.0586\n",
      "Epoch 8948/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6502.3053 - val_loss: 5555.5952\n",
      "Epoch 8949/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6501.8432 - val_loss: 5555.1318\n",
      "Epoch 8950/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6501.3807 - val_loss: 5554.6709\n",
      "Epoch 8951/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6500.9183 - val_loss: 5554.2065\n",
      "Epoch 8952/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6500.4556 - val_loss: 5553.7441\n",
      "Epoch 8953/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6499.9919 - val_loss: 5553.2827\n",
      "Epoch 8954/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6499.5300 - val_loss: 5552.8193\n",
      "Epoch 8955/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6499.0679 - val_loss: 5552.3555\n",
      "Epoch 8956/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6498.6042 - val_loss: 5551.8936\n",
      "Epoch 8957/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6498.1413 - val_loss: 5551.4326\n",
      "Epoch 8958/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6497.6786 - val_loss: 5550.9683\n",
      "Epoch 8959/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6497.2167 - val_loss: 5550.5039\n",
      "Epoch 8960/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6496.7535 - val_loss: 5550.0430\n",
      "Epoch 8961/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6496.2903 - val_loss: 5549.5811\n",
      "Epoch 8962/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6495.8278 - val_loss: 5549.1167\n",
      "Epoch 8963/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6495.3654 - val_loss: 5548.6553\n",
      "Epoch 8964/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6494.9023 - val_loss: 5548.1919\n",
      "Epoch 8965/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6494.4405 - val_loss: 5547.7295\n",
      "Epoch 8966/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6493.9766 - val_loss: 5547.2651\n",
      "Epoch 8967/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 6493.5142 - val_loss: 5546.8047\n",
      "Epoch 8968/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6493.0524 - val_loss: 5546.3423\n",
      "Epoch 8969/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6492.5895 - val_loss: 5545.8779\n",
      "Epoch 8970/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6492.1262 - val_loss: 5545.4165\n",
      "Epoch 8971/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6491.6635 - val_loss: 5544.9531\n",
      "Epoch 8972/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6491.2013 - val_loss: 5544.4907\n",
      "Epoch 8973/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6490.7389 - val_loss: 5544.0273\n",
      "Epoch 8974/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6490.2751 - val_loss: 5543.5659\n",
      "Epoch 8975/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6489.8125 - val_loss: 5543.1030\n",
      "Epoch 8976/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6489.3504 - val_loss: 5542.6396\n",
      "Epoch 8977/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6488.8874 - val_loss: 5542.1777\n",
      "Epoch 8978/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6488.4245 - val_loss: 5541.7148\n",
      "Epoch 8979/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6487.9615 - val_loss: 5541.2524\n",
      "Epoch 8980/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6487.5000 - val_loss: 5540.7886\n",
      "Epoch 8981/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 120us/step - loss: 6487.0368 - val_loss: 5540.3271\n",
      "Epoch 8982/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6486.5740 - val_loss: 5539.8628\n",
      "Epoch 8983/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6486.1118 - val_loss: 5539.4004\n",
      "Epoch 8984/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6485.6483 - val_loss: 5538.9375\n",
      "Epoch 8985/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6485.1859 - val_loss: 5538.4756\n",
      "Epoch 8986/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6484.7240 - val_loss: 5538.0117\n",
      "Epoch 8987/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6484.2610 - val_loss: 5537.5498\n",
      "Epoch 8988/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6483.7974 - val_loss: 5537.0879\n",
      "Epoch 8989/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6483.3350 - val_loss: 5536.6240\n",
      "Epoch 8990/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6482.8731 - val_loss: 5536.1606\n",
      "Epoch 8991/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6482.4096 - val_loss: 5535.6997\n",
      "Epoch 8992/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6481.9465 - val_loss: 5535.2368\n",
      "Epoch 8993/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6481.4841 - val_loss: 5534.7725\n",
      "Epoch 8994/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6481.0219 - val_loss: 5534.3115\n",
      "Epoch 8995/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6480.5581 - val_loss: 5533.8481\n",
      "Epoch 8996/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6480.0963 - val_loss: 5533.3857\n",
      "Epoch 8997/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6479.6337 - val_loss: 5532.9233\n",
      "Epoch 8998/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6479.1707 - val_loss: 5532.4604\n",
      "Epoch 8999/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 6478.7086 - val_loss: 5531.9980\n",
      "Epoch 9000/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6478.2460 - val_loss: 5531.5342\n",
      "Epoch 9001/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6477.7829 - val_loss: 5531.0732\n",
      "Epoch 9002/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6477.3196 - val_loss: 5530.6094\n",
      "Epoch 9003/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6476.8576 - val_loss: 5530.1470\n",
      "Epoch 9004/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6476.3946 - val_loss: 5529.6841\n",
      "Epoch 9005/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6475.9313 - val_loss: 5529.2217\n",
      "Epoch 9006/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6475.4687 - val_loss: 5528.7573\n",
      "Epoch 9007/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6475.0066 - val_loss: 5528.2959\n",
      "Epoch 9008/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6474.5433 - val_loss: 5527.8350\n",
      "Epoch 9009/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6474.0809 - val_loss: 5527.3706\n",
      "Epoch 9010/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6473.6183 - val_loss: 5526.9067\n",
      "Epoch 9011/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6473.1556 - val_loss: 5526.4453\n",
      "Epoch 9012/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6472.6932 - val_loss: 5525.9829\n",
      "Epoch 9013/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6472.2308 - val_loss: 5525.5190\n",
      "Epoch 9014/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6471.7681 - val_loss: 5525.0576\n",
      "Epoch 9015/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6471.3044 - val_loss: 5524.5962\n",
      "Epoch 9016/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6470.8420 - val_loss: 5524.1318\n",
      "Epoch 9017/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6470.3798 - val_loss: 5523.6680\n",
      "Epoch 9018/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6469.9165 - val_loss: 5523.2065\n",
      "Epoch 9019/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6469.4533 - val_loss: 5522.7441\n",
      "Epoch 9020/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6468.9910 - val_loss: 5522.2803\n",
      "Epoch 9021/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6468.5293 - val_loss: 5521.8184\n",
      "Epoch 9022/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6468.0659 - val_loss: 5521.3560\n",
      "Epoch 9023/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6467.6026 - val_loss: 5520.8926\n",
      "Epoch 9024/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6467.1403 - val_loss: 5520.4292\n",
      "Epoch 9025/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6466.6779 - val_loss: 5519.9678\n",
      "Epoch 9026/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6466.2153 - val_loss: 5519.5044\n",
      "Epoch 9027/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6465.7528 - val_loss: 5519.0415\n",
      "Epoch 9028/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6465.2893 - val_loss: 5518.5801\n",
      "Epoch 9029/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6464.8270 - val_loss: 5518.1167\n",
      "Epoch 9030/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6464.3650 - val_loss: 5517.6523\n",
      "Epoch 9031/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6463.9018 - val_loss: 5517.1914\n",
      "Epoch 9032/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6463.4386 - val_loss: 5516.7295\n",
      "Epoch 9033/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6462.9758 - val_loss: 5516.2656\n",
      "Epoch 9034/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6462.5139 - val_loss: 5515.8013\n",
      "Epoch 9035/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6462.0506 - val_loss: 5515.3403\n",
      "Epoch 9036/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6461.5874 - val_loss: 5514.8779\n",
      "Epoch 9037/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6461.1251 - val_loss: 5514.4141\n",
      "Epoch 9038/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6460.6628 - val_loss: 5513.9521\n",
      "Epoch 9039/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6460.1993 - val_loss: 5513.4902\n",
      "Epoch 9040/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6459.7375 - val_loss: 5513.0269\n",
      "Epoch 9041/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6459.2747 - val_loss: 5512.5625\n",
      "Epoch 9042/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6458.8116 - val_loss: 5512.1011\n",
      "Epoch 9043/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6458.3494 - val_loss: 5511.6396\n",
      "Epoch 9044/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6457.8868 - val_loss: 5511.1748\n",
      "Epoch 9045/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6457.4238 - val_loss: 5510.7139\n",
      "Epoch 9046/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6456.9606 - val_loss: 5510.2515\n",
      "Epoch 9047/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6456.4984 - val_loss: 5509.7881\n",
      "Epoch 9048/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6456.0358 - val_loss: 5509.3247\n",
      "Epoch 9049/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6455.5727 - val_loss: 5508.8628\n",
      "Epoch 9050/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6455.1094 - val_loss: 5508.4004\n",
      "Epoch 9051/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6454.6477 - val_loss: 5507.9365\n",
      "Epoch 9052/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6454.1850 - val_loss: 5507.4746\n",
      "Epoch 9053/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 118us/step - loss: 6453.7222 - val_loss: 5507.0117\n",
      "Epoch 9054/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6453.2593 - val_loss: 5506.5493\n",
      "Epoch 9055/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 6452.7972 - val_loss: 5506.0859\n",
      "Epoch 9056/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6452.3341 - val_loss: 5505.6240\n",
      "Epoch 9057/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6451.8716 - val_loss: 5505.1606\n",
      "Epoch 9058/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6451.4094 - val_loss: 5504.6968\n",
      "Epoch 9059/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6450.9453 - val_loss: 5504.2358\n",
      "Epoch 9060/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 6450.4831 - val_loss: 5503.7734\n",
      "Epoch 9061/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6450.0213 - val_loss: 5503.3091\n",
      "Epoch 9062/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 6449.5581 - val_loss: 5502.8477\n",
      "Epoch 9063/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6449.0948 - val_loss: 5502.3853\n",
      "Epoch 9064/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6448.6319 - val_loss: 5501.9219\n",
      "Epoch 9065/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6448.1701 - val_loss: 5501.4595\n",
      "Epoch 9066/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6447.7069 - val_loss: 5500.9976\n",
      "Epoch 9067/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6447.2440 - val_loss: 5500.5342\n",
      "Epoch 9068/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6446.7818 - val_loss: 5500.0698\n",
      "Epoch 9069/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6446.3189 - val_loss: 5499.6084\n",
      "Epoch 9070/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6445.8562 - val_loss: 5499.1470\n",
      "Epoch 9071/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6445.3942 - val_loss: 5498.6826\n",
      "Epoch 9072/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6444.9311 - val_loss: 5498.2212\n",
      "Epoch 9073/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6444.4680 - val_loss: 5497.7573\n",
      "Epoch 9074/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6444.0056 - val_loss: 5497.2959\n",
      "Epoch 9075/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6443.5435 - val_loss: 5496.8311\n",
      "Epoch 9076/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6443.0802 - val_loss: 5496.3701\n",
      "Epoch 9077/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6442.6167 - val_loss: 5495.9077\n",
      "Epoch 9078/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6442.1548 - val_loss: 5495.4438\n",
      "Epoch 9079/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6441.6924 - val_loss: 5494.9829\n",
      "Epoch 9080/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6441.2286 - val_loss: 5494.5186\n",
      "Epoch 9081/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 6438.64 - 0s 116us/step - loss: 6440.7658 - val_loss: 5494.0552\n",
      "Epoch 9082/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6440.3037 - val_loss: 5493.5928\n",
      "Epoch 9083/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6439.8411 - val_loss: 5493.1318\n",
      "Epoch 9084/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6439.3786 - val_loss: 5492.6680\n",
      "Epoch 9085/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6438.9152 - val_loss: 5492.2046\n",
      "Epoch 9086/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6438.4529 - val_loss: 5491.7427\n",
      "Epoch 9087/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6437.9905 - val_loss: 5491.2803\n",
      "Epoch 9088/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6437.5279 - val_loss: 5490.8164\n",
      "Epoch 9089/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6437.0653 - val_loss: 5490.3545\n",
      "Epoch 9090/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6436.6021 - val_loss: 5489.8921\n",
      "Epoch 9091/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6436.1392 - val_loss: 5489.4292\n",
      "Epoch 9092/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6435.6776 - val_loss: 5488.9648\n",
      "Epoch 9093/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6435.2139 - val_loss: 5488.5044\n",
      "Epoch 9094/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6434.7508 - val_loss: 5488.0415\n",
      "Epoch 9095/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6434.2885 - val_loss: 5487.5776\n",
      "Epoch 9096/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6433.8263 - val_loss: 5487.1157\n",
      "Epoch 9097/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6433.3631 - val_loss: 5486.6528\n",
      "Epoch 9098/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6432.8997 - val_loss: 5486.1899\n",
      "Epoch 9099/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6432.4382 - val_loss: 5485.7261\n",
      "Epoch 9100/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6431.9753 - val_loss: 5485.2651\n",
      "Epoch 9101/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6431.5133 - val_loss: 5484.8013\n",
      "Epoch 9102/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6431.0500 - val_loss: 5484.3389\n",
      "Epoch 9103/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 6430.5873 - val_loss: 5483.8774\n",
      "Epoch 9104/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6430.1244 - val_loss: 5483.4136\n",
      "Epoch 9105/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6429.6619 - val_loss: 5482.9497\n",
      "Epoch 9106/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6429.1991 - val_loss: 5482.4878\n",
      "Epoch 9107/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6428.7364 - val_loss: 5482.0264\n",
      "Epoch 9108/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6428.2729 - val_loss: 5481.5635\n",
      "Epoch 9109/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6427.8111 - val_loss: 5481.0996\n",
      "Epoch 9110/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6427.3482 - val_loss: 5480.6392\n",
      "Epoch 9111/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6426.8851 - val_loss: 5480.1753\n",
      "Epoch 9112/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6426.4221 - val_loss: 5479.7114\n",
      "Epoch 9113/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6425.9599 - val_loss: 5479.2490\n",
      "Epoch 9114/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6425.4972 - val_loss: 5478.7881\n",
      "Epoch 9115/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6425.0354 - val_loss: 5478.3232\n",
      "Epoch 9116/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6424.5721 - val_loss: 5477.8608\n",
      "Epoch 9117/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6424.1087 - val_loss: 5477.3984\n",
      "Epoch 9118/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6423.6469 - val_loss: 5476.9365\n",
      "Epoch 9119/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6423.1844 - val_loss: 5476.4727\n",
      "Epoch 9120/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6422.7213 - val_loss: 5476.0107\n",
      "Epoch 9121/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6422.2581 - val_loss: 5475.5483\n",
      "Epoch 9122/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6421.7958 - val_loss: 5475.0850\n",
      "Epoch 9123/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6421.3330 - val_loss: 5474.6216\n",
      "Epoch 9124/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6420.8700 - val_loss: 5474.1606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9125/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6420.4073 - val_loss: 5473.6982\n",
      "Epoch 9126/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6419.9447 - val_loss: 5473.2334\n",
      "Epoch 9127/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6419.4823 - val_loss: 5472.7725\n",
      "Epoch 9128/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6419.0198 - val_loss: 5472.3091\n",
      "Epoch 9129/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6418.5569 - val_loss: 5471.8467\n",
      "Epoch 9130/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6418.0945 - val_loss: 5471.3843\n",
      "Epoch 9131/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6417.6314 - val_loss: 5470.9214\n",
      "Epoch 9132/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6417.1694 - val_loss: 5470.4575\n",
      "Epoch 9133/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6416.7071 - val_loss: 5469.9951\n",
      "Epoch 9134/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6416.2429 - val_loss: 5469.5327\n",
      "Epoch 9135/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6415.7805 - val_loss: 5469.0698\n",
      "Epoch 9136/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6415.3185 - val_loss: 5468.6079\n",
      "Epoch 9137/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6414.8553 - val_loss: 5468.1440\n",
      "Epoch 9138/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6414.3924 - val_loss: 5467.6826\n",
      "Epoch 9139/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6413.9293 - val_loss: 5467.2192\n",
      "Epoch 9140/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6413.4673 - val_loss: 5466.7559\n",
      "Epoch 9141/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6413.0043 - val_loss: 5466.2954\n",
      "Epoch 9142/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6412.5416 - val_loss: 5465.8315\n",
      "Epoch 9143/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6412.0790 - val_loss: 5465.3677\n",
      "Epoch 9144/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6411.6160 - val_loss: 5464.9053\n",
      "Epoch 9145/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6411.1537 - val_loss: 5464.4443\n",
      "Epoch 9146/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6410.6914 - val_loss: 5463.9795\n",
      "Epoch 9147/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6410.2288 - val_loss: 5463.5181\n",
      "Epoch 9148/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 6409.7652 - val_loss: 5463.0562\n",
      "Epoch 9149/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6409.3029 - val_loss: 5462.5928\n",
      "Epoch 9150/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 6408.8405 - val_loss: 5462.1289\n",
      "Epoch 9151/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6408.3778 - val_loss: 5461.6670\n",
      "Epoch 9152/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6407.9139 - val_loss: 5461.2046\n",
      "Epoch 9153/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6407.4519 - val_loss: 5460.7412\n",
      "Epoch 9154/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6406.9900 - val_loss: 5460.2778\n",
      "Epoch 9155/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6406.5261 - val_loss: 5459.8154\n",
      "Epoch 9156/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6406.0636 - val_loss: 5459.3535\n",
      "Epoch 9157/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6405.6013 - val_loss: 5458.8896\n",
      "Epoch 9158/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6405.1381 - val_loss: 5458.4287\n",
      "Epoch 9159/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6404.6762 - val_loss: 5457.9653\n",
      "Epoch 9160/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6404.2132 - val_loss: 5457.5015\n",
      "Epoch 9161/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6403.7500 - val_loss: 5457.0410\n",
      "Epoch 9162/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6403.2876 - val_loss: 5456.5771\n",
      "Epoch 9163/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6402.8251 - val_loss: 5456.1138\n",
      "Epoch 9164/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6402.3625 - val_loss: 5455.6523\n",
      "Epoch 9165/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6401.8994 - val_loss: 5455.1899\n",
      "Epoch 9166/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6401.4366 - val_loss: 5454.7261\n",
      "Epoch 9167/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6400.9747 - val_loss: 5454.2622\n",
      "Epoch 9168/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6400.5114 - val_loss: 5453.8013\n",
      "Epoch 9169/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6400.0480 - val_loss: 5453.3389\n",
      "Epoch 9170/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6399.5855 - val_loss: 5452.8750\n",
      "Epoch 9171/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6399.1237 - val_loss: 5452.4131\n",
      "Epoch 9172/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6398.6608 - val_loss: 5451.9497\n",
      "Epoch 9173/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6398.1982 - val_loss: 5451.4873\n",
      "Epoch 9174/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6397.7353 - val_loss: 5451.0239\n",
      "Epoch 9175/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 6397.2723 - val_loss: 5450.5625\n",
      "Epoch 9176/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 6396.8103 - val_loss: 5450.1001\n",
      "Epoch 9177/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 6396.3474 - val_loss: 5449.6357\n",
      "Epoch 9178/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 6395.8847 - val_loss: 5449.1748\n",
      "Epoch 9179/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 6395.4214 - val_loss: 5448.7114\n",
      "Epoch 9180/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 6394.9590 - val_loss: 5448.2480\n",
      "Epoch 9181/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6394.4967 - val_loss: 5447.7856\n",
      "Epoch 9182/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6394.0335 - val_loss: 5447.3242\n",
      "Epoch 9183/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6393.5702 - val_loss: 5446.8599\n",
      "Epoch 9184/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6393.1082 - val_loss: 5446.3975\n",
      "Epoch 9185/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6392.6456 - val_loss: 5445.9355\n",
      "Epoch 9186/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6392.1826 - val_loss: 5445.4722\n",
      "Epoch 9187/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6391.7199 - val_loss: 5445.0098\n",
      "Epoch 9188/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6391.2572 - val_loss: 5444.5464\n",
      "Epoch 9189/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 6390.7949 - val_loss: 5444.0850\n",
      "Epoch 9190/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6390.3323 - val_loss: 5443.6211\n",
      "Epoch 9191/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6389.8694 - val_loss: 5443.1577\n",
      "Epoch 9192/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6389.4065 - val_loss: 5442.6968\n",
      "Epoch 9193/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6388.9440 - val_loss: 5442.2344\n",
      "Epoch 9194/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6388.4819 - val_loss: 5441.7690\n",
      "Epoch 9195/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6388.0185 - val_loss: 5441.3086\n",
      "Epoch 9196/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6387.5558 - val_loss: 5440.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9197/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6387.0930 - val_loss: 5440.3818\n",
      "Epoch 9198/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6386.6301 - val_loss: 5439.9204\n",
      "Epoch 9199/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 6386.1677 - val_loss: 5439.4575\n",
      "Epoch 9200/10000\n",
      "750/750 [==============================] - 0s 91us/step - loss: 6385.7048 - val_loss: 5438.9951\n",
      "Epoch 9201/10000\n",
      "750/750 [==============================] - 0s 95us/step - loss: 6385.2416 - val_loss: 5438.5308\n",
      "Epoch 9202/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 6384.7799 - val_loss: 5438.0693\n",
      "Epoch 9203/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 6384.3169 - val_loss: 5437.6060\n",
      "Epoch 9204/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 6383.8548 - val_loss: 5437.1436\n",
      "Epoch 9205/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 6383.3916 - val_loss: 5436.6812\n",
      "Epoch 9206/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6382.9286 - val_loss: 5436.2178\n",
      "Epoch 9207/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6382.4665 - val_loss: 5435.7568\n",
      "Epoch 9208/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6382.0042 - val_loss: 5435.2920\n",
      "Epoch 9209/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6381.5402 - val_loss: 5434.8306\n",
      "Epoch 9210/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6381.0776 - val_loss: 5434.3687\n",
      "Epoch 9211/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6380.6157 - val_loss: 5433.9028\n",
      "Epoch 9212/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6380.1526 - val_loss: 5433.4419\n",
      "Epoch 9213/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6379.6897 - val_loss: 5432.9795\n",
      "Epoch 9214/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6379.2267 - val_loss: 5432.5161\n",
      "Epoch 9215/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6378.7644 - val_loss: 5432.0537\n",
      "Epoch 9216/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6378.3021 - val_loss: 5431.5923\n",
      "Epoch 9217/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6377.8387 - val_loss: 5431.1284\n",
      "Epoch 9218/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6377.3763 - val_loss: 5430.6655\n",
      "Epoch 9219/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6376.9134 - val_loss: 5430.2026\n",
      "Epoch 9220/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6376.4513 - val_loss: 5429.7412\n",
      "Epoch 9221/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6375.9886 - val_loss: 5429.2778\n",
      "Epoch 9222/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6375.5255 - val_loss: 5428.8149\n",
      "Epoch 9223/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6375.0624 - val_loss: 5428.3530\n",
      "Epoch 9224/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6374.5999 - val_loss: 5427.8901\n",
      "Epoch 9225/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6374.1384 - val_loss: 5427.4253\n",
      "Epoch 9226/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6373.6743 - val_loss: 5426.9648\n",
      "Epoch 9227/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6373.2117 - val_loss: 5426.5024\n",
      "Epoch 9228/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6372.7493 - val_loss: 5426.0381\n",
      "Epoch 9229/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6372.2870 - val_loss: 5425.5767\n",
      "Epoch 9230/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6371.8234 - val_loss: 5425.1128\n",
      "Epoch 9231/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6371.3614 - val_loss: 5424.6504\n",
      "Epoch 9232/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6370.8986 - val_loss: 5424.1885\n",
      "Epoch 9233/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6370.4355 - val_loss: 5423.7256\n",
      "Epoch 9234/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6369.9737 - val_loss: 5423.2622\n",
      "Epoch 9235/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6369.5111 - val_loss: 5422.7998\n",
      "Epoch 9236/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6369.0477 - val_loss: 5422.3364\n",
      "Epoch 9237/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6368.5844 - val_loss: 5421.8740\n",
      "Epoch 9238/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6368.1222 - val_loss: 5421.4126\n",
      "Epoch 9239/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6367.6598 - val_loss: 5420.9492\n",
      "Epoch 9240/10000\n",
      "750/750 [==============================] - 0s 148us/step - loss: 6367.1962 - val_loss: 5420.4868\n",
      "Epoch 9241/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6366.7336 - val_loss: 5420.0239\n",
      "Epoch 9242/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6366.2719 - val_loss: 5419.5591\n",
      "Epoch 9243/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6365.8090 - val_loss: 5419.0981\n",
      "Epoch 9244/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6365.3453 - val_loss: 5418.6357\n",
      "Epoch 9245/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6364.8828 - val_loss: 5418.1719\n",
      "Epoch 9246/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6364.4208 - val_loss: 5417.7104\n",
      "Epoch 9247/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6363.9584 - val_loss: 5417.2476\n",
      "Epoch 9248/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6363.4949 - val_loss: 5416.7842\n",
      "Epoch 9249/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6363.0328 - val_loss: 5416.3223\n",
      "Epoch 9250/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6362.5697 - val_loss: 5415.8589\n",
      "Epoch 9251/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6362.1073 - val_loss: 5415.3975\n",
      "Epoch 9252/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6361.6447 - val_loss: 5414.9336\n",
      "Epoch 9253/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6361.1818 - val_loss: 5414.4717\n",
      "Epoch 9254/10000\n",
      "750/750 [==============================] - 0s 161us/step - loss: 6360.7187 - val_loss: 5414.0083\n",
      "Epoch 9255/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6360.2561 - val_loss: 5413.5459\n",
      "Epoch 9256/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6359.7940 - val_loss: 5413.0825\n",
      "Epoch 9257/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6359.3307 - val_loss: 5412.6211\n",
      "Epoch 9258/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6358.8674 - val_loss: 5412.1587\n",
      "Epoch 9259/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6358.4058 - val_loss: 5411.6943\n",
      "Epoch 9260/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6357.9428 - val_loss: 5411.2334\n",
      "Epoch 9261/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6357.4800 - val_loss: 5410.7700\n",
      "Epoch 9262/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6357.0173 - val_loss: 5410.3066\n",
      "Epoch 9263/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6356.5547 - val_loss: 5409.8442\n",
      "Epoch 9264/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6356.0920 - val_loss: 5409.3818\n",
      "Epoch 9265/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6355.6299 - val_loss: 5408.9180\n",
      "Epoch 9266/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 6355.1666 - val_loss: 5408.4561\n",
      "Epoch 9267/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6354.7037 - val_loss: 5407.9937\n",
      "Epoch 9268/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6354.2411 - val_loss: 5407.5303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9269/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6353.7789 - val_loss: 5407.0669\n",
      "Epoch 9270/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6353.3159 - val_loss: 5406.6050\n",
      "Epoch 9271/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6352.8531 - val_loss: 5406.1431\n",
      "Epoch 9272/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6352.3898 - val_loss: 5405.6797\n",
      "Epoch 9273/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6351.9280 - val_loss: 5405.2173\n",
      "Epoch 9274/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6351.4649 - val_loss: 5404.7559\n",
      "Epoch 9275/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6351.0021 - val_loss: 5404.2920\n",
      "Epoch 9276/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6350.5392 - val_loss: 5403.8281\n",
      "Epoch 9277/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6350.0767 - val_loss: 5403.3667\n",
      "Epoch 9278/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6349.6148 - val_loss: 5402.9048\n",
      "Epoch 9279/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6349.1516 - val_loss: 5402.4404\n",
      "Epoch 9280/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6348.6886 - val_loss: 5401.9790\n",
      "Epoch 9281/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6348.2259 - val_loss: 5401.5161\n",
      "Epoch 9282/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6347.7640 - val_loss: 5401.0527\n",
      "Epoch 9283/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6347.3013 - val_loss: 5400.5898\n",
      "Epoch 9284/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6346.8379 - val_loss: 5400.1279\n",
      "Epoch 9285/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6346.3751 - val_loss: 5399.6655\n",
      "Epoch 9286/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6345.9126 - val_loss: 5399.2021\n",
      "Epoch 9287/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6345.4497 - val_loss: 5398.7388\n",
      "Epoch 9288/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6344.9870 - val_loss: 5398.2769\n",
      "Epoch 9289/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6344.5241 - val_loss: 5397.8149\n",
      "Epoch 9290/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6344.0616 - val_loss: 5397.3506\n",
      "Epoch 9291/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6343.5989 - val_loss: 5396.8896\n",
      "Epoch 9292/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6343.1364 - val_loss: 5396.4263\n",
      "Epoch 9293/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6342.6744 - val_loss: 5395.9624\n",
      "Epoch 9294/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6342.2108 - val_loss: 5395.5010\n",
      "Epoch 9295/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6341.7483 - val_loss: 5395.0381\n",
      "Epoch 9296/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6341.2861 - val_loss: 5394.5742\n",
      "Epoch 9297/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6340.8227 - val_loss: 5394.1128\n",
      "Epoch 9298/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6340.3598 - val_loss: 5393.6509\n",
      "Epoch 9299/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6339.8973 - val_loss: 5393.1865\n",
      "Epoch 9300/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6339.4355 - val_loss: 5392.7231\n",
      "Epoch 9301/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6338.9719 - val_loss: 5392.2617\n",
      "Epoch 9302/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6338.5092 - val_loss: 5391.7993\n",
      "Epoch 9303/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6338.0467 - val_loss: 5391.3359\n",
      "Epoch 9304/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6337.5842 - val_loss: 5390.8740\n",
      "Epoch 9305/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6337.1206 - val_loss: 5390.4102\n",
      "Epoch 9306/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6336.6588 - val_loss: 5389.9482\n",
      "Epoch 9307/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6336.1957 - val_loss: 5389.4844\n",
      "Epoch 9308/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6335.7328 - val_loss: 5389.0229\n",
      "Epoch 9309/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6335.2711 - val_loss: 5388.5605\n",
      "Epoch 9310/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6334.8083 - val_loss: 5388.0967\n",
      "Epoch 9311/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6334.3451 - val_loss: 5387.6353\n",
      "Epoch 9312/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6333.8820 - val_loss: 5387.1719\n",
      "Epoch 9313/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6333.4199 - val_loss: 5386.7095\n",
      "Epoch 9314/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6332.9575 - val_loss: 5386.2466\n",
      "Epoch 9315/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6332.4934 - val_loss: 5385.7842\n",
      "Epoch 9316/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6332.0312 - val_loss: 5385.3208\n",
      "Epoch 9317/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6331.5690 - val_loss: 5384.8584\n",
      "Epoch 9318/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6331.1062 - val_loss: 5384.3950\n",
      "Epoch 9319/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6330.6427 - val_loss: 5383.9326\n",
      "Epoch 9320/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6330.1805 - val_loss: 5383.4712\n",
      "Epoch 9321/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6329.7183 - val_loss: 5383.0073\n",
      "Epoch 9322/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6329.2554 - val_loss: 5382.5449\n",
      "Epoch 9323/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6328.7927 - val_loss: 5382.0815\n",
      "Epoch 9324/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 6328.3304 - val_loss: 5381.6191\n",
      "Epoch 9325/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6327.8670 - val_loss: 5381.1567\n",
      "Epoch 9326/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6327.4044 - val_loss: 5380.6943\n",
      "Epoch 9327/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6326.9425 - val_loss: 5380.2305\n",
      "Epoch 9328/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6326.4792 - val_loss: 5379.7690\n",
      "Epoch 9329/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6326.0163 - val_loss: 5379.3062\n",
      "Epoch 9330/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6325.5535 - val_loss: 5378.8428\n",
      "Epoch 9331/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6325.0910 - val_loss: 5378.3813\n",
      "Epoch 9332/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6324.6286 - val_loss: 5377.9180\n",
      "Epoch 9333/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6324.1647 - val_loss: 5377.4556\n",
      "Epoch 9334/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6323.7028 - val_loss: 5376.9922\n",
      "Epoch 9335/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6323.2404 - val_loss: 5376.5303\n",
      "Epoch 9336/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6322.7773 - val_loss: 5376.0669\n",
      "Epoch 9337/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6322.3152 - val_loss: 5375.6045\n",
      "Epoch 9338/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6321.8519 - val_loss: 5375.1421\n",
      "Epoch 9339/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6321.3896 - val_loss: 5374.6787\n",
      "Epoch 9340/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6320.9267 - val_loss: 5374.2168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9341/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6320.4640 - val_loss: 5373.7529\n",
      "Epoch 9342/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6320.0013 - val_loss: 5373.2915\n",
      "Epoch 9343/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6319.5382 - val_loss: 5372.8281\n",
      "Epoch 9344/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6319.0759 - val_loss: 5372.3638\n",
      "Epoch 9345/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6318.6134 - val_loss: 5371.9028\n",
      "Epoch 9346/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6318.1505 - val_loss: 5371.4404\n",
      "Epoch 9347/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6317.6872 - val_loss: 5370.9761\n",
      "Epoch 9348/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6317.2250 - val_loss: 5370.5151\n",
      "Epoch 9349/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6316.7629 - val_loss: 5370.0527\n",
      "Epoch 9350/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6316.2998 - val_loss: 5369.5889\n",
      "Epoch 9351/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6315.8368 - val_loss: 5369.1265\n",
      "Epoch 9352/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6315.3744 - val_loss: 5368.6636\n",
      "Epoch 9353/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6314.9118 - val_loss: 5368.2017\n",
      "Epoch 9354/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6314.4491 - val_loss: 5367.7378\n",
      "Epoch 9355/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6313.9858 - val_loss: 5367.2759\n",
      "Epoch 9356/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6313.5232 - val_loss: 5366.8125\n",
      "Epoch 9357/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6313.0607 - val_loss: 5366.3506\n",
      "Epoch 9358/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6312.5987 - val_loss: 5365.8867\n",
      "Epoch 9359/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6312.1350 - val_loss: 5365.4248\n",
      "Epoch 9360/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6311.6726 - val_loss: 5364.9634\n",
      "Epoch 9361/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6311.2099 - val_loss: 5364.4990\n",
      "Epoch 9362/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6310.7472 - val_loss: 5364.0376\n",
      "Epoch 9363/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6310.2848 - val_loss: 5363.5742\n",
      "Epoch 9364/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6309.8217 - val_loss: 5363.1108\n",
      "Epoch 9365/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6309.3591 - val_loss: 5362.6484\n",
      "Epoch 9366/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6308.8966 - val_loss: 5362.1865\n",
      "Epoch 9367/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6308.4342 - val_loss: 5361.7231\n",
      "Epoch 9368/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6307.9712 - val_loss: 5361.2607\n",
      "Epoch 9369/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6307.5081 - val_loss: 5360.7974\n",
      "Epoch 9370/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6307.0455 - val_loss: 5360.3350\n",
      "Epoch 9371/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6306.5836 - val_loss: 5359.8716\n",
      "Epoch 9372/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6306.1204 - val_loss: 5359.4097\n",
      "Epoch 9373/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6305.6578 - val_loss: 5358.9478\n",
      "Epoch 9374/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6305.1944 - val_loss: 5358.4858\n",
      "Epoch 9375/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6304.7328 - val_loss: 5358.0200\n",
      "Epoch 9376/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6304.2692 - val_loss: 5357.5591\n",
      "Epoch 9377/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6303.8064 - val_loss: 5357.0967\n",
      "Epoch 9378/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6303.3443 - val_loss: 5356.6323\n",
      "Epoch 9379/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6302.8812 - val_loss: 5356.1714\n",
      "Epoch 9380/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6302.4187 - val_loss: 5355.7075\n",
      "Epoch 9381/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6301.9560 - val_loss: 5355.2451\n",
      "Epoch 9382/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6301.4932 - val_loss: 5354.7827\n",
      "Epoch 9383/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6301.0303 - val_loss: 5354.3198\n",
      "Epoch 9384/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6300.5682 - val_loss: 5353.8579\n",
      "Epoch 9385/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6300.1053 - val_loss: 5353.3940\n",
      "Epoch 9386/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6299.6423 - val_loss: 5352.9321\n",
      "Epoch 9387/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6299.1794 - val_loss: 5352.4683\n",
      "Epoch 9388/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6298.7169 - val_loss: 5352.0068\n",
      "Epoch 9389/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6298.2549 - val_loss: 5351.5435\n",
      "Epoch 9390/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6297.7913 - val_loss: 5351.0811\n",
      "Epoch 9391/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6297.3284 - val_loss: 5350.6187\n",
      "Epoch 9392/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6296.8664 - val_loss: 5350.1553\n",
      "Epoch 9393/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6296.4030 - val_loss: 5349.6934\n",
      "Epoch 9394/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6295.9409 - val_loss: 5349.2305\n",
      "Epoch 9395/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6295.4785 - val_loss: 5348.7676\n",
      "Epoch 9396/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6295.0151 - val_loss: 5348.3062\n",
      "Epoch 9397/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6294.5527 - val_loss: 5347.8423\n",
      "Epoch 9398/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6294.0903 - val_loss: 5347.3789\n",
      "Epoch 9399/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6293.6274 - val_loss: 5346.9170\n",
      "Epoch 9400/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6293.1639 - val_loss: 5346.4536\n",
      "Epoch 9401/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6292.7018 - val_loss: 5345.9917\n",
      "Epoch 9402/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6292.2399 - val_loss: 5345.5278\n",
      "Epoch 9403/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6291.7762 - val_loss: 5345.0654\n",
      "Epoch 9404/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6291.3131 - val_loss: 5344.6030\n",
      "Epoch 9405/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6290.8506 - val_loss: 5344.1406\n",
      "Epoch 9406/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6290.3889 - val_loss: 5343.6782\n",
      "Epoch 9407/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6289.9260 - val_loss: 5343.2148\n",
      "Epoch 9408/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6289.4621 - val_loss: 5342.7529\n",
      "Epoch 9409/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6289.0001 - val_loss: 5342.2886\n",
      "Epoch 9410/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6288.5375 - val_loss: 5341.8271\n",
      "Epoch 9411/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6288.0749 - val_loss: 5341.3638\n",
      "Epoch 9412/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6287.6119 - val_loss: 5340.9014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9413/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6287.1494 - val_loss: 5340.4404\n",
      "Epoch 9414/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6286.6865 - val_loss: 5339.9761\n",
      "Epoch 9415/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6286.2244 - val_loss: 5339.5142\n",
      "Epoch 9416/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6285.7612 - val_loss: 5339.0503\n",
      "Epoch 9417/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6285.2986 - val_loss: 5338.5884\n",
      "Epoch 9418/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6284.8357 - val_loss: 5338.1250\n",
      "Epoch 9419/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6284.3731 - val_loss: 5337.6626\n",
      "Epoch 9420/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6283.9106 - val_loss: 5337.1997\n",
      "Epoch 9421/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6283.4477 - val_loss: 5336.7373\n",
      "Epoch 9422/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 6282.9843 - val_loss: 5336.2754\n",
      "Epoch 9423/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6282.5224 - val_loss: 5335.8115\n",
      "Epoch 9424/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6282.0602 - val_loss: 5335.3501\n",
      "Epoch 9425/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6281.5969 - val_loss: 5334.8867\n",
      "Epoch 9426/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6281.1343 - val_loss: 5334.4233\n",
      "Epoch 9427/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6280.6714 - val_loss: 5333.9614\n",
      "Epoch 9428/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6280.2090 - val_loss: 5333.4985\n",
      "Epoch 9429/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6279.7462 - val_loss: 5333.0352\n",
      "Epoch 9430/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6279.2836 - val_loss: 5332.5737\n",
      "Epoch 9431/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6278.8203 - val_loss: 5332.1108\n",
      "Epoch 9432/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6278.3581 - val_loss: 5331.6475\n",
      "Epoch 9433/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6277.8958 - val_loss: 5331.1841\n",
      "Epoch 9434/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6277.4328 - val_loss: 5330.7217\n",
      "Epoch 9435/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6276.9705 - val_loss: 5330.2598\n",
      "Epoch 9436/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6276.5070 - val_loss: 5329.7964\n",
      "Epoch 9437/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6276.0447 - val_loss: 5329.3345\n",
      "Epoch 9438/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6275.5826 - val_loss: 5328.8711\n",
      "Epoch 9439/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6275.1189 - val_loss: 5328.4092\n",
      "Epoch 9440/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6274.6562 - val_loss: 5327.9448\n",
      "Epoch 9441/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6274.1938 - val_loss: 5327.4839\n",
      "Epoch 9442/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6273.7318 - val_loss: 5327.0200\n",
      "Epoch 9443/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6273.2684 - val_loss: 5326.5576\n",
      "Epoch 9444/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6272.8054 - val_loss: 5326.0962\n",
      "Epoch 9445/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6272.3432 - val_loss: 5325.6323\n",
      "Epoch 9446/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6271.8807 - val_loss: 5325.1685\n",
      "Epoch 9447/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6271.4178 - val_loss: 5324.7075\n",
      "Epoch 9448/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6270.9550 - val_loss: 5324.2451\n",
      "Epoch 9449/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6270.4919 - val_loss: 5323.7827\n",
      "Epoch 9450/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6270.0297 - val_loss: 5323.3193\n",
      "Epoch 9451/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6269.5666 - val_loss: 5322.8560\n",
      "Epoch 9452/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6269.1038 - val_loss: 5322.3936\n",
      "Epoch 9453/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6268.6413 - val_loss: 5321.9312\n",
      "Epoch 9454/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6268.1788 - val_loss: 5321.4678\n",
      "Epoch 9455/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6267.7160 - val_loss: 5321.0044\n",
      "Epoch 9456/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6267.2537 - val_loss: 5320.5430\n",
      "Epoch 9457/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6266.7904 - val_loss: 5320.0796\n",
      "Epoch 9458/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 6266.3274 - val_loss: 5319.6177\n",
      "Epoch 9459/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6265.8652 - val_loss: 5319.1553\n",
      "Epoch 9460/10000\n",
      "750/750 [==============================] - 0s 157us/step - loss: 6265.4029 - val_loss: 5318.6904\n",
      "Epoch 9461/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6264.9396 - val_loss: 5318.2295\n",
      "Epoch 9462/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6264.4765 - val_loss: 5317.7661\n",
      "Epoch 9463/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6264.0141 - val_loss: 5317.3037\n",
      "Epoch 9464/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6263.5516 - val_loss: 5316.8418\n",
      "Epoch 9465/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6263.0891 - val_loss: 5316.3784\n",
      "Epoch 9466/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6262.6256 - val_loss: 5315.9165\n",
      "Epoch 9467/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6262.1635 - val_loss: 5315.4526\n",
      "Epoch 9468/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6261.7007 - val_loss: 5314.9902\n",
      "Epoch 9469/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6261.2384 - val_loss: 5314.5273\n",
      "Epoch 9470/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6260.7749 - val_loss: 5314.0654\n",
      "Epoch 9471/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6260.3122 - val_loss: 5313.6021\n",
      "Epoch 9472/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6259.8501 - val_loss: 5313.1396\n",
      "Epoch 9473/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6259.3876 - val_loss: 5312.6777\n",
      "Epoch 9474/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6258.9248 - val_loss: 5312.2139\n",
      "Epoch 9475/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6258.4610 - val_loss: 5311.7524\n",
      "Epoch 9476/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6257.9992 - val_loss: 5311.2886\n",
      "Epoch 9477/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6257.5369 - val_loss: 5310.8247\n",
      "Epoch 9478/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6257.0738 - val_loss: 5310.3638\n",
      "Epoch 9479/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6256.6108 - val_loss: 5309.9004\n",
      "Epoch 9480/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6256.1482 - val_loss: 5309.4375\n",
      "Epoch 9481/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6255.6860 - val_loss: 5308.9761\n",
      "Epoch 9482/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6255.2232 - val_loss: 5308.5122\n",
      "Epoch 9483/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6254.7596 - val_loss: 5308.0498\n",
      "Epoch 9484/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6254.2974 - val_loss: 5307.5874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9485/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6253.8348 - val_loss: 5307.1240\n",
      "Epoch 9486/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6253.3719 - val_loss: 5306.6621\n",
      "Epoch 9487/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6252.9097 - val_loss: 5306.1987\n",
      "Epoch 9488/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6252.4467 - val_loss: 5305.7368\n",
      "Epoch 9489/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6251.9837 - val_loss: 5305.2729\n",
      "Epoch 9490/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6251.5213 - val_loss: 5304.8115\n",
      "Epoch 9491/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6251.0592 - val_loss: 5304.3477\n",
      "Epoch 9492/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6250.5957 - val_loss: 5303.8857\n",
      "Epoch 9493/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6250.1326 - val_loss: 5303.4233\n",
      "Epoch 9494/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6249.6706 - val_loss: 5302.9595\n",
      "Epoch 9495/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6249.2080 - val_loss: 5302.4985\n",
      "Epoch 9496/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6248.7452 - val_loss: 5302.0342\n",
      "Epoch 9497/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6248.2819 - val_loss: 5301.5718\n",
      "Epoch 9498/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6247.8194 - val_loss: 5301.1108\n",
      "Epoch 9499/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6247.3573 - val_loss: 5300.6470\n",
      "Epoch 9500/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6246.8946 - val_loss: 5300.1841\n",
      "Epoch 9501/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6246.4314 - val_loss: 5299.7217\n",
      "Epoch 9502/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6245.9690 - val_loss: 5299.2573\n",
      "Epoch 9503/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6245.5061 - val_loss: 5298.7959\n",
      "Epoch 9504/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6245.0438 - val_loss: 5298.3325\n",
      "Epoch 9505/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6244.5811 - val_loss: 5297.8701\n",
      "Epoch 9506/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6244.1179 - val_loss: 5297.4077\n",
      "Epoch 9507/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6243.6552 - val_loss: 5296.9453\n",
      "Epoch 9508/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6243.1934 - val_loss: 5296.4805\n",
      "Epoch 9509/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6242.7299 - val_loss: 5296.0200\n",
      "Epoch 9510/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6242.2668 - val_loss: 5295.5576\n",
      "Epoch 9511/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6241.8043 - val_loss: 5295.0928\n",
      "Epoch 9512/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6241.3414 - val_loss: 5294.6318\n",
      "Epoch 9513/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6240.8801 - val_loss: 5294.1685\n",
      "Epoch 9514/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6240.4161 - val_loss: 5293.7061\n",
      "Epoch 9515/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6239.9537 - val_loss: 5293.2437\n",
      "Epoch 9516/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6239.4912 - val_loss: 5292.7803\n",
      "Epoch 9517/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6239.0290 - val_loss: 5292.3184\n",
      "Epoch 9518/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6238.5659 - val_loss: 5291.8550\n",
      "Epoch 9519/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6238.1031 - val_loss: 5291.3926\n",
      "Epoch 9520/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6237.6401 - val_loss: 5290.9302\n",
      "Epoch 9521/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6237.1777 - val_loss: 5290.4678\n",
      "Epoch 9522/10000\n",
      "750/750 [==============================] - 0s 150us/step - loss: 6236.7149 - val_loss: 5290.0044\n",
      "Epoch 9523/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6236.2522 - val_loss: 5289.5420\n",
      "Epoch 9524/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 6235.7888 - val_loss: 5289.0801\n",
      "Epoch 9525/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6235.3271 - val_loss: 5288.6152\n",
      "Epoch 9526/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6234.8641 - val_loss: 5288.1543\n",
      "Epoch 9527/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6234.4013 - val_loss: 5287.6909\n",
      "Epoch 9528/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6233.9383 - val_loss: 5287.2271\n",
      "Epoch 9529/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6233.4758 - val_loss: 5286.7661\n",
      "Epoch 9530/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6233.0134 - val_loss: 5286.3032\n",
      "Epoch 9531/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6232.5512 - val_loss: 5285.8394\n",
      "Epoch 9532/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6232.0879 - val_loss: 5285.3779\n",
      "Epoch 9533/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6231.6249 - val_loss: 5284.9146\n",
      "Epoch 9534/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6231.1626 - val_loss: 5284.4521\n",
      "Epoch 9535/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6230.7002 - val_loss: 5283.9878\n",
      "Epoch 9536/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6230.2370 - val_loss: 5283.5264\n",
      "Epoch 9537/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6229.7735 - val_loss: 5283.0649\n",
      "Epoch 9538/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6229.3113 - val_loss: 5282.6011\n",
      "Epoch 9539/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6228.8494 - val_loss: 5282.1392\n",
      "Epoch 9540/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6228.3858 - val_loss: 5281.6763\n",
      "Epoch 9541/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6227.9234 - val_loss: 5281.2134\n",
      "Epoch 9542/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6227.4608 - val_loss: 5280.7490\n",
      "Epoch 9543/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6226.9975 - val_loss: 5280.2881\n",
      "Epoch 9544/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6226.5356 - val_loss: 5279.8247\n",
      "Epoch 9545/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6226.0727 - val_loss: 5279.3618\n",
      "Epoch 9546/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6225.6098 - val_loss: 5278.9009\n",
      "Epoch 9547/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6225.1472 - val_loss: 5278.4365\n",
      "Epoch 9548/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6224.6846 - val_loss: 5277.9731\n",
      "Epoch 9549/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6224.2219 - val_loss: 5277.5117\n",
      "Epoch 9550/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 6223.7588 - val_loss: 5277.0483\n",
      "Epoch 9551/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6223.2963 - val_loss: 5276.5864\n",
      "Epoch 9552/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6222.8340 - val_loss: 5276.1235\n",
      "Epoch 9553/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6222.3710 - val_loss: 5275.6602\n",
      "Epoch 9554/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6221.9083 - val_loss: 5275.1982\n",
      "Epoch 9555/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6221.4452 - val_loss: 5274.7358\n",
      "Epoch 9556/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6220.9833 - val_loss: 5274.2725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9557/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6220.5202 - val_loss: 5273.8101\n",
      "Epoch 9558/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 6220.0574 - val_loss: 5273.3467\n",
      "Epoch 9559/10000\n",
      "750/750 [==============================] - 0s 109us/step - loss: 6219.5955 - val_loss: 5272.8833\n",
      "Epoch 9560/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 6219.1317 - val_loss: 5272.4219\n",
      "Epoch 9561/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 6218.6696 - val_loss: 5271.9595\n",
      "Epoch 9562/10000\n",
      "750/750 [==============================] - 0s 89us/step - loss: 6218.2073 - val_loss: 5271.4956\n",
      "Epoch 9563/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 6217.7443 - val_loss: 5271.0342\n",
      "Epoch 9564/10000\n",
      "750/750 [==============================] - 0s 97us/step - loss: 6217.2809 - val_loss: 5270.5708\n",
      "Epoch 9565/10000\n",
      "750/750 [==============================] - 0s 98us/step - loss: 6216.8188 - val_loss: 5270.1084\n",
      "Epoch 9566/10000\n",
      "750/750 [==============================] - 0s 106us/step - loss: 6216.3562 - val_loss: 5269.6450\n",
      "Epoch 9567/10000\n",
      "750/750 [==============================] - 0s 105us/step - loss: 6215.8933 - val_loss: 5269.1826\n",
      "Epoch 9568/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6215.4302 - val_loss: 5268.7212\n",
      "Epoch 9569/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6214.9677 - val_loss: 5268.2568\n",
      "Epoch 9570/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6214.5055 - val_loss: 5267.7935\n",
      "Epoch 9571/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6214.0427 - val_loss: 5267.3315\n",
      "Epoch 9572/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6213.5792 - val_loss: 5266.8701\n",
      "Epoch 9573/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6213.1169 - val_loss: 5266.4067\n",
      "Epoch 9574/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6212.6544 - val_loss: 5265.9443\n",
      "Epoch 9575/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6212.1917 - val_loss: 5265.4810\n",
      "Epoch 9576/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6211.7289 - val_loss: 5265.0181\n",
      "Epoch 9577/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6211.2662 - val_loss: 5264.5566\n",
      "Epoch 9578/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6210.8036 - val_loss: 5264.0928\n",
      "Epoch 9579/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6210.3408 - val_loss: 5263.6294\n",
      "Epoch 9580/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6209.8787 - val_loss: 5263.1685\n",
      "Epoch 9581/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6209.4152 - val_loss: 5262.7051\n",
      "Epoch 9582/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6208.9527 - val_loss: 5262.2422\n",
      "Epoch 9583/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6208.4903 - val_loss: 5261.7793\n",
      "Epoch 9584/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6208.0275 - val_loss: 5261.3164\n",
      "Epoch 9585/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6207.5648 - val_loss: 5260.8545\n",
      "Epoch 9586/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6207.1018 - val_loss: 5260.3911\n",
      "Epoch 9587/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6206.6392 - val_loss: 5259.9287\n",
      "Epoch 9588/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6206.1765 - val_loss: 5259.4653\n",
      "Epoch 9589/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6205.7137 - val_loss: 5259.0039\n",
      "Epoch 9590/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6205.2507 - val_loss: 5258.5405\n",
      "Epoch 9591/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6204.7886 - val_loss: 5258.0786\n",
      "Epoch 9592/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6204.3258 - val_loss: 5257.6152\n",
      "Epoch 9593/10000\n",
      "750/750 [==============================] - 0s 104us/step - loss: 6203.8635 - val_loss: 5257.1514\n",
      "Epoch 9594/10000\n",
      "750/750 [==============================] - 0s 101us/step - loss: 6203.4006 - val_loss: 5256.6904\n",
      "Epoch 9595/10000\n",
      "750/750 [==============================] - 0s 96us/step - loss: 6202.9378 - val_loss: 5256.2271\n",
      "Epoch 9596/10000\n",
      "750/750 [==============================] - 0s 100us/step - loss: 6202.4750 - val_loss: 5255.7642\n",
      "Epoch 9597/10000\n",
      "750/750 [==============================] - 0s 94us/step - loss: 6202.0125 - val_loss: 5255.3032\n",
      "Epoch 9598/10000\n",
      "750/750 [==============================] - 0s 102us/step - loss: 6201.5498 - val_loss: 5254.8394\n",
      "Epoch 9599/10000\n",
      "750/750 [==============================] - 0s 93us/step - loss: 6201.0861 - val_loss: 5254.3770\n",
      "Epoch 9600/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6200.6242 - val_loss: 5253.9131\n",
      "Epoch 9601/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6200.1613 - val_loss: 5253.4497\n",
      "Epoch 9602/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6199.6988 - val_loss: 5252.9878\n",
      "Epoch 9603/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6199.2354 - val_loss: 5252.5259\n",
      "Epoch 9604/10000\n",
      "750/750 [==============================] - 0s 107us/step - loss: 6198.7733 - val_loss: 5252.0625\n",
      "Epoch 9605/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6198.3108 - val_loss: 5251.6006\n",
      "Epoch 9606/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6197.8482 - val_loss: 5251.1372\n",
      "Epoch 9607/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6197.3853 - val_loss: 5250.6738\n",
      "Epoch 9608/10000\n",
      "750/750 [==============================] - 0s 108us/step - loss: 6196.9224 - val_loss: 5250.2129\n",
      "Epoch 9609/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6196.4599 - val_loss: 5249.7490\n",
      "Epoch 9610/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6195.9971 - val_loss: 5249.2852\n",
      "Epoch 9611/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6195.5343 - val_loss: 5248.8237\n",
      "Epoch 9612/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6195.0715 - val_loss: 5248.3618\n",
      "Epoch 9613/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6194.6087 - val_loss: 5247.8979\n",
      "Epoch 9614/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6194.1464 - val_loss: 5247.4365\n",
      "Epoch 9615/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6193.6835 - val_loss: 5246.9731\n",
      "Epoch 9616/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6193.2209 - val_loss: 5246.5107\n",
      "Epoch 9617/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6192.7578 - val_loss: 5246.0469\n",
      "Epoch 9618/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6192.2954 - val_loss: 5245.5850\n",
      "Epoch 9619/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6191.8328 - val_loss: 5245.1230\n",
      "Epoch 9620/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6191.3703 - val_loss: 5244.6592\n",
      "Epoch 9621/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6190.9069 - val_loss: 5244.1968\n",
      "Epoch 9622/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6190.4446 - val_loss: 5243.7344\n",
      "Epoch 9623/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6189.9820 - val_loss: 5243.2725\n",
      "Epoch 9624/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6189.5199 - val_loss: 5242.8076\n",
      "Epoch 9625/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6189.0560 - val_loss: 5242.3467\n",
      "Epoch 9626/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6188.5935 - val_loss: 5241.8833\n",
      "Epoch 9627/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6188.1311 - val_loss: 5241.4204\n",
      "Epoch 9628/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6187.6685 - val_loss: 5240.9575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9629/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6187.2057 - val_loss: 5240.4951\n",
      "Epoch 9630/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6186.7423 - val_loss: 5240.0327\n",
      "Epoch 9631/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6186.2802 - val_loss: 5239.5698\n",
      "Epoch 9632/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6185.8178 - val_loss: 5239.1079\n",
      "Epoch 9633/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6185.3552 - val_loss: 5238.6440\n",
      "Epoch 9634/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6184.8921 - val_loss: 5238.1816\n",
      "Epoch 9635/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6184.4296 - val_loss: 5237.7178\n",
      "Epoch 9636/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6183.9667 - val_loss: 5237.2568\n",
      "Epoch 9637/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6183.5049 - val_loss: 5236.7935\n",
      "Epoch 9638/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6183.0417 - val_loss: 5236.3311\n",
      "Epoch 9639/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6182.5785 - val_loss: 5235.8687\n",
      "Epoch 9640/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6182.1161 - val_loss: 5235.4058\n",
      "Epoch 9641/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6181.6541 - val_loss: 5234.9414\n",
      "Epoch 9642/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6181.1899 - val_loss: 5234.4805\n",
      "Epoch 9643/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6180.7277 - val_loss: 5234.0181\n",
      "Epoch 9644/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6180.2650 - val_loss: 5233.5537\n",
      "Epoch 9645/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6179.8028 - val_loss: 5233.0928\n",
      "Epoch 9646/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6179.3397 - val_loss: 5232.6289\n",
      "Epoch 9647/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6178.8770 - val_loss: 5232.1665\n",
      "Epoch 9648/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6178.4141 - val_loss: 5231.7051\n",
      "Epoch 9649/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6177.9518 - val_loss: 5231.2412\n",
      "Epoch 9650/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6177.4888 - val_loss: 5230.7788\n",
      "Epoch 9651/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6177.0263 - val_loss: 5230.3154\n",
      "Epoch 9652/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6176.5641 - val_loss: 5229.8521\n",
      "Epoch 9653/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6176.1001 - val_loss: 5229.3906\n",
      "Epoch 9654/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6175.6384 - val_loss: 5228.9282\n",
      "Epoch 9655/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6175.1762 - val_loss: 5228.4648\n",
      "Epoch 9656/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6174.7127 - val_loss: 5228.0024\n",
      "Epoch 9657/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6174.2496 - val_loss: 5227.5410\n",
      "Epoch 9658/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6173.7878 - val_loss: 5227.0747\n",
      "Epoch 9659/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6173.3247 - val_loss: 5226.6138\n",
      "Epoch 9660/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6172.8621 - val_loss: 5226.1514\n",
      "Epoch 9661/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6172.3987 - val_loss: 5225.6885\n",
      "Epoch 9662/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6171.9365 - val_loss: 5225.2261\n",
      "Epoch 9663/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6171.4742 - val_loss: 5224.7642\n",
      "Epoch 9664/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6171.0112 - val_loss: 5224.3003\n",
      "Epoch 9665/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6170.5484 - val_loss: 5223.8389\n",
      "Epoch 9666/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6170.0857 - val_loss: 5223.3750\n",
      "Epoch 9667/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6169.6233 - val_loss: 5222.9131\n",
      "Epoch 9668/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6169.1609 - val_loss: 5222.4497\n",
      "Epoch 9669/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6168.6983 - val_loss: 5221.9873\n",
      "Epoch 9670/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6168.2343 - val_loss: 5221.5249\n",
      "Epoch 9671/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6167.7721 - val_loss: 5221.0615\n",
      "Epoch 9672/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6167.3102 - val_loss: 5220.5996\n",
      "Epoch 9673/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6166.8469 - val_loss: 5220.1367\n",
      "Epoch 9674/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6166.3834 - val_loss: 5219.6748\n",
      "Epoch 9675/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6165.9213 - val_loss: 5219.2100\n",
      "Epoch 9676/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6165.4586 - val_loss: 5218.7490\n",
      "Epoch 9677/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6164.9959 - val_loss: 5218.2852\n",
      "Epoch 9678/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6164.5332 - val_loss: 5217.8223\n",
      "Epoch 9679/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6164.0703 - val_loss: 5217.3608\n",
      "Epoch 9680/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6163.6081 - val_loss: 5216.8975\n",
      "Epoch 9681/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6163.1455 - val_loss: 5216.4341\n",
      "Epoch 9682/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6162.6828 - val_loss: 5215.9717\n",
      "Epoch 9683/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6162.2201 - val_loss: 5215.5098\n",
      "Epoch 9684/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6161.7569 - val_loss: 5215.0469\n",
      "Epoch 9685/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6161.2947 - val_loss: 5214.5825\n",
      "Epoch 9686/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 6160.8319 - val_loss: 5214.1206\n",
      "Epoch 9687/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6160.3687 - val_loss: 5213.6592\n",
      "Epoch 9688/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6159.9059 - val_loss: 5213.1958\n",
      "Epoch 9689/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6159.4437 - val_loss: 5212.7334\n",
      "Epoch 9690/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6158.9807 - val_loss: 5212.2720\n",
      "Epoch 9691/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6158.5183 - val_loss: 5211.8076\n",
      "Epoch 9692/10000\n",
      "750/750 [==============================] - 0s 156us/step - loss: 6158.0548 - val_loss: 5211.3433\n",
      "Epoch 9693/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 6157.5926 - val_loss: 5210.8823\n",
      "Epoch 9694/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6157.1306 - val_loss: 5210.4204\n",
      "Epoch 9695/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6156.6675 - val_loss: 5209.9561\n",
      "Epoch 9696/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6156.2042 - val_loss: 5209.4951\n",
      "Epoch 9697/10000\n",
      "750/750 [==============================] - 0s 112us/step - loss: 6155.7416 - val_loss: 5209.0308\n",
      "Epoch 9698/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6155.2795 - val_loss: 5208.5688\n",
      "Epoch 9699/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6154.8167 - val_loss: 5208.1055\n",
      "Epoch 9700/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6154.3538 - val_loss: 5207.6436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9701/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6153.8906 - val_loss: 5207.1821\n",
      "Epoch 9702/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6153.4284 - val_loss: 5206.7178\n",
      "Epoch 9703/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6152.9655 - val_loss: 5206.2544\n",
      "Epoch 9704/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6152.5031 - val_loss: 5205.7930\n",
      "Epoch 9705/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6152.0401 - val_loss: 5205.3306\n",
      "Epoch 9706/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6151.5774 - val_loss: 5204.8667\n",
      "Epoch 9707/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6151.1151 - val_loss: 5204.4053\n",
      "Epoch 9708/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6150.6520 - val_loss: 5203.9414\n",
      "Epoch 9709/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6150.1896 - val_loss: 5203.4780\n",
      "Epoch 9710/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6149.7263 - val_loss: 5203.0171\n",
      "Epoch 9711/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6149.2644 - val_loss: 5202.5537\n",
      "Epoch 9712/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6148.8019 - val_loss: 5202.0903\n",
      "Epoch 9713/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6148.3386 - val_loss: 5201.6289\n",
      "Epoch 9714/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6147.8758 - val_loss: 5201.1655\n",
      "Epoch 9715/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6147.4131 - val_loss: 5200.7026\n",
      "Epoch 9716/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6146.9510 - val_loss: 5200.2388\n",
      "Epoch 9717/10000\n",
      "750/750 [==============================] - 0s 151us/step - loss: 6146.4878 - val_loss: 5199.7764\n",
      "Epoch 9718/10000\n",
      "750/750 [==============================] - 0s 155us/step - loss: 6146.0250 - val_loss: 5199.3154\n",
      "Epoch 9719/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6145.5625 - val_loss: 5198.8521\n",
      "Epoch 9720/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6145.0998 - val_loss: 5198.3892\n",
      "Epoch 9721/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6144.6373 - val_loss: 5197.9263\n",
      "Epoch 9722/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6144.1745 - val_loss: 5197.4648\n",
      "Epoch 9723/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6143.7112 - val_loss: 5197.0015\n",
      "Epoch 9724/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6143.2491 - val_loss: 5196.5386\n",
      "Epoch 9725/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 6142.7866 - val_loss: 5196.0767\n",
      "Epoch 9726/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6142.3237 - val_loss: 5195.6123\n",
      "Epoch 9727/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6141.8607 - val_loss: 5195.1514\n",
      "Epoch 9728/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6141.3980 - val_loss: 5194.6875\n",
      "Epoch 9729/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6140.9358 - val_loss: 5194.2251\n",
      "Epoch 9730/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6140.4734 - val_loss: 5193.7622\n",
      "Epoch 9731/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6140.0095 - val_loss: 5193.2993\n",
      "Epoch 9732/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6139.5470 - val_loss: 5192.8374\n",
      "Epoch 9733/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6139.0850 - val_loss: 5192.3740\n",
      "Epoch 9734/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6138.6220 - val_loss: 5191.9106\n",
      "Epoch 9735/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6138.1598 - val_loss: 5191.4492\n",
      "Epoch 9736/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6137.6962 - val_loss: 5190.9868\n",
      "Epoch 9737/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6137.2339 - val_loss: 5190.5234\n",
      "Epoch 9738/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6136.7716 - val_loss: 5190.0610\n",
      "Epoch 9739/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6136.3085 - val_loss: 5189.5981\n",
      "Epoch 9740/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6135.8460 - val_loss: 5189.1357\n",
      "Epoch 9741/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6135.3826 - val_loss: 5188.6733\n",
      "Epoch 9742/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6134.9203 - val_loss: 5188.2100\n",
      "Epoch 9743/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6134.4578 - val_loss: 5187.7466\n",
      "Epoch 9744/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6133.9949 - val_loss: 5187.2852\n",
      "Epoch 9745/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6133.5320 - val_loss: 5186.8223\n",
      "Epoch 9746/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6133.0693 - val_loss: 5186.3589\n",
      "Epoch 9747/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6132.6074 - val_loss: 5185.8975\n",
      "Epoch 9748/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6132.1445 - val_loss: 5185.4336\n",
      "Epoch 9749/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6131.6808 - val_loss: 5184.9712\n",
      "Epoch 9750/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6131.2190 - val_loss: 5184.5073\n",
      "Epoch 9751/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6130.7563 - val_loss: 5184.0459\n",
      "Epoch 9752/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6130.2937 - val_loss: 5183.5825\n",
      "Epoch 9753/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6129.8307 - val_loss: 5183.1206\n",
      "Epoch 9754/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6129.3675 - val_loss: 5182.6582\n",
      "Epoch 9755/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6128.9054 - val_loss: 5182.1943\n",
      "Epoch 9756/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6128.4428 - val_loss: 5181.7329\n",
      "Epoch 9757/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6127.9803 - val_loss: 5181.2686\n",
      "Epoch 9758/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6127.5172 - val_loss: 5180.8076\n",
      "Epoch 9759/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6127.0541 - val_loss: 5180.3433\n",
      "Epoch 9760/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6126.5918 - val_loss: 5179.8794\n",
      "Epoch 9761/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6126.1292 - val_loss: 5179.4185\n",
      "Epoch 9762/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6125.6662 - val_loss: 5178.9561\n",
      "Epoch 9763/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6125.2035 - val_loss: 5178.4922\n",
      "Epoch 9764/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6124.7409 - val_loss: 5178.0308\n",
      "Epoch 9765/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6124.2784 - val_loss: 5177.5669\n",
      "Epoch 9766/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6123.8158 - val_loss: 5177.1050\n",
      "Epoch 9767/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6123.3523 - val_loss: 5176.6426\n",
      "Epoch 9768/10000\n",
      "750/750 [==============================] - 0s 119us/step - loss: 6122.8902 - val_loss: 5176.1792\n",
      "Epoch 9769/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 6122.4278 - val_loss: 5175.7178\n",
      "Epoch 9770/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6121.9645 - val_loss: 5175.2544\n",
      "Epoch 9771/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6121.5014 - val_loss: 5174.7920\n",
      "Epoch 9772/10000\n",
      "750/750 [==============================] - 0s 110us/step - loss: 6121.0390 - val_loss: 5174.3286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9773/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6120.5765 - val_loss: 5173.8662\n",
      "Epoch 9774/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6120.1143 - val_loss: 5173.4023\n",
      "Epoch 9775/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6119.6510 - val_loss: 5172.9414\n",
      "Epoch 9776/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6119.1885 - val_loss: 5172.4785\n",
      "Epoch 9777/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6118.7257 - val_loss: 5172.0146\n",
      "Epoch 9778/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6118.2629 - val_loss: 5171.5537\n",
      "Epoch 9779/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6117.8006 - val_loss: 5171.0898\n",
      "Epoch 9780/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6117.3379 - val_loss: 5170.6265\n",
      "Epoch 9781/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6116.8745 - val_loss: 5170.1655\n",
      "Epoch 9782/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6116.4122 - val_loss: 5169.7017\n",
      "Epoch 9783/10000\n",
      "750/750 [==============================] - 0s 146us/step - loss: 6115.9499 - val_loss: 5169.2388\n",
      "Epoch 9784/10000\n",
      "750/750 [==============================] - 0s 172us/step - loss: 6115.4865 - val_loss: 5168.7764\n",
      "Epoch 9785/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 6115.0237 - val_loss: 5168.3135\n",
      "Epoch 9786/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 6114.5612 - val_loss: 5167.8511\n",
      "Epoch 9787/10000\n",
      "750/750 [==============================] - 0s 147us/step - loss: 6114.0992 - val_loss: 5167.3892\n",
      "Epoch 9788/10000\n",
      "750/750 [==============================] - 0s 162us/step - loss: 6113.6362 - val_loss: 5166.9253\n",
      "Epoch 9789/10000\n",
      "750/750 [==============================] - 0s 154us/step - loss: 6113.1734 - val_loss: 5166.4634\n",
      "Epoch 9790/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6112.7104 - val_loss: 5166.0015\n",
      "Epoch 9791/10000\n",
      "750/750 [==============================] - 0s 149us/step - loss: 6112.2487 - val_loss: 5165.5356\n",
      "Epoch 9792/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6111.7851 - val_loss: 5165.0747\n",
      "Epoch 9793/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6111.3226 - val_loss: 5164.6123\n",
      "Epoch 9794/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6110.8599 - val_loss: 5164.1479\n",
      "Epoch 9795/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6110.3968 - val_loss: 5163.6865\n",
      "Epoch 9796/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6109.9351 - val_loss: 5163.2231\n",
      "Epoch 9797/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6109.4722 - val_loss: 5162.7607\n",
      "Epoch 9798/10000\n",
      "750/750 [==============================] - 0s 135us/step - loss: 6109.0087 - val_loss: 5162.2998\n",
      "Epoch 9799/10000\n",
      "750/750 [==============================] - 0s 145us/step - loss: 6108.5461 - val_loss: 5161.8354\n",
      "Epoch 9800/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6108.0841 - val_loss: 5161.3740\n",
      "Epoch 9801/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6107.6210 - val_loss: 5160.9106\n",
      "Epoch 9802/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6107.1586 - val_loss: 5160.4473\n",
      "Epoch 9803/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6106.6952 - val_loss: 5159.9849\n",
      "Epoch 9804/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6106.2329 - val_loss: 5159.5229\n",
      "Epoch 9805/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6105.7708 - val_loss: 5159.0591\n",
      "Epoch 9806/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6105.3072 - val_loss: 5158.5967\n",
      "Epoch 9807/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6104.8448 - val_loss: 5158.1357\n",
      "Epoch 9808/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6104.3824 - val_loss: 5157.6709\n",
      "Epoch 9809/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6103.9190 - val_loss: 5157.2095\n",
      "Epoch 9810/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6103.4566 - val_loss: 5156.7461\n",
      "Epoch 9811/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6102.9937 - val_loss: 5156.2837\n",
      "Epoch 9812/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6102.5310 - val_loss: 5155.8218\n",
      "Epoch 9813/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6102.0687 - val_loss: 5155.3584\n",
      "Epoch 9814/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6101.6061 - val_loss: 5154.8940\n",
      "Epoch 9815/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6101.1430 - val_loss: 5154.4336\n",
      "Epoch 9816/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6100.6802 - val_loss: 5153.9702\n",
      "Epoch 9817/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6100.2178 - val_loss: 5153.5068\n",
      "Epoch 9818/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6099.7554 - val_loss: 5153.0454\n",
      "Epoch 9819/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6099.2926 - val_loss: 5152.5815\n",
      "Epoch 9820/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6098.8295 - val_loss: 5152.1191\n",
      "Epoch 9821/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6098.3666 - val_loss: 5151.6577\n",
      "Epoch 9822/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6097.9049 - val_loss: 5151.1943\n",
      "Epoch 9823/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6097.4418 - val_loss: 5150.7310\n",
      "Epoch 9824/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6096.9787 - val_loss: 5150.2690\n",
      "Epoch 9825/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6096.5161 - val_loss: 5149.8047\n",
      "Epoch 9826/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6096.0533 - val_loss: 5149.3428\n",
      "Epoch 9827/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6095.5907 - val_loss: 5148.8794\n",
      "Epoch 9828/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6095.1283 - val_loss: 5148.4170\n",
      "Epoch 9829/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6094.6650 - val_loss: 5147.9561\n",
      "Epoch 9830/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6094.2025 - val_loss: 5147.4917\n",
      "Epoch 9831/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6093.7403 - val_loss: 5147.0293\n",
      "Epoch 9832/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6093.2772 - val_loss: 5146.5664\n",
      "Epoch 9833/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6092.8148 - val_loss: 5146.1045\n",
      "Epoch 9834/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6092.3513 - val_loss: 5145.6411\n",
      "Epoch 9835/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6091.8892 - val_loss: 5145.1787\n",
      "Epoch 9836/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6091.4267 - val_loss: 5144.7153\n",
      "Epoch 9837/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6090.9634 - val_loss: 5144.2529\n",
      "Epoch 9838/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6090.5007 - val_loss: 5143.7915\n",
      "Epoch 9839/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6090.0384 - val_loss: 5143.3281\n",
      "Epoch 9840/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6089.5759 - val_loss: 5142.8662\n",
      "Epoch 9841/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6089.1128 - val_loss: 5142.4028\n",
      "Epoch 9842/10000\n",
      "750/750 [==============================] - 0s 139us/step - loss: 6088.6499 - val_loss: 5141.9390\n",
      "Epoch 9843/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6088.1872 - val_loss: 5141.4780\n",
      "Epoch 9844/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6087.7251 - val_loss: 5141.0146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9845/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6087.2618 - val_loss: 5140.5503\n",
      "Epoch 9846/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6086.7996 - val_loss: 5140.0898\n",
      "Epoch 9847/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6086.3363 - val_loss: 5139.6265\n",
      "Epoch 9848/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6085.8739 - val_loss: 5139.1631\n",
      "Epoch 9849/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6085.4110 - val_loss: 5138.7017\n",
      "Epoch 9850/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6084.9487 - val_loss: 5138.2373\n",
      "Epoch 9851/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6084.4863 - val_loss: 5137.7754\n",
      "Epoch 9852/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6084.0229 - val_loss: 5137.3120\n",
      "Epoch 9853/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6083.5602 - val_loss: 5136.8501\n",
      "Epoch 9854/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6083.0982 - val_loss: 5136.3867\n",
      "Epoch 9855/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6082.6346 - val_loss: 5135.9248\n",
      "Epoch 9856/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6082.1720 - val_loss: 5135.4624\n",
      "Epoch 9857/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6081.7097 - val_loss: 5134.9990\n",
      "Epoch 9858/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6081.2475 - val_loss: 5134.5356\n",
      "Epoch 9859/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6080.7842 - val_loss: 5134.0732\n",
      "Epoch 9860/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6080.3206 - val_loss: 5133.6118\n",
      "Epoch 9861/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6079.8588 - val_loss: 5133.1484\n",
      "Epoch 9862/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6079.3964 - val_loss: 5132.6855\n",
      "Epoch 9863/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6078.9337 - val_loss: 5132.2231\n",
      "Epoch 9864/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6078.4702 - val_loss: 5131.7607\n",
      "Epoch 9865/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6078.0077 - val_loss: 5131.2969\n",
      "Epoch 9866/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6077.5454 - val_loss: 5130.8350\n",
      "Epoch 9867/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6077.0828 - val_loss: 5130.3711\n",
      "Epoch 9868/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6076.6198 - val_loss: 5129.9092\n",
      "Epoch 9869/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6076.1572 - val_loss: 5129.4468\n",
      "Epoch 9870/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6075.6942 - val_loss: 5128.9839\n",
      "Epoch 9871/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6075.2322 - val_loss: 5128.5200\n",
      "Epoch 9872/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6074.7691 - val_loss: 5128.0591\n",
      "Epoch 9873/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6074.3057 - val_loss: 5127.5962\n",
      "Epoch 9874/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6073.8437 - val_loss: 5127.1333\n",
      "Epoch 9875/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6073.3809 - val_loss: 5126.6709\n",
      "Epoch 9876/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6072.9186 - val_loss: 5126.2065\n",
      "Epoch 9877/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6072.4554 - val_loss: 5125.7451\n",
      "Epoch 9878/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6071.9928 - val_loss: 5125.2827\n",
      "Epoch 9879/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6071.5304 - val_loss: 5124.8193\n",
      "Epoch 9880/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6071.0681 - val_loss: 5124.3579\n",
      "Epoch 9881/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6070.6048 - val_loss: 5123.8936\n",
      "Epoch 9882/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6070.1423 - val_loss: 5123.4316\n",
      "Epoch 9883/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6069.6792 - val_loss: 5122.9678\n",
      "Epoch 9884/10000\n",
      "750/750 [==============================] - ETA: 0s - loss: 6067.52 - 0s 118us/step - loss: 6069.2165 - val_loss: 5122.5059\n",
      "Epoch 9885/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6068.7543 - val_loss: 5122.0430\n",
      "Epoch 9886/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6068.2913 - val_loss: 5121.5811\n",
      "Epoch 9887/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6067.8280 - val_loss: 5121.1177\n",
      "Epoch 9888/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6067.3657 - val_loss: 5120.6553\n",
      "Epoch 9889/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6066.9032 - val_loss: 5120.1938\n",
      "Epoch 9890/10000\n",
      "750/750 [==============================] - 0s 114us/step - loss: 6066.4411 - val_loss: 5119.7295\n",
      "Epoch 9891/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6065.9769 - val_loss: 5119.2681\n",
      "Epoch 9892/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6065.5148 - val_loss: 5118.8047\n",
      "Epoch 9893/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6065.0528 - val_loss: 5118.3418\n",
      "Epoch 9894/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6064.5901 - val_loss: 5117.8794\n",
      "Epoch 9895/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6064.1264 - val_loss: 5117.4170\n",
      "Epoch 9896/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6063.6641 - val_loss: 5116.9536\n",
      "Epoch 9897/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6063.2017 - val_loss: 5116.4922\n",
      "Epoch 9898/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6062.7390 - val_loss: 5116.0278\n",
      "Epoch 9899/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6062.2759 - val_loss: 5115.5654\n",
      "Epoch 9900/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6061.8134 - val_loss: 5115.1030\n",
      "Epoch 9901/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6061.3505 - val_loss: 5114.6396\n",
      "Epoch 9902/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6060.8883 - val_loss: 5114.1777\n",
      "Epoch 9903/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6060.4253 - val_loss: 5113.7153\n",
      "Epoch 9904/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6059.9623 - val_loss: 5113.2529\n",
      "Epoch 9905/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6059.4998 - val_loss: 5112.7886\n",
      "Epoch 9906/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6059.0371 - val_loss: 5112.3271\n",
      "Epoch 9907/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6058.5750 - val_loss: 5111.8628\n",
      "Epoch 9908/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6058.1118 - val_loss: 5111.4014\n",
      "Epoch 9909/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6057.6490 - val_loss: 5110.9385\n",
      "Epoch 9910/10000\n",
      "750/750 [==============================] - 0s 153us/step - loss: 6057.1864 - val_loss: 5110.4751\n",
      "Epoch 9911/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6056.7238 - val_loss: 5110.0142\n",
      "Epoch 9912/10000\n",
      "750/750 [==============================] - 0s 137us/step - loss: 6056.2609 - val_loss: 5109.5503\n",
      "Epoch 9913/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6055.7982 - val_loss: 5109.0874\n",
      "Epoch 9914/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6055.3354 - val_loss: 5108.6265\n",
      "Epoch 9915/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6054.8728 - val_loss: 5108.1621\n",
      "Epoch 9916/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6054.4105 - val_loss: 5107.6997\n",
      "Epoch 9917/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6053.9478 - val_loss: 5107.2373\n",
      "Epoch 9918/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6053.4845 - val_loss: 5106.7739\n",
      "Epoch 9919/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6053.0221 - val_loss: 5106.3115\n",
      "Epoch 9920/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6052.5595 - val_loss: 5105.8496\n",
      "Epoch 9921/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6052.0970 - val_loss: 5105.3862\n",
      "Epoch 9922/10000\n",
      "750/750 [==============================] - 0s 131us/step - loss: 6051.6341 - val_loss: 5104.9233\n",
      "Epoch 9923/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6051.1710 - val_loss: 5104.4609\n",
      "Epoch 9924/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6050.7092 - val_loss: 5103.9966\n",
      "Epoch 9925/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6050.2456 - val_loss: 5103.5356\n",
      "Epoch 9926/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6049.7830 - val_loss: 5103.0728\n",
      "Epoch 9927/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6049.3200 - val_loss: 5102.6089\n",
      "Epoch 9928/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6048.8575 - val_loss: 5102.1479\n",
      "Epoch 9929/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6048.3960 - val_loss: 5101.6841\n",
      "Epoch 9930/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6047.9320 - val_loss: 5101.2212\n",
      "Epoch 9931/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6047.4694 - val_loss: 5100.7603\n",
      "Epoch 9932/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6047.0069 - val_loss: 5100.2959\n",
      "Epoch 9933/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6046.5442 - val_loss: 5099.8335\n",
      "Epoch 9934/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6046.0814 - val_loss: 5099.3706\n",
      "Epoch 9935/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6045.6191 - val_loss: 5098.9087\n",
      "Epoch 9936/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6045.1560 - val_loss: 5098.4458\n",
      "Epoch 9937/10000\n",
      "750/750 [==============================] - 0s 123us/step - loss: 6044.6934 - val_loss: 5097.9834\n",
      "Epoch 9938/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6044.2307 - val_loss: 5097.5200\n",
      "Epoch 9939/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6043.7679 - val_loss: 5097.0576\n",
      "Epoch 9940/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6043.3050 - val_loss: 5096.5962\n",
      "Epoch 9941/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6042.8429 - val_loss: 5096.1309\n",
      "Epoch 9942/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6042.3796 - val_loss: 5095.6704\n",
      "Epoch 9943/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6041.9174 - val_loss: 5095.2065\n",
      "Epoch 9944/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6041.4544 - val_loss: 5094.7437\n",
      "Epoch 9945/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6040.9917 - val_loss: 5094.2817\n",
      "Epoch 9946/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6040.5296 - val_loss: 5093.8188\n",
      "Epoch 9947/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6040.0668 - val_loss: 5093.3555\n",
      "Epoch 9948/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6039.6031 - val_loss: 5092.8936\n",
      "Epoch 9949/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6039.1410 - val_loss: 5092.4302\n",
      "Epoch 9950/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6038.6783 - val_loss: 5091.9678\n",
      "Epoch 9951/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6038.2162 - val_loss: 5091.5044\n",
      "Epoch 9952/10000\n",
      "750/750 [==============================] - 0s 116us/step - loss: 6037.7532 - val_loss: 5091.0420\n",
      "Epoch 9953/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6037.2894 - val_loss: 5090.5801\n",
      "Epoch 9954/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6036.8274 - val_loss: 5090.1172\n",
      "Epoch 9955/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6036.3650 - val_loss: 5089.6543\n",
      "Epoch 9956/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6035.9022 - val_loss: 5089.1914\n",
      "Epoch 9957/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6035.4397 - val_loss: 5088.7295\n",
      "Epoch 9958/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6034.9766 - val_loss: 5088.2646\n",
      "Epoch 9959/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6034.5133 - val_loss: 5087.8037\n",
      "Epoch 9960/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6034.0522 - val_loss: 5087.3403\n",
      "Epoch 9961/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6033.5885 - val_loss: 5086.8779\n",
      "Epoch 9962/10000\n",
      "750/750 [==============================] - 0s 133us/step - loss: 6033.1256 - val_loss: 5086.4165\n",
      "Epoch 9963/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6032.6634 - val_loss: 5085.9526\n",
      "Epoch 9964/10000\n",
      "750/750 [==============================] - 0s 115us/step - loss: 6032.2010 - val_loss: 5085.4902\n",
      "Epoch 9965/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6031.7378 - val_loss: 5085.0273\n",
      "Epoch 9966/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6031.2747 - val_loss: 5084.5649\n",
      "Epoch 9967/10000\n",
      "750/750 [==============================] - 0s 118us/step - loss: 6030.8122 - val_loss: 5084.1021\n",
      "Epoch 9968/10000\n",
      "750/750 [==============================] - 0s 113us/step - loss: 6030.3499 - val_loss: 5083.6396\n",
      "Epoch 9969/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6029.8871 - val_loss: 5083.1753\n",
      "Epoch 9970/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6029.4239 - val_loss: 5082.7139\n",
      "Epoch 9971/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6028.9612 - val_loss: 5082.2524\n",
      "Epoch 9972/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6028.4991 - val_loss: 5081.7881\n",
      "Epoch 9973/10000\n",
      "750/750 [==============================] - 0s 128us/step - loss: 6028.0364 - val_loss: 5081.3267\n",
      "Epoch 9974/10000\n",
      "750/750 [==============================] - 0s 121us/step - loss: 6027.5737 - val_loss: 5080.8638\n",
      "Epoch 9975/10000\n",
      "750/750 [==============================] - 0s 141us/step - loss: 6027.1113 - val_loss: 5080.3989\n",
      "Epoch 9976/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6026.6475 - val_loss: 5079.9370\n",
      "Epoch 9977/10000\n",
      "750/750 [==============================] - 0s 129us/step - loss: 6026.1855 - val_loss: 5079.4756\n",
      "Epoch 9978/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6025.7231 - val_loss: 5079.0117\n",
      "Epoch 9979/10000\n",
      "750/750 [==============================] - 0s 134us/step - loss: 6025.2600 - val_loss: 5078.5503\n",
      "Epoch 9980/10000\n",
      "750/750 [==============================] - 0s 144us/step - loss: 6024.7972 - val_loss: 5078.0874\n",
      "Epoch 9981/10000\n",
      "750/750 [==============================] - 0s 142us/step - loss: 6024.3347 - val_loss: 5077.6240\n",
      "Epoch 9982/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6023.8720 - val_loss: 5077.1621\n",
      "Epoch 9983/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6023.4094 - val_loss: 5076.6982\n",
      "Epoch 9984/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6022.9459 - val_loss: 5076.2363\n",
      "Epoch 9985/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6022.4841 - val_loss: 5075.7729\n",
      "Epoch 9986/10000\n",
      "750/750 [==============================] - 0s 127us/step - loss: 6022.0215 - val_loss: 5075.3091\n",
      "Epoch 9987/10000\n",
      "750/750 [==============================] - 0s 125us/step - loss: 6021.5582 - val_loss: 5074.8477\n",
      "Epoch 9988/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 0s 126us/step - loss: 6021.0954 - val_loss: 5074.3857\n",
      "Epoch 9989/10000\n",
      "750/750 [==============================] - 0s 132us/step - loss: 6020.6329 - val_loss: 5073.9224\n",
      "Epoch 9990/10000\n",
      "750/750 [==============================] - 0s 140us/step - loss: 6020.1704 - val_loss: 5073.4600\n",
      "Epoch 9991/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6019.7078 - val_loss: 5072.9980\n",
      "Epoch 9992/10000\n",
      "750/750 [==============================] - 0s 130us/step - loss: 6019.2450 - val_loss: 5072.5342\n",
      "Epoch 9993/10000\n",
      "750/750 [==============================] - 0s 117us/step - loss: 6018.7818 - val_loss: 5072.0728\n",
      "Epoch 9994/10000\n",
      "750/750 [==============================] - 0s 120us/step - loss: 6018.3194 - val_loss: 5071.6089\n",
      "Epoch 9995/10000\n",
      "750/750 [==============================] - 0s 122us/step - loss: 6017.8571 - val_loss: 5071.1450\n",
      "Epoch 9996/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6017.3942 - val_loss: 5070.6841\n",
      "Epoch 9997/10000\n",
      "750/750 [==============================] - 0s 136us/step - loss: 6016.9309 - val_loss: 5070.2212\n",
      "Epoch 9998/10000\n",
      "750/750 [==============================] - 0s 126us/step - loss: 6016.4684 - val_loss: 5069.7573\n",
      "Epoch 9999/10000\n",
      "750/750 [==============================] - 0s 138us/step - loss: 6016.0060 - val_loss: 5069.2959\n",
      "Epoch 10000/10000\n",
      "750/750 [==============================] - 0s 124us/step - loss: 6015.5433 - val_loss: 5068.8315\n"
     ]
    }
   ],
   "source": [
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(all_feature, 1, 1)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,np.newaxis]\n",
    "Y_val = Y_val[:,np.newaxis]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(250, input_length=X_train .shape[1], input_dim=X_train .shape[2],return_sequences=True))\n",
    "model.add(LSTM(100,return_sequences=True))\n",
    "# output shape: (1, 1)\n",
    "model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
    "model.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=10000, batch_size=128, validation_data=(X_val, Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10663.096989583333,\n",
       " 10662.495322916666,\n",
       " 10661.778057291667,\n",
       " 10660.924565104167,\n",
       " 10659.942489583333,\n",
       " 10658.853541666667,\n",
       " 10657.6985078125,\n",
       " 10656.503859375,\n",
       " 10655.304,\n",
       " 10654.124596354166,\n",
       " 10652.979708333334,\n",
       " 10651.880401041666,\n",
       " 10650.831057291667,\n",
       " 10649.831723958334,\n",
       " 10648.8771640625,\n",
       " 10647.968015625,\n",
       " 10647.0974296875,\n",
       " 10646.260354166667,\n",
       " 10645.454216145834,\n",
       " 10644.673276041667,\n",
       " 10643.915403645833,\n",
       " 10643.1769453125,\n",
       " 10642.456666666667,\n",
       " 10641.751377604167,\n",
       " 10641.0634296875,\n",
       " 10640.388184895834,\n",
       " 10639.7238203125,\n",
       " 10639.070674479166,\n",
       " 10638.427291666667,\n",
       " 10637.792234375,\n",
       " 10637.165013020833,\n",
       " 10636.546497395833,\n",
       " 10635.93346875,\n",
       " 10635.328002604167,\n",
       " 10634.727135416666,\n",
       " 10634.132981770834,\n",
       " 10633.542736979167,\n",
       " 10632.957598958334,\n",
       " 10632.3773046875,\n",
       " 10631.800677083333,\n",
       " 10631.228234375,\n",
       " 10630.659151041667,\n",
       " 10630.0931875,\n",
       " 10629.5313515625,\n",
       " 10628.971809895833,\n",
       " 10628.41515625,\n",
       " 10627.861497395834,\n",
       " 10627.310223958333,\n",
       " 10626.762171875,\n",
       " 10626.215833333334,\n",
       " 10625.672109375,\n",
       " 10625.130505208333,\n",
       " 10624.5904609375,\n",
       " 10624.052692708334,\n",
       " 10623.516716145834,\n",
       " 10622.982356770834,\n",
       " 10622.450315104166,\n",
       " 10621.919322916667,\n",
       " 10621.3900234375,\n",
       " 10620.862630208334,\n",
       " 10620.33621875,\n",
       " 10619.810880208333,\n",
       " 10619.2875078125,\n",
       " 10618.765494791667,\n",
       " 10618.2444921875,\n",
       " 10617.7248828125,\n",
       " 10617.205986979166,\n",
       " 10616.688653645833,\n",
       " 10616.1721171875,\n",
       " 10615.656203125,\n",
       " 10615.1423046875,\n",
       " 10614.6286015625,\n",
       " 10614.116361979166,\n",
       " 10613.605026041667,\n",
       " 10613.094028645834,\n",
       " 10612.584447916666,\n",
       " 10612.07534375,\n",
       " 10611.567390625,\n",
       " 10611.060109375,\n",
       " 10610.553666666667,\n",
       " 10610.047515625,\n",
       " 10609.5424375,\n",
       " 10609.038171875,\n",
       " 10608.534598958333,\n",
       " 10608.031455729166,\n",
       " 10607.528903645833,\n",
       " 10607.027544270833,\n",
       " 10606.526127604167,\n",
       " 10606.025359375,\n",
       " 10605.525096354168,\n",
       " 10605.026559895834,\n",
       " 10604.5273828125,\n",
       " 10604.028872395833,\n",
       " 10603.531403645833,\n",
       " 10603.034104166667,\n",
       " 10602.537317708333,\n",
       " 10602.040942708334,\n",
       " 10601.545283854166,\n",
       " 10601.049861979167,\n",
       " 10600.5553359375,\n",
       " 10600.060828125,\n",
       " 10599.566674479167,\n",
       " 10599.073513020834,\n",
       " 10598.580182291667,\n",
       " 10598.087158854167,\n",
       " 10597.595052083334,\n",
       " 10597.103869791667,\n",
       " 10596.612013020833,\n",
       " 10596.120770833333,\n",
       " 10595.629872395833,\n",
       " 10595.139390625,\n",
       " 10594.649390625,\n",
       " 10594.159354166666,\n",
       " 10593.670328125,\n",
       " 10593.181208333333,\n",
       " 10592.692401041666,\n",
       " 10592.204059895834,\n",
       " 10591.715783854166,\n",
       " 10591.228510416666,\n",
       " 10590.740802083334,\n",
       " 10590.253643229167,\n",
       " 10589.766841145833,\n",
       " 10589.2799375,\n",
       " 10588.7940703125,\n",
       " 10588.3080390625,\n",
       " 10587.822127604166,\n",
       " 10587.336627604167,\n",
       " 10586.85146875,\n",
       " 10586.366270833334,\n",
       " 10585.882208333333,\n",
       " 10585.397276041667,\n",
       " 10584.912776041667,\n",
       " 10584.4293828125,\n",
       " 10583.945356770833,\n",
       " 10583.461684895834,\n",
       " 10582.9781171875,\n",
       " 10582.495052083334,\n",
       " 10582.012596354167,\n",
       " 10581.529890625,\n",
       " 10581.0475234375,\n",
       " 10580.565151041666,\n",
       " 10580.083265625,\n",
       " 10579.601513020832,\n",
       " 10579.119869791666,\n",
       " 10578.638877604166,\n",
       " 10578.157200520833,\n",
       " 10577.676713541667,\n",
       " 10577.195510416666,\n",
       " 10576.715143229167,\n",
       " 10576.2348984375,\n",
       " 10575.7544296875,\n",
       " 10575.274372395834,\n",
       " 10574.7943984375,\n",
       " 10574.315018229167,\n",
       " 10573.835135416666,\n",
       " 10573.355203125,\n",
       " 10572.8763359375,\n",
       " 10572.3976171875,\n",
       " 10571.9183046875,\n",
       " 10571.439729166666,\n",
       " 10570.9608515625,\n",
       " 10570.4825625,\n",
       " 10570.004216145833,\n",
       " 10569.526166666667,\n",
       " 10569.0476328125,\n",
       " 10568.570221354166,\n",
       " 10568.092721354167,\n",
       " 10567.614979166667,\n",
       " 10567.137734375,\n",
       " 10566.660453125,\n",
       " 10566.183020833334,\n",
       " 10565.7061171875,\n",
       " 10565.229567708333,\n",
       " 10564.752875,\n",
       " 10564.275934895833,\n",
       " 10563.799890625,\n",
       " 10563.32315625,\n",
       " 10562.846619791666,\n",
       " 10562.370703125,\n",
       " 10561.894705729166,\n",
       " 10561.4189296875,\n",
       " 10560.9429765625,\n",
       " 10560.4671640625,\n",
       " 10559.992020833333,\n",
       " 10559.515984375,\n",
       " 10559.041145833333,\n",
       " 10558.566184895833,\n",
       " 10558.091234375,\n",
       " 10557.616190104167,\n",
       " 10557.1409453125,\n",
       " 10556.665942708334,\n",
       " 10556.191815104166,\n",
       " 10555.716942708334,\n",
       " 10555.242721354167,\n",
       " 10554.768489583334,\n",
       " 10554.294216145834,\n",
       " 10553.819630208332,\n",
       " 10553.346057291667,\n",
       " 10552.872138020834,\n",
       " 10552.398536458333,\n",
       " 10551.923953125,\n",
       " 10551.4506015625,\n",
       " 10550.977377604167,\n",
       " 10550.503296875,\n",
       " 10550.0301875,\n",
       " 10549.556846354166,\n",
       " 10549.0836796875,\n",
       " 10548.610385416667,\n",
       " 10548.137192708333,\n",
       " 10547.664315104166,\n",
       " 10547.191625,\n",
       " 10546.718622395832,\n",
       " 10546.245979166666,\n",
       " 10545.773567708333,\n",
       " 10545.300979166666,\n",
       " 10544.828447916667,\n",
       " 10544.356450520834,\n",
       " 10543.884002604167,\n",
       " 10543.411903645834,\n",
       " 10542.939455729167,\n",
       " 10542.467161458333,\n",
       " 10541.995703125,\n",
       " 10541.523393229167,\n",
       " 10541.051528645834,\n",
       " 10540.579453125,\n",
       " 10540.107609375,\n",
       " 10539.636213541668,\n",
       " 10539.164713541666,\n",
       " 10538.693033854166,\n",
       " 10538.2211640625,\n",
       " 10537.750119791666,\n",
       " 10537.278828125,\n",
       " 10536.80784375,\n",
       " 10536.33628125,\n",
       " 10535.865286458333,\n",
       " 10535.394377604167,\n",
       " 10534.923182291666,\n",
       " 10534.452005208334,\n",
       " 10533.981473958333,\n",
       " 10533.510731770833,\n",
       " 10533.039971354166,\n",
       " 10532.569213541667,\n",
       " 10532.098895833333,\n",
       " 10531.628229166667,\n",
       " 10531.157807291667,\n",
       " 10530.687166666667,\n",
       " 10530.216677083334,\n",
       " 10529.746346354166,\n",
       " 10529.276291666667,\n",
       " 10528.8063984375,\n",
       " 10528.335598958334,\n",
       " 10527.8658359375,\n",
       " 10527.3956171875,\n",
       " 10526.92553125,\n",
       " 10526.4558671875,\n",
       " 10525.985640625,\n",
       " 10525.515963541666,\n",
       " 10525.046635416667,\n",
       " 10524.576885416667,\n",
       " 10524.106955729167,\n",
       " 10523.637270833333,\n",
       " 10523.167833333333,\n",
       " 10522.698174479166,\n",
       " 10522.228708333334,\n",
       " 10521.759125,\n",
       " 10521.290536458333,\n",
       " 10520.82046875,\n",
       " 10520.351369791666,\n",
       " 10519.8820546875,\n",
       " 10519.412596354166,\n",
       " 10518.943888020833,\n",
       " 10518.474533854167,\n",
       " 10518.005635416666,\n",
       " 10517.536268229167,\n",
       " 10517.067661458334,\n",
       " 10516.598466145833,\n",
       " 10516.1296328125,\n",
       " 10515.661033854167,\n",
       " 10515.192106770834,\n",
       " 10514.723671875,\n",
       " 10514.254708333334,\n",
       " 10513.785989583334,\n",
       " 10513.317450520834,\n",
       " 10512.848958333334,\n",
       " 10512.380044270833,\n",
       " 10511.911765625,\n",
       " 10511.443359375,\n",
       " 10510.974890625,\n",
       " 10510.5059296875,\n",
       " 10510.0379453125,\n",
       " 10509.569549479167,\n",
       " 10509.101317708333,\n",
       " 10508.63296875,\n",
       " 10508.165361979167,\n",
       " 10507.696971354168,\n",
       " 10507.228841145834,\n",
       " 10506.760505208333,\n",
       " 10506.292447916667,\n",
       " 10505.825127604166,\n",
       " 10505.357395833333,\n",
       " 10504.889236979167,\n",
       " 10504.42121875,\n",
       " 10503.952770833333,\n",
       " 10503.4847890625,\n",
       " 10503.017190104167,\n",
       " 10502.549692708333,\n",
       " 10502.081447916667,\n",
       " 10501.614364583333,\n",
       " 10501.146473958333,\n",
       " 10500.6789296875,\n",
       " 10500.211549479167,\n",
       " 10499.744341145833,\n",
       " 10499.276432291666,\n",
       " 10498.808778645833,\n",
       " 10498.3417109375,\n",
       " 10497.874015625,\n",
       " 10497.406067708333,\n",
       " 10496.938778645834,\n",
       " 10496.47184375,\n",
       " 10496.004348958333,\n",
       " 10495.537174479166,\n",
       " 10495.0701171875,\n",
       " 10494.602875,\n",
       " 10494.135809895834,\n",
       " 10493.668296875,\n",
       " 10493.2009375,\n",
       " 10492.733669270834,\n",
       " 10492.2670078125,\n",
       " 10491.799942708334,\n",
       " 10491.3329765625,\n",
       " 10490.8658828125,\n",
       " 10490.399,\n",
       " 10489.932098958334,\n",
       " 10489.465013020834,\n",
       " 10488.998013020833,\n",
       " 10488.531130208334,\n",
       " 10488.0645625,\n",
       " 10487.597614583334,\n",
       " 10487.131114583333,\n",
       " 10486.6643671875,\n",
       " 10486.197341145833,\n",
       " 10485.730565104166,\n",
       " 10485.264127604167,\n",
       " 10484.7969765625,\n",
       " 10484.3307109375,\n",
       " 10483.864,\n",
       " 10483.39753125,\n",
       " 10482.9308125,\n",
       " 10482.464432291667,\n",
       " 10481.998174479168,\n",
       " 10481.531518229167,\n",
       " 10481.065317708333,\n",
       " 10480.598544270833,\n",
       " 10480.132041666666,\n",
       " 10479.665453125,\n",
       " 10479.199513020832,\n",
       " 10478.733385416666,\n",
       " 10478.266940104166,\n",
       " 10477.8004609375,\n",
       " 10477.333973958333,\n",
       " 10476.8678828125,\n",
       " 10476.401877604167,\n",
       " 10475.935807291668,\n",
       " 10475.4696875,\n",
       " 10475.003174479167,\n",
       " 10474.536828125,\n",
       " 10474.070989583333,\n",
       " 10473.604736979167,\n",
       " 10473.138856770833,\n",
       " 10472.673041666667,\n",
       " 10472.206276041667,\n",
       " 10471.7406015625,\n",
       " 10471.274854166666,\n",
       " 10470.808716145833,\n",
       " 10470.342807291667,\n",
       " 10469.876815104166,\n",
       " 10469.410640625,\n",
       " 10468.944325520833,\n",
       " 10468.479010416666,\n",
       " 10468.013567708333,\n",
       " 10467.5475,\n",
       " 10467.081388020833,\n",
       " 10466.614940104168,\n",
       " 10466.149859375,\n",
       " 10465.684442708332,\n",
       " 10465.2184921875,\n",
       " 10464.752565104167,\n",
       " 10464.286348958334,\n",
       " 10463.8215,\n",
       " 10463.355731770833,\n",
       " 10462.890096354167,\n",
       " 10462.424278645833,\n",
       " 10461.957799479167,\n",
       " 10461.493221354167,\n",
       " 10461.027669270834,\n",
       " 10460.5618984375,\n",
       " 10460.096122395833,\n",
       " 10459.6306640625,\n",
       " 10459.1651484375,\n",
       " 10458.699861979167,\n",
       " 10458.2339765625,\n",
       " 10457.768638020832,\n",
       " 10457.303611979167,\n",
       " 10456.8380078125,\n",
       " 10456.372533854166,\n",
       " 10455.906950520834,\n",
       " 10455.441567708333,\n",
       " 10454.976348958333,\n",
       " 10454.511268229167,\n",
       " 10454.045557291667,\n",
       " 10453.580174479166,\n",
       " 10453.115143229166,\n",
       " 10452.6500390625,\n",
       " 10452.184596354167,\n",
       " 10451.718640625,\n",
       " 10451.253705729167,\n",
       " 10450.788682291666,\n",
       " 10450.323614583333,\n",
       " 10449.858106770833,\n",
       " 10449.392963541666,\n",
       " 10448.927841145833,\n",
       " 10448.463377604166,\n",
       " 10447.997216145834,\n",
       " 10447.531950520834,\n",
       " 10447.0671171875,\n",
       " 10446.602403645833,\n",
       " 10446.137046875,\n",
       " 10445.6715625,\n",
       " 10445.2066796875,\n",
       " 10444.741901041667,\n",
       " 10444.276606770833,\n",
       " 10443.811442708333,\n",
       " 10443.346372395834,\n",
       " 10442.881604166667,\n",
       " 10442.417044270833,\n",
       " 10441.951169270833,\n",
       " 10441.486364583334,\n",
       " 10441.02221875,\n",
       " 10440.556791666666,\n",
       " 10440.0914609375,\n",
       " 10439.626643229167,\n",
       " 10439.161942708333,\n",
       " 10438.697140625,\n",
       " 10438.2320234375,\n",
       " 10437.767325520834,\n",
       " 10437.3024375,\n",
       " 10436.8381171875,\n",
       " 10436.372645833333,\n",
       " 10435.907783854167,\n",
       " 10435.443721354166,\n",
       " 10434.978200520833,\n",
       " 10434.513125,\n",
       " 10434.048515625,\n",
       " 10433.584174479167,\n",
       " 10433.118973958333,\n",
       " 10432.654221354167,\n",
       " 10432.1900234375,\n",
       " 10431.725221354167,\n",
       " 10431.2600078125,\n",
       " 10430.795466145833,\n",
       " 10430.330958333334,\n",
       " 10429.8663203125,\n",
       " 10429.4016015625,\n",
       " 10428.937174479166,\n",
       " 10428.4727265625,\n",
       " 10428.007627604167,\n",
       " 10427.542791666667,\n",
       " 10427.078401041666,\n",
       " 10426.613877604166,\n",
       " 10426.149518229167,\n",
       " 10425.684390625,\n",
       " 10425.2201796875,\n",
       " 10424.755984375,\n",
       " 10424.2906796875,\n",
       " 10423.826286458334,\n",
       " 10423.362356770833,\n",
       " 10422.897966145832,\n",
       " 10422.432510416667,\n",
       " 10421.968341145834,\n",
       " 10421.504177083334,\n",
       " 10421.0386328125,\n",
       " 10420.574643229167,\n",
       " 10420.110690104166,\n",
       " 10419.646455729167,\n",
       " 10419.181341145833,\n",
       " 10418.7171171875,\n",
       " 10418.252890625,\n",
       " 10417.788109375,\n",
       " 10417.324213541666,\n",
       " 10416.859127604166,\n",
       " 10416.395486979167,\n",
       " 10415.930221354167,\n",
       " 10415.466104166666,\n",
       " 10415.002458333332,\n",
       " 10414.537822916667,\n",
       " 10414.073130208333,\n",
       " 10413.6090703125,\n",
       " 10413.144888020834,\n",
       " 10412.679786458333,\n",
       " 10412.215846354167,\n",
       " 10411.752182291666,\n",
       " 10411.287065104167,\n",
       " 10410.822627604166,\n",
       " 10410.3586328125,\n",
       " 10409.894807291666,\n",
       " 10409.4296484375,\n",
       " 10408.965958333334,\n",
       " 10408.501893229166,\n",
       " 10408.037122395834,\n",
       " 10407.573442708333,\n",
       " 10407.109106770833,\n",
       " 10406.6451796875,\n",
       " 10406.180203125,\n",
       " 10405.715950520833,\n",
       " 10405.2526171875,\n",
       " 10404.7876328125,\n",
       " 10404.323375,\n",
       " 10403.8598515625,\n",
       " 10403.39534375,\n",
       " 10402.9309375,\n",
       " 10402.467234375,\n",
       " 10402.003380208333,\n",
       " 10401.538619791667,\n",
       " 10401.074815104166,\n",
       " 10400.610645833332,\n",
       " 10400.1462890625,\n",
       " 10399.682020833334,\n",
       " 10399.218893229166,\n",
       " 10398.754046875,\n",
       " 10398.289674479167,\n",
       " 10397.826197916667,\n",
       " 10397.362296875,\n",
       " 10396.897138020833,\n",
       " 10396.4340546875,\n",
       " 10395.9698984375,\n",
       " 10395.505473958334,\n",
       " 10395.041541666667,\n",
       " 10394.578010416666,\n",
       " 10394.113296875,\n",
       " 10393.649322916666,\n",
       " 10393.186231770833,\n",
       " 10392.721958333334,\n",
       " 10392.257526041667,\n",
       " 10391.793557291667,\n",
       " 10391.330143229166,\n",
       " 10390.865046875,\n",
       " 10390.4013671875,\n",
       " 10389.937953125,\n",
       " 10389.473309895833,\n",
       " 10389.009682291668,\n",
       " 10388.546098958333,\n",
       " 10388.081489583334,\n",
       " 10387.617674479166,\n",
       " 10387.154283854166,\n",
       " 10386.689609375,\n",
       " 10386.225763020833,\n",
       " 10385.762526041666,\n",
       " 10385.298481770833,\n",
       " 10384.834018229167,\n",
       " 10384.370809895834,\n",
       " 10383.9070078125,\n",
       " 10383.442026041666,\n",
       " 10382.9788203125,\n",
       " 10382.515052083334,\n",
       " 10382.050361979167,\n",
       " 10381.587098958333,\n",
       " 10381.123638020834,\n",
       " 10380.659598958333,\n",
       " 10380.1952265625,\n",
       " 10379.732296875,\n",
       " 10379.267768229167,\n",
       " 10378.803708333333,\n",
       " 10378.340848958333,\n",
       " 10377.876395833333,\n",
       " 10377.4123984375,\n",
       " 10376.948958333332,\n",
       " 10376.485234375,\n",
       " 10376.0210546875,\n",
       " 10375.55778125,\n",
       " 10375.093828125,\n",
       " 10374.629541666667,\n",
       " 10374.166385416667,\n",
       " 10373.702729166667,\n",
       " 10373.238265625,\n",
       " 10372.774763020834,\n",
       " 10372.311291666667,\n",
       " 10371.8469375,\n",
       " 10371.383442708333,\n",
       " 10370.91984375,\n",
       " 10370.4553515625,\n",
       " 10369.9922265625,\n",
       " 10369.528723958334,\n",
       " 10369.06471875,\n",
       " 10368.6011875,\n",
       " 10368.137778645832,\n",
       " 10367.673567708332,\n",
       " 10367.209567708333,\n",
       " 10366.74634375,\n",
       " 10366.282458333333,\n",
       " 10365.818721354166,\n",
       " 10365.355140625,\n",
       " 10364.891401041667,\n",
       " 10364.427481770834,\n",
       " 10363.96415625,\n",
       " 10363.500536458334,\n",
       " 10363.036466145833,\n",
       " 10362.57353125,\n",
       " 10362.108955729167,\n",
       " 10361.645440104166,\n",
       " 10361.182385416667,\n",
       " 10360.7181640625,\n",
       " 10360.254143229167,\n",
       " 10359.790979166666,\n",
       " 10359.327700520833,\n",
       " 10358.8636015625,\n",
       " 10358.400098958333,\n",
       " 10357.936614583334,\n",
       " 10357.472627604167,\n",
       " 10357.009598958333,\n",
       " 10356.545890625,\n",
       " 10356.081875,\n",
       " 10355.619,\n",
       " 10355.1551015625,\n",
       " 10354.691130208334,\n",
       " 10354.22815625,\n",
       " 10353.764526041667,\n",
       " 10353.3005703125,\n",
       " 10352.836799479166,\n",
       " 10352.3737734375,\n",
       " 10351.909341145833,\n",
       " 10351.446153645833,\n",
       " 10350.9829921875,\n",
       " 10350.5190390625,\n",
       " 10350.056130208333,\n",
       " 10349.592520833334,\n",
       " 10349.128125,\n",
       " 10348.665018229167,\n",
       " 10348.2015234375,\n",
       " 10347.737299479166,\n",
       " 10347.274375,\n",
       " 10346.810619791666,\n",
       " 10346.346557291667,\n",
       " 10345.883705729168,\n",
       " 10345.420013020834,\n",
       " 10344.9559921875,\n",
       " 10344.492901041667,\n",
       " 10344.02928125,\n",
       " 10343.565317708333,\n",
       " 10343.102174479167,\n",
       " 10342.638875,\n",
       " 10342.174838541667,\n",
       " 10341.712205729167,\n",
       " 10341.2483828125,\n",
       " 10340.784328125,\n",
       " 10340.321513020834,\n",
       " 10339.857817708333,\n",
       " 10339.3939375,\n",
       " 10338.931348958333,\n",
       " 10338.466768229167,\n",
       " 10338.003619791667,\n",
       " 10337.5409296875,\n",
       " 10337.076557291666,\n",
       " 10336.613153645832,\n",
       " 10336.1501875,\n",
       " 10335.686802083334,\n",
       " 10335.2226484375,\n",
       " 10334.760643229167,\n",
       " 10334.295734375,\n",
       " 10333.832645833334,\n",
       " 10333.370197916667,\n",
       " 10332.9059765625,\n",
       " 10332.4423984375,\n",
       " 10331.979375,\n",
       " 10331.515541666668,\n",
       " 10331.052520833333,\n",
       " 10330.5896171875,\n",
       " 10330.124885416666,\n",
       " 10329.6623984375,\n",
       " 10329.199229166667,\n",
       " 10328.735143229167,\n",
       " 10328.272390625,\n",
       " 10327.808609375,\n",
       " 10327.344455729166,\n",
       " 10326.881979166666,\n",
       " 10326.419057291667,\n",
       " 10325.954973958333,\n",
       " 10325.492361979166,\n",
       " 10325.028052083333,\n",
       " 10324.564330729167,\n",
       " 10324.1018828125,\n",
       " 10323.638364583334,\n",
       " 10323.1743046875,\n",
       " 10322.712236979167,\n",
       " 10322.247861979167,\n",
       " 10321.7842109375,\n",
       " 10321.321630208333,\n",
       " 10320.857518229166,\n",
       " 10320.393822916667,\n",
       " 10319.931734375,\n",
       " 10319.4676015625,\n",
       " 10319.004104166666,\n",
       " 10318.541330729167,\n",
       " 10318.0771640625,\n",
       " 10317.614302083333,\n",
       " 10317.151067708333,\n",
       " 10316.6871796875,\n",
       " 10316.2239921875,\n",
       " 10315.761330729167,\n",
       " 10315.296606770833,\n",
       " 10314.833877604167,\n",
       " 10314.3709296875,\n",
       " 10313.906643229167,\n",
       " 10313.444153645833,\n",
       " 10312.980638020834,\n",
       " 10312.516533854166,\n",
       " 10312.054020833333,\n",
       " 10311.590713541667,\n",
       " 10311.126322916667,\n",
       " 10310.6639453125,\n",
       " 10310.200263020834,\n",
       " 10309.736604166666,\n",
       " 10309.27403125,\n",
       " 10308.810208333334,\n",
       " 10308.3466015625,\n",
       " 10307.8840546875,\n",
       " 10307.419809895833,\n",
       " 10306.956901041667,\n",
       " 10306.493807291667,\n",
       " 10306.029734375,\n",
       " 10305.566778645833,\n",
       " 10305.103536458333,\n",
       " 10304.639799479166,\n",
       " 10304.176942708333,\n",
       " 10303.7136484375,\n",
       " 10303.249401041667,\n",
       " 10302.7868359375,\n",
       " 10302.323669270834,\n",
       " 10301.859682291666,\n",
       " 10301.39703125,\n",
       " 10300.933546875,\n",
       " 10300.469973958334,\n",
       " 10300.007703125,\n",
       " 10299.5433046875,\n",
       " 10299.080143229166,\n",
       " 10298.617619791667,\n",
       " 10298.153609375,\n",
       " 10297.690169270832,\n",
       " 10297.2278359375,\n",
       " 10296.7638203125,\n",
       " 10296.300872395834,\n",
       " 10295.838119791666,\n",
       " 10295.373888020833,\n",
       " 10294.91146875,\n",
       " 10294.4485,\n",
       " 10293.984447916666,\n",
       " 10293.521609375,\n",
       " 10293.058046875,\n",
       " 10292.594830729167,\n",
       " 10292.132700520833,\n",
       " 10291.668489583333,\n",
       " 10291.205356770834,\n",
       " 10290.742817708333,\n",
       " 10290.2788046875,\n",
       " 10289.816098958334,\n",
       " 10289.3535546875,\n",
       " 10288.8891328125,\n",
       " 10288.426895833334,\n",
       " 10287.963783854168,\n",
       " 10287.499278645833,\n",
       " 10287.037140625,\n",
       " 10286.573794270833,\n",
       " 10286.109828125,\n",
       " 10285.647541666667,\n",
       " 10285.1843671875,\n",
       " 10284.721158854167,\n",
       " 10284.258161458334,\n",
       " 10283.794028645832,\n",
       " 10283.331625,\n",
       " 10282.868658854166,\n",
       " 10282.404661458333,\n",
       " 10281.942494791667,\n",
       " 10281.478833333333,\n",
       " 10281.0151875,\n",
       " 10280.553640625,\n",
       " 10280.089609375,\n",
       " 10279.6260546875,\n",
       " 10279.163703125,\n",
       " 10278.699479166668,\n",
       " 10278.236403645833,\n",
       " 10277.774111979166,\n",
       " 10277.30984375,\n",
       " 10276.847557291667,\n",
       " 10276.384434895834,\n",
       " 10275.920565104167,\n",
       " 10275.458125,\n",
       " 10274.994666666667,\n",
       " 10274.530841145834,\n",
       " 10274.0691796875,\n",
       " 10273.604786458332,\n",
       " 10273.141893229167,\n",
       " 10272.679518229166,\n",
       " 10272.215145833334,\n",
       " 10271.752364583333,\n",
       " 10271.290192708333,\n",
       " 10270.825765625,\n",
       " 10270.363395833334,\n",
       " 10269.9001328125,\n",
       " 10269.436265625,\n",
       " 10268.973671875,\n",
       " 10268.510184895833,\n",
       " 10268.0464765625,\n",
       " 10267.584190104166,\n",
       " 10267.120825520833,\n",
       " 10266.657447916667,\n",
       " 10266.1951171875,\n",
       " 10265.730981770834,\n",
       " 10265.268104166667,\n",
       " 10264.805669270834,\n",
       " 10264.341981770833,\n",
       " 10263.878981770833,\n",
       " 10263.41596875,\n",
       " 10262.951815104167,\n",
       " 10262.4900703125,\n",
       " 10262.025989583333,\n",
       " 10261.5626875,\n",
       " 10261.100786458333,\n",
       " 10260.6361953125,\n",
       " 10260.1733203125,\n",
       " 10259.711119791667,\n",
       " 10259.247151041667,\n",
       " 10258.784041666666,\n",
       " 10258.321380208334,\n",
       " 10257.857359375,\n",
       " 10257.394947916666,\n",
       " 10256.931216145833,\n",
       " 10256.46746875,\n",
       " 10256.005838541667,\n",
       " 10255.542161458334,\n",
       " 10255.078877604166,\n",
       " 10254.6161015625,\n",
       " 10254.152122395833,\n",
       " 10253.6892890625,\n",
       " 10253.22628125,\n",
       " 10252.762473958333,\n",
       " 10252.3008671875,\n",
       " 10251.8368984375,\n",
       " 10251.3730390625,\n",
       " 10250.910994791666,\n",
       " 10250.447716145833,\n",
       " 10249.983729166666,\n",
       " 10249.5214921875,\n",
       " 10249.0578203125,\n",
       " 10248.594388020834,\n",
       " 10248.1323046875,\n",
       " 10247.667830729166,\n",
       " 10247.205364583333,\n",
       " 10246.742286458333,\n",
       " 10246.278291666667,\n",
       " 10245.815354166667,\n",
       " 10245.352692708333,\n",
       " 10244.888955729166,\n",
       " 10244.426817708334,\n",
       " 10243.963359375,\n",
       " 10243.499559895834,\n",
       " 10243.037549479166,\n",
       " 10242.573700520834,\n",
       " 10242.110565104167,\n",
       " 10241.647661458333,\n",
       " 10241.183317708334,\n",
       " 10240.721135416667,\n",
       " 10240.258265625,\n",
       " 10239.7943125,\n",
       " 10239.331794270833,\n",
       " 10238.868466145834,\n",
       " 10238.404494791666,\n",
       " 10237.942502604166,\n",
       " 10237.47871875,\n",
       " 10237.015315104167,\n",
       " 10236.5530234375,\n",
       " 10236.0889921875,\n",
       " 10235.626375,\n",
       " 10235.163796875,\n",
       " 10234.699317708333,\n",
       " 10234.237096354167,\n",
       " 10233.773971354167,\n",
       " 10233.310013020833,\n",
       " 10232.848010416667,\n",
       " 10232.3843671875,\n",
       " 10231.920708333333,\n",
       " 10231.458375,\n",
       " 10230.994458333333,\n",
       " 10230.531010416667,\n",
       " 10230.0690703125,\n",
       " 10229.604875,\n",
       " 10229.142203125,\n",
       " 10228.679778645834,\n",
       " 10228.215385416666,\n",
       " 10227.752815104166,\n",
       " 10227.289697916667,\n",
       " 10226.825515625,\n",
       " 10226.363611979166,\n",
       " 10225.899716145834,\n",
       " 10225.4361796875,\n",
       " 10224.974473958333,\n",
       " 10224.510364583333,\n",
       " 10224.047036458333,\n",
       " 10223.584809895834,\n",
       " 10223.120901041666,\n",
       " 10222.657643229166,\n",
       " 10222.195286458333,\n",
       " 10221.7314765625,\n",
       " 10221.268651041666,\n",
       " 10220.8056796875,\n",
       " 10220.342018229167,\n",
       " 10219.879713541666,\n",
       " 10219.415864583334,\n",
       " 10218.952359375,\n",
       " 10218.490388020833,\n",
       " 10218.026364583333,\n",
       " 10217.56378125,\n",
       " 10217.100447916666,\n",
       " 10216.636276041667,\n",
       " 10216.174109375,\n",
       " 10215.711690104166,\n",
       " 10215.246859375,\n",
       " 10214.784958333334,\n",
       " 10214.321697916666,\n",
       " 10213.857140625,\n",
       " 10213.395830729167,\n",
       " 10212.931690104168,\n",
       " 10212.469018229167,\n",
       " 10212.006364583332,\n",
       " 10211.542020833333,\n",
       " 10211.079114583334,\n",
       " 10210.6164375,\n",
       " 10210.152643229167,\n",
       " 10209.6901640625,\n",
       " 10209.2268359375,\n",
       " 10208.763221354167,\n",
       " 10208.301182291667,\n",
       " 10207.837630208332,\n",
       " 10207.3733828125,\n",
       " 10206.91165625,\n",
       " 10206.447518229166,\n",
       " 10205.984674479167,\n",
       " 10205.521640625,\n",
       " 10205.0579765625,\n",
       " 10204.595283854167,\n",
       " 10204.13321875,\n",
       " 10203.668479166667,\n",
       " 10203.206348958332,\n",
       " 10202.7429453125,\n",
       " 10202.279557291668,\n",
       " 10201.817296875,\n",
       " 10201.353135416666,\n",
       " 10200.890328125,\n",
       " 10200.428020833333,\n",
       " 10199.964067708333,\n",
       " 10199.500854166667,\n",
       " 10199.038440104167,\n",
       " 10198.5746640625,\n",
       " 10198.112346354166,\n",
       " 10197.649122395833,\n",
       " 10197.1851953125,\n",
       " 10196.723182291667,\n",
       " 10196.259682291668,\n",
       " 10195.7965,\n",
       " 10195.33446875,\n",
       " 10194.870158854166,\n",
       " 10194.407325520833,\n",
       " 10193.94521875,\n",
       " 10193.480434895833,\n",
       " 10193.018604166668,\n",
       " 10192.555666666667,\n",
       " 10192.092065104167,\n",
       " 10191.629611979166,\n",
       " 10191.165890625,\n",
       " 10190.703231770833,\n",
       " 10190.241364583333,\n",
       " 10189.777169270834,\n",
       " 10189.314466145834,\n",
       " 10188.8512734375,\n",
       " 10188.38765625,\n",
       " 10187.926286458332,\n",
       " 10187.462184895834,\n",
       " 10186.999229166666,\n",
       " 10186.537114583334,\n",
       " 10186.072776041667,\n",
       " 10185.611192708333,\n",
       " 10185.148276041667,\n",
       " 10184.683893229167,\n",
       " 10184.221859375,\n",
       " 10183.7590625,\n",
       " 10183.295643229167,\n",
       " 10182.833770833333,\n",
       " 10182.369263020833,\n",
       " 10181.906955729166,\n",
       " 10181.444455729166,\n",
       " 10180.980658854167,\n",
       " 10180.518877604167,\n",
       " ...]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_5_input to have 3 dimensions, but got array with shape (3, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-281-fe4cfff9e142>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_5_input to have 3 dimensions, but got array with shape (3, 100)"
     ]
    }
   ],
   "source": [
    "xt = test_data[:]\n",
    "model.predict(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#繪圖\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss)+ 1)\n",
    "plt.plot(epochs, loss,'bo',label='Training loss')\n",
    "plt.plot(epochs, val_loss,'b',label='Validation loss')\n",
    "plt.title('訓練與驗證的損失函數')\n",
    "plt.xlabel('Epohs')\n",
    "plt.xlabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#視覺化,紅色為標準答案\n",
    "plt.style.use('seaborn')\n",
    "test_targets_array=test_targets.values\n",
    "plt.figure(figsize=(15, 6)) \n",
    "xt = test_data[:]\n",
    "plt.plot(test_targets_array, 'rd', label='test_targets_array')\n",
    "plt.plot(Vote.predict(xt), 'gd', label='model_xgb')\n",
    "# plt.plot(model_AdaBoostRegressor.predict(xt), '#00008B', label='model_AdaBoostRegressor')\n",
    "# plt.plot(Vote.predict(xt), 'bd', label='Vote')\n",
    "\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False,\n",
    "                labelbottom=False)\n",
    "plt.ylabel('predicted')\n",
    "plt.xlabel('training samples')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Comparison of individual predictions with test_targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PCA\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "x = X_scaler.fit_transform(fdsd.iloc[:,1:Significant_factor.shape[1]])\n",
    "# PCA\n",
    "pca = PCA(n_components=0.9)# 保證降維後的資料保持90%的資訊\n",
    "pca.fit(x)\n",
    "asas = pca.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
